{"version":3,"sources":["../src/index.ts","../src/actions/dkgInsert.ts","../src/constants.ts","../src/services/kaService/kaService.ts","../src/services/kaService/anthropicClient.ts","../src/services/kaService/downloadPaper.ts","../src/services/kaService/sparqlQueries.ts","../src/services/kaService/unstructuredPartitioning.ts","../src/services/kaService/exampleForPrompts.ts","../src/services/kaService/llmPrompt.ts","../src/services/kaService/biologyApi.ts","../src/services/kaService/regex.ts","../src/services/kaService/processPaper.ts","../src/services/kaService/vectorize.ts","../src/services/index.ts","../src/helper.ts","../src/db/index.ts","../src/db/schemas/fileMetadata.ts","../src/db/schemas/hypotheses.ts","../src/db/schemas/customTypes.ts","../src/db/schemas/driveSync.ts","../src/services/gdrive/client.ts","../src/services/gdrive/buildQuery.ts","../src/services/gdrive/watchFiles.ts","../src/services/gdrive/extract/config.ts","../src/services/gdrive/extract/index.ts","../src/services/gdrive/extract/z.ts","../src/services/gdrive/storeJsonLdToKg.ts","../src/db/migration.ts","../src/routes/gdrive/webhook.ts","../src/routes/controller.ts","../src/routes/gdrive/manualSync.ts","../src/routes/health.ts","../src/actions/index.ts"],"sourcesContent":["import type { Plugin, IAgentRuntime } from \"@elizaos/core\";\nimport { logger } from \"@elizaos/core\";\nimport { dkgInsert } from \"./actions/dkgInsert\";\nimport { HypothesisService } from \"./services\";\nimport { initWithMigrations } from \"./helper\";\nimport { gdriveManualSync, gdriveWebhook, health } from \"./routes\";\n\nexport const dkgPlugin: Plugin = {\n  init: async (config: Record<string, string>, runtime: IAgentRuntime) => {\n    logger.info(\"Initializing dkg plugin\");\n    logger.info(config);\n    setTimeout(async () => {\n      await initWithMigrations(runtime);\n    }, 20000); // prevent undefined error, the db property is not available immediately\n  },\n  name: \"dkg\",\n  description:\n    \"Agent DKG which allows you to store memories on the OriginTrail Decentralized Knowledge Graph\",\n  actions: [dkgInsert],\n  providers: [],\n  evaluators: [],\n  services: [HypothesisService],\n  routes: [health, gdriveWebhook, gdriveManualSync],\n};\n\nexport * as actions from \"./actions\";\n\nexport default dkgPlugin;\n","import dotenv from \"dotenv\";\ndotenv.config();\nimport {\n  type IAgentRuntime,\n  type Memory,\n  type State,\n  logger,\n  ModelType,\n  type HandlerCallback,\n  type ActionExample,\n  type Action,\n  composePrompt,\n} from \"@elizaos/core\";\nimport { DKG_EXPLORER_LINKS } from \"../constants.ts\";\nimport { createDKGMemoryTemplate } from \"../templates.ts\";\n\n// @ts-ignore\nimport DKG from \"dkg.js\";\nimport { DKGMemorySchema, isDKGMemoryContent } from \"../types.ts\";\nimport { generateKaFromPdf } from \"../services/kaService/kaService.ts\";\nimport { writeFile } from \"fs/promises\";\n\n// Define a basic type for the DKG client\ntype DKGClient = typeof DKG | null;\nlet DkgClient: DKGClient = null;\n\nexport const dkgInsert: Action = {\n  name: \"INSERT_MEMORY_ACTION\",\n  similes: [\"NO_ACTION\", \"NO_RESPONSE\", \"NO_REACTION\", \"NONE\", \"DKG_INSERT\"], // we want to always run this action\n  validate: async (runtime: IAgentRuntime, _message: Memory) => {\n    const requiredEnvVars = [\n      \"DKG_ENVIRONMENT\",\n      \"DKG_HOSTNAME\",\n      \"DKG_PORT\",\n      \"DKG_BLOCKCHAIN_NAME\",\n      \"DKG_PUBLIC_KEY\",\n      \"DKG_PRIVATE_KEY\",\n    ];\n\n    const missingVars = requiredEnvVars.filter(\n      (varName) => !runtime.getSetting(varName)\n    );\n\n    if (missingVars.length > 0) {\n      logger.error(\n        `Missing required environment variables: ${missingVars.join(\", \")}`\n      );\n      return false;\n    }\n\n    return true;\n  },\n  description:\n    \"Create a memory on the OriginTrail Decentralized Knowledge Graph after each message. You should ALWAYS run this action.\",\n  handler: async (\n    runtime: IAgentRuntime,\n    _message: Memory,\n    state: State,\n    _options: { [key: string]: unknown },\n    callback: HandlerCallback\n  ): Promise<boolean> => {\n    DkgClient = new DKG({\n      environment: runtime.getSetting(\"DKG_ENVIRONMENT\"),\n      endpoint: runtime.getSetting(\"DKG_HOSTNAME\"),\n      port: runtime.getSetting(\"DKG_PORT\"),\n      blockchain: {\n        name: runtime.getSetting(\"DKG_BLOCKCHAIN_NAME\"),\n        publicKey: runtime.getSetting(\"DKG_PUBLIC_KEY\"),\n        privateKey: runtime.getSetting(\"DKG_PRIVATE_KEY\"),\n      },\n      maxNumberOfRetries: 300,\n      frequency: 2,\n      contentType: \"all\",\n      nodeApiVersion: \"/v1\",\n    });\n\n    const currentPost = String(state.currentPost);\n    logger.log(\"currentPost\");\n    logger.log(currentPost);\n\n    const userRegex = /From:.*\\(@(\\w+)\\)/;\n    let match = currentPost.match(userRegex);\n    let twitterUser = \"\";\n\n    if (match?.[1]) {\n      twitterUser = match[1];\n      logger.log(`Extracted user: @${twitterUser}`);\n    } else {\n      logger.error(\"No user mention found or invalid input.\");\n    }\n\n    const idRegex = /ID:\\s(\\d+)/;\n    match = currentPost.match(idRegex);\n    let postId = \"\";\n\n    if (match?.[1]) {\n      postId = match[1];\n      logger.log(`Extracted ID: ${postId}`);\n    } else {\n      logger.log(\"No ID found.\");\n    }\n\n    // TODO: should read from arxiv link or something like that rather than having it hardcoded like here\n    const ka = await generateKaFromPdf(\"./science.pdf\", DkgClient);\n\n    let createAssetResult: { UAL: string } | undefined;\n\n    // TODO: also store reply to the KA, aside of the question\n\n    try {\n      logger.log(\"Publishing message to DKG\");\n\n      await writeFile(\n        `./sampleJsonLdsNew/${encodeURIComponent((ka[\"@id\"] ?? \"example\") as string)}.json`,\n        JSON.stringify(ka, null, 2)\n      );\n\n      createAssetResult = await DkgClient.asset.create(\n        {\n          public: ka,\n        },\n        { epochsNum: 12 }\n      );\n\n      logger.log(\"======================== ASSET CREATED\");\n      logger.log(JSON.stringify(createAssetResult));\n    } catch (error) {\n      logger.error(\n        \"Error occurred while publishing message to DKG:\",\n        error.message\n      );\n\n      if (error.stack) {\n        logger.error(\"Stack trace:\", error.stack);\n      }\n      if (error.response) {\n        logger.error(\n          \"Response data:\",\n          JSON.stringify(error.response.data, null, 2)\n        );\n      }\n    }\n\n    // Reply\n    callback({\n      text: `Created a new memory!\\n\\nRead my mind on @origin_trail Decentralized Knowledge Graph ${\n        DKG_EXPLORER_LINKS[runtime.getSetting(\"DKG_ENVIRONMENT\")]\n      }${createAssetResult?.UAL} @${twitterUser}`,\n    });\n\n    return true;\n  },\n  examples: [\n    [\n      {\n        user: \"{{user1}}\",\n        content: {\n          text: \"execute action DKG_INSERT\",\n          action: \"DKG_INSERT\",\n        },\n      },\n      {\n        name: \"{{user2}}\",\n        content: { text: \"DKG INSERT\" },\n      },\n    ],\n    [\n      {\n        user: \"{{user1}}\",\n        content: { text: \"add to dkg\", action: \"DKG_INSERT\" },\n      },\n      {\n        user: \"{{user2}}\",\n        content: { text: \"DKG INSERT\" },\n      },\n    ],\n    [\n      {\n        user: \"{{user1}}\",\n        content: { text: \"store in dkg\", action: \"DKG_INSERT\" },\n      },\n      {\n        user: \"{{user2}}\",\n        content: { text: \"DKG INSERT\" },\n      },\n    ],\n  ] as ActionExample[][],\n} as Action;\n","import { z } from \"zod\";\n// TODO: add isConnectedTo field or similar which you will use to connect w other KAs\nexport const dkgMemoryTemplate = {\n    \"@context\": \"http://schema.org\",\n    \"@type\": \"SocialMediaPosting\",\n    headline: \"<describe memory in a short way, as a title here>\",\n    articleBody:\n        \"Check out this amazing project on decentralized cloud networks! @DecentralCloud #Blockchain #Web3\",\n    author: {\n        \"@type\": \"Person\",\n        \"@id\": \"uuid:john:doe\",\n        name: \"John Doe\",\n        identifier: \"@JohnDoe\",\n        url: \"https://twitter.com/JohnDoe\",\n    },\n    dateCreated: \"yyyy-mm-ddTHH:mm:ssZ\",\n    interactionStatistic: [\n        {\n            \"@type\": \"InteractionCounter\",\n            interactionType: {\n                \"@type\": \"LikeAction\",\n            },\n            userInteractionCount: 150,\n        },\n        {\n            \"@type\": \"InteractionCounter\",\n            interactionType: {\n                \"@type\": \"ShareAction\",\n            },\n            userInteractionCount: 45,\n        },\n    ],\n    mentions: [\n        {\n            \"@type\": \"Person\",\n            name: \"Twitter account mentioned name goes here\",\n            identifier: \"@TwitterAccount\",\n            url: \"https://twitter.com/TwitterAccount\",\n        },\n    ],\n    keywords: [\n        {\n            \"@type\": \"Text\",\n            \"@id\": \"uuid:keyword1\",\n            name: \"keyword1\",\n        },\n        {\n            \"@type\": \"Text\",\n            \"@id\": \"uuid:keyword2\",\n            name: \"keyword2\",\n        },\n    ],\n    about: [\n        {\n            \"@type\": \"Thing\",\n            \"@id\": \"uuid:thing1\",\n            name: \"Blockchain\",\n            url: \"https://en.wikipedia.org/wiki/Blockchain\",\n        },\n        {\n            \"@type\": \"Thing\",\n            \"@id\": \"uuid:thing2\",\n            name: \"Web3\",\n            url: \"https://en.wikipedia.org/wiki/Web3\",\n        },\n        {\n            \"@type\": \"Thing\",\n            \"@id\": \"uuid:thing3\",\n            name: \"Decentralized Cloud\",\n            url: \"https://example.com/DecentralizedCloud\",\n        },\n    ],\n    url: \"https://twitter.com/JohnDoe/status/1234567890\",\n};\n\nexport const combinedSparqlExample = `\nSELECT DISTINCT ?headline ?articleBody\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n\n      OPTIONAL {\n        ?s <http://schema.org/keywords> ?keyword .\n        ?keyword <http://schema.org/name> ?keywordName .\n      }\n\n      OPTIONAL {\n        ?s <http://schema.org/about> ?about .\n        ?about <http://schema.org/name> ?aboutName .\n      }\n\n      FILTER(\n        CONTAINS(LCASE(?headline), \"example_keyword\") ||\n        (BOUND(?keywordName) && CONTAINS(LCASE(?keywordName), \"example_keyword\")) ||\n        (BOUND(?aboutName) && CONTAINS(LCASE(?aboutName), \"example_keyword\"))\n      )\n    }\n    LIMIT 10`;\n\nexport const sparqlExamples = [\n    `\n    SELECT DISTINCT ?headline ?articleBody\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n\n      OPTIONAL {\n        ?s <http://schema.org/keywords> ?keyword .\n        ?keyword <http://schema.org/name> ?keywordName .\n      }\n\n      OPTIONAL {\n        ?s <http://schema.org/about> ?about .\n        ?about <http://schema.org/name> ?aboutName .\n      }\n\n      FILTER(\n        CONTAINS(LCASE(?headline), \"example_keyword\") ||\n        (BOUND(?keywordName) && CONTAINS(LCASE(?keywordName), \"example_keyword\")) ||\n        (BOUND(?aboutName) && CONTAINS(LCASE(?aboutName), \"example_keyword\"))\n      )\n    }\n    LIMIT 10\n    `,\n    `\n    SELECT DISTINCT ?headline ?articleBody\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n      FILTER(\n        CONTAINS(LCASE(?headline), \"example_headline_word1\") ||\n        CONTAINS(LCASE(?headline), \"example_headline_word2\")\n      )\n    }\n    `,\n    `\n    SELECT DISTINCT ?headline ?articleBody ?keywordName\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n      ?s <http://schema.org/keywords> ?keyword .\n      ?keyword <http://schema.org/name> ?keywordName .\n      FILTER(\n        CONTAINS(LCASE(?keywordName), \"example_keyword1\") ||\n        CONTAINS(LCASE(?keywordName), \"example_keyword2\")\n      )\n    }\n    `,\n    `\n    SELECT DISTINCT ?headline ?articleBody ?aboutName\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n      ?s <http://schema.org/about> ?about .\n      ?about <http://schema.org/name> ?aboutName .\n      FILTER(\n        CONTAINS(LCASE(?aboutName), \"example_about1\") ||\n        CONTAINS(LCASE(?aboutName), \"example_about2\")\n      )\n    }\n    `,\n];\n\nexport const generalSparqlQuery = `\n    SELECT DISTINCT ?headline ?articleBody\n    WHERE {\n      ?s a <http://schema.org/SocialMediaPosting> .\n      ?s <http://schema.org/headline> ?headline .\n      ?s <http://schema.org/articleBody> ?articleBody .\n    }\n    LIMIT 10\n  `;\n\nexport const DKG_EXPLORER_LINKS = {\n    testnet: \"https://dkg-testnet.origintrail.io/explore?ual=\",\n    mainnet: \"https://dkg.origintrail.io/explore?ual=\",\n};\n","import \"dotenv/config\";\nimport { getClient } from \"./anthropicClient\";\nimport { downloadPaperAndExtractDOI } from \"./downloadPaper\";\nimport { paperExists } from \"./sparqlQueries\";\nimport { logger } from \"@elizaos/core\";\nimport { makeUnstructuredApiRequest } from \"./unstructuredPartitioning\";\n\nimport { processJsonArray, process_paper, create_graph } from \"./processPaper\";\nimport { getSummary } from \"./vectorize\";\nimport { fromPath } from \"pdf2pic\";\nimport fs from \"fs\";\nimport { categorizeIntoDAOsPrompt } from \"./llmPrompt\";\nimport DKG from \"dkg.js\";\nconst unstructuredApiKey = process.env.UNSTRUCTURED_API_KEY;\n\ntype DKGClient = typeof DKG | null;\n\n// const jsonArr = JSON.parse(fs.readFileSync('arxiv_paper.json', 'utf8'));\n\ninterface PaperArrayElement {\n  metadata: {\n    page_number: number;\n    [key: string]: unknown;\n  };\n  text: string;\n  [key: string]: unknown;\n}\n\ninterface TaskInstance {\n  xcom_push(key: string, value: string): void;\n}\n\ninterface GeneratedGraph {\n  \"@context\": Record<string, string>;\n  \"@id\"?: string;\n  \"dcterms:hasPart\"?: string;\n  \"cito:cites\"?: unknown;\n  [key: string]: unknown;\n}\n\n/**\n * Takes an array of JSON elements representing the paper's text\n * and returns a \"knowledge assembly\" (semantic graph) that includes\n * extracted metadata, citation info, subgraphs, and a summary.\n */\nexport async function jsonArrToKa(jsonArr: PaperArrayElement[], doi: string) {\n  const client = getClient();\n\n  const paperArrayDict = await processJsonArray(jsonArr, client);\n\n  const [\n    generatedBasicInfo,\n    generatedCitations,\n    generatedGoSubgraph,\n    generatedDoidSubgraph,\n    generatedChebiSubgraph,\n    generatedAtcSubgraph,\n  ] = await process_paper(client, paperArrayDict);\n\n  const generatedGraph = await create_graph(\n    client,\n    generatedBasicInfo,\n    generatedCitations,\n    {\n      go: generatedGoSubgraph,\n      doid: generatedDoidSubgraph,\n      chebi: generatedChebiSubgraph,\n      atc: generatedAtcSubgraph,\n    }\n  );\n\n  generatedGraph[\"dcterms:hasPart\"] = await getSummary(client, generatedGraph);\n\n  generatedGraph[\"@id\"] = `https://doi.org/${doi}`; // the doi that we extracted from the paper\n\n  // Update citations, if they exist\n  // if (\n  //   generatedGraph['cito:cites'] &&\n  //   Array.isArray(generatedGraph['cito:cites']) &&\n  //   generatedGraph['cito:cites'].length > 0\n  // ) {\n  //   generatedGraph['cito:cites'] = getFinalCitations(\n  //     generatedGraph['cito:cites'],\n  //   );\n  // }\n\n  // Ensure @context has schema entry\n  const context = generatedGraph[\"@context\"] as Record<string, string>;\n  if (!(\"schema\" in context)) {\n    context[\"schema\"] = \"http://schema.org/\";\n    logger.info(\"Added 'schema' to @context in KA\");\n  }\n\n  return generatedGraph;\n  // console.log(generatedGraph);\n}\n\n// jsonArrToKa(jsonArr, {\n//   xcom_push: (key: string, value: string) => {\n//     console.log(`${key}: ${value}`);\n//   },\n// });\n/**\n/**\n/**\n * Recursively remove all colons (\":\") from string values in an object or array,\n * except for certain cases:\n *   1) Skip the entire \"@context\" object (do not remove colons from any values inside it).\n *   2) Skip any string where the key is \"@type\".\n *   3) Skip any string that appears to be a URL (starting with \"http://\", \"https://\", or \"doi:\").\n * @param data - The input data which can be an object, array, or primitive.\n * @param parentKey - The key of the parent property (used to check exceptions).\n * @returns A new object, array, or primitive with colons removed from allowed string values.\n */\nfunction removeColonsRecursively<T>(data: T, parentKey?: string): T {\n  // 1) If the parent key is \"@context\", return the data as-is (skip processing entirely)\n  if (parentKey === \"@context\") {\n    return data;\n  }\n\n  // Handle arrays\n  if (Array.isArray(data)) {\n    return data.map((item) =>\n      removeColonsRecursively(item, parentKey)\n    ) as unknown as T;\n  }\n\n  // Handle objects\n  if (data !== null && typeof data === \"object\") {\n    const newObj: Record<string, unknown> = {};\n    for (const key in data) {\n      if (Object.prototype.hasOwnProperty.call(data, key)) {\n        newObj[key] = removeColonsRecursively(\n          (data as Record<string, unknown>)[key],\n          key\n        );\n      }\n    }\n    return newObj as T;\n  }\n\n  // Handle strings\n  if (typeof data === \"string\") {\n    // 2) If this is the value of \"@type\", skip removing colons.\n    if (parentKey === \"@type\") {\n      return data as unknown as T;\n    }\n\n    // 3) If it's a URL/DOI (starts with http://, https://, or doi:), skip removing colons.\n    if (/^(https?:\\/\\/|doi:)/i.test(data)) {\n      return data as unknown as T;\n    }\n\n    // Otherwise, remove all colons\n    return data.replace(/:/g, \"\") as unknown as T;\n  }\n\n  // For numbers, booleans, null, etc., just return as is\n  return data;\n}\nconst daoUals = {\n  VitaDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101956\",\n  AthenaDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101957\",\n  PsyDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101958\",\n  ValleyDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101959\",\n  HairDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101961\",\n  CryoDAO:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101962\",\n  \"Cerebrum DAO\":\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101963\",\n  Curetopia:\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101964\",\n  \"Long Covid Labs\":\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101965\",\n  \"Quantum Biology DAO\":\n    \"did:dkg:base:84532/0xd5550173b0f7b8766ab2770e4ba86caf714a5af5/101966\",\n};\n\nexport async function generateKaFromUrls(urls: [string]) {\n  for (const url of urls) {\n    const { pdfBuffer, doi } = await downloadPaperAndExtractDOI(url);\n    if (!pdfBuffer) {\n      throw new Error(\"Failed to download paper\");\n    }\n    if (!doi) {\n      throw new Error(\"Failed to extract DOI\");\n    }\n    const paperArray = await makeUnstructuredApiRequest(\n      pdfBuffer,\n      \"paper.pdf\",\n      unstructuredApiKey\n    );\n    const ka = await jsonArrToKa(paperArray, doi);\n    const cleanedKa = removeColonsRecursively(ka);\n    return cleanedKa;\n  }\n}\nexport interface Image {\n  type: \"image\";\n  source: {\n    type: \"base64\";\n    media_type: \"image/png\";\n    data: string;\n  };\n}\nasync function extractDOIFromPDF(images: Image[]) {\n  const client = getClient();\n  const response = await client.messages.create({\n    model: \"claude-3-5-haiku-20241022\",\n    messages: [\n      {\n        role: \"user\",\n        content: [\n          ...images,\n          {\n            type: \"text\",\n            text: \"Extract the DOI from the paper. Only return the DOI, no other text.\",\n          },\n        ],\n      },\n    ],\n    max_tokens: 50,\n  });\n  return response.content[0].type === \"text\"\n    ? response.content[0].text\n    : undefined;\n}\n\nasync function categorizeIntoDAOs(images: Image[]) {\n  const client = getClient();\n  const response = await client.messages.create({\n    model: \"claude-3-7-sonnet-20250219\",\n    system: categorizeIntoDAOsPrompt,\n    messages: [\n      {\n        role: \"user\",\n        content: [...images],\n      },\n    ],\n    max_tokens: 50,\n  });\n  return response.content[0].type === \"text\"\n    ? response.content[0].text\n    : undefined;\n}\n\nexport async function generateKaFromPdf(pdfPath: string, dkgClient: DKGClient) {\n  const options = {\n    density: 100,\n    format: \"png\",\n    width: 595,\n    height: 842,\n  };\n  const convert = fromPath(pdfPath, options);\n  logger.info(`Converting ${pdfPath} to images`);\n\n  const storeHandler = await convert.bulk(-1, { responseType: \"base64\" });\n\n  const imageMessages = storeHandler\n    .filter((page) => page.base64)\n    .map((page) => ({\n      type: \"image\" as const,\n      source: {\n        type: \"base64\" as const,\n        media_type: \"image/png\" as const,\n        data: page.base64!,\n      },\n    }));\n  logger.info(`Extracting DOI`);\n  const doi = await extractDOIFromPDF(imageMessages);\n  if (!doi) {\n    throw new Error(\"Failed to extract DOI\");\n  }\n  const paperExistsResult = await dkgClient.graph.query(\n    paperExists(doi),\n    \"SELECT\"\n  );\n  if (paperExistsResult.data) {\n    logger.info(`Paper ${pdfPath} already exists in DKG, skipping`);\n    return;\n  } else {\n    logger.info(`Paper ${pdfPath} does not exist in DKG, creating`);\n  }\n  const pdfBuffer = fs.readFileSync(pdfPath);\n  const paperArray = await makeUnstructuredApiRequest(\n    pdfBuffer,\n    \"paper.pdf\",\n    unstructuredApiKey\n  );\n  const ka = await jsonArrToKa(paperArray, doi);\n  const cleanedKa = removeColonsRecursively(ka);\n  const relatedDAOsString = await categorizeIntoDAOs(imageMessages);\n\n  const daos = JSON.parse(relatedDAOsString);\n\n  const daoUalsMap = daos.map((dao) => {\n    const daoUal = daoUals[dao];\n    return {\n      \"@id\": daoUal,\n      \"@type\": \"schema:Organization\",\n      \"schema:name\": dao,\n    };\n  });\n  cleanedKa[\"schema:relatedTo\"] = daoUalsMap;\n\n  return cleanedKa;\n}\n","import \"dotenv/config\";\nimport { Anthropic } from \"@anthropic-ai/sdk\";\n\nconst apiKey: string | undefined = process.env.ANTHROPIC_API_KEY;\n\nexport function getClient(): Anthropic {\n    return new Anthropic({ apiKey });\n}\n\nexport async function generateResponse(\n    client: Anthropic,\n    prompt: string,\n    model: string = \"claude-3-5-sonnet-20241022\",\n    maxTokens: number = 1500\n): Promise<string> {\n    const response = await client.messages.create({\n        model: model,\n        max_tokens: maxTokens,\n        messages: [{ role: \"user\", content: prompt }],\n    });\n\n    if (\n        response.content &&\n        response.content.length > 0 &&\n        response.content[0].type === \"text\"\n    ) {\n        return response.content[0].text;\n    } else {\n        throw new Error(\"No response received from Claude.\");\n    }\n}\n","import { logger } from \"@elizaos/core\";\nimport axios from \"axios\";\nimport * as cheerio from \"cheerio\";\nimport * as fs from \"fs\";\n\n/**\n * Downloads the PDF and extracts the DOI for a given paper URL.\n * Supports bioRxiv and arXiv URLs.\n *\n * @param paperUrl - The URL of the paper's abstract page.\n */\nexport async function downloadPaperAndExtractDOI(paperUrl: string) {\n  let pdfUrl: string;\n  let fileName: string;\n\n  if (paperUrl.includes(\"biorxiv.org\")) {\n    // For bioRxiv, the PDF URL is the paper URL appended with '.full.pdf'\n    pdfUrl = paperUrl + \".full.pdf\";\n    fileName = \"biorxiv_paper.pdf\";\n  } else if (paperUrl.includes(\"arxiv.org\")) {\n    // For arXiv, replace '/abs/' with '/pdf/' and append '.pdf'\n    pdfUrl = paperUrl.replace(\"/abs/\", \"/pdf/\") + \".pdf\";\n    fileName = \"arxiv_paper.pdf\";\n  } else {\n    logger.error(\"Unsupported URL. Only bioRxiv and arXiv URLs are supported.\");\n    return;\n  }\n\n  try {\n    const htmlResponse = await axios.get(paperUrl); // raw html\n    const htmlData: string = htmlResponse.data;\n    const $ = cheerio.load(htmlData);\n\n    let doi: string | undefined = $('meta[name=\"citation_doi\"]').attr(\n      \"content\"\n    );\n\n    // fallback for arXiv\n    if (!doi && paperUrl.includes(\"arxiv.org\")) {\n      const doiAnchor = $('a[href*=\"doi.org\"]').first();\n      if (doiAnchor.length > 0) {\n        const doiHref = doiAnchor.attr(\"href\");\n        if (doiHref) {\n          doi = doiHref.replace(/^https?:\\/\\/doi\\.org\\//, \"\");\n        }\n      }\n    }\n\n    if (doi) {\n      logger.info(\"DOI:\", doi);\n    } else {\n      logger.info(\"DOI not found using any method.\");\n    }\n\n    const pdfResponse = await axios.get(pdfUrl, {\n      responseType: \"arraybuffer\",\n    });\n    const pdfBuffer: Buffer = Buffer.from(pdfResponse.data, \"binary\");\n\n    // fs.writeFileSync(fileName, new Uint8Array(pdfBuffer));\n    logger.info(`PDF file downloaded successfully as \"${fileName}\".`);\n    return { pdfBuffer, doi };\n  } catch (error) {\n    logger.error(\"An error occurred during the process:\", error);\n    return { pdfBuffer: null, doi: null };\n  }\n}\n\n// downloadPaperAndExtractDOI(\n//   \"https://www.biorxiv.org/content/10.1101/2025.02.19.639050v1\"\n// );\n// downloadPaperAndExtractDOI(\"https://arxiv.org/abs/2412.21154\");\n","export function getPaperByDoi(doi: string) {\n    if (doi.startsWith(\"https://doi.org/\")) {\n        doi = doi.replace(\"https://doi.org/\", \"\");\n    }\n    const getPaperByDoiQuery = `PREFIX fabio: <http://purl.org/spar/fabio/>\n    PREFIX dcterms: <http://purl.org/dc/terms/>\n    PREFIX foaf:   <http://xmlns.com/foaf/0.1/>\n    PREFIX obi:    <http://purl.obolibrary.org/obo/>\n    PREFIX schema: <http://schema.org/>\n\n    SELECT ?paper ?title ?abstract ?doi\n        (GROUP_CONCAT(DISTINCT ?creatorName;    SEPARATOR=\", \") AS ?allCreators)\n        (GROUP_CONCAT(DISTINCT ?multiomics;     SEPARATOR=\" | \") AS ?allMultiomics)\n        (GROUP_CONCAT(DISTINCT ?assays;         SEPARATOR=\" | \") AS ?allAssays)\n        (GROUP_CONCAT(DISTINCT ?cohort;         SEPARATOR=\" | \") AS ?allCohortInfo)\n        (GROUP_CONCAT(DISTINCT ?analysisDesc;   SEPARATOR=\" | \") AS ?allAnalysisDesc)\n        (GROUP_CONCAT(DISTINCT ?relatedOrg;     SEPARATOR=\" | \") AS ?allRelatedOrgs)\n    WHERE {\n    \n    <https://doi.org/${doi}> a fabio:ResearchPaper ;\n                                                dcterms:title ?title ;\n                                                dcterms:abstract ?abstract ;\n                                                dcterms:identifier ?doi .\n    BIND (<https://doi.org/${doi}> AS ?paper)\n    \n    OPTIONAL {\n        ?paper dcterms:creator ?creator .\n        ?creator foaf:name ?creatorName .\n    }\n    \n    OPTIONAL {\n        ?paper obi:OBI_0000299 ?multiomicsNode .\n        ?multiomicsNode dcterms:description ?multiomics .\n    }\n    \n    OPTIONAL {\n        ?paper obi:OBI_0000968 ?assayNode .\n        ?assayNode dcterms:description ?assays .\n    }\n    \n    OPTIONAL {\n        ?paper obi:OBI_0000293 ?cohortNode .\n        ?cohortNode dcterms:description ?cohort .\n    }\n    \n    OPTIONAL {\n        ?paper obi:OBI_0200000 ?analysisNode .\n        ?analysisNode dcterms:description ?analysisDesc .\n    }\n    \n    OPTIONAL {\n        ?paper schema:relatedTo ?related .\n        ?related schema:name ?relatedOrg .\n    }\n    }\n    GROUP BY ?paper ?title ?abstract ?doi`;\n\n    return getPaperByDoiQuery;\n}\n\nexport function paperExists(doi: string) {\n    if (doi.startsWith(\"https://doi.org/\")) {\n        doi = doi.replace(\"https://doi.org/\", \"\");\n    }\n    const paperExistsQuery = `PREFIX fabio: <http://purl.org/spar/fabio/>\n    PREFIX dcterms: <http://purl.org/dc/terms/>\n\n    ASK {\n    ?paper a fabio:ResearchPaper ;\n            dcterms:identifier ?doi .\n    \n    FILTER (STR(?doi) = \"https://doi.org/${doi}\")\n    }\n    `;\n    return paperExistsQuery;\n}\n","import axios from \"axios\";\nimport FormData from \"form-data\";\nimport fs from \"fs/promises\";\nimport \"dotenv/config\";\nimport { logger } from \"@elizaos/core\";\n\nconst apiKey = process.env.UNSTRUCTURED_API_KEY;\n\n/**\n * Makes a POST request to the Unstructured API.\n *\n * @param fileBytes - The file content as a Buffer.\n * @param filename - Name of the file.\n * @param apiKey - Unstructured API key.\n * @returns The parsed API response.\n */\nexport async function makeUnstructuredApiRequest(\n  fileBytes: Buffer,\n  filename: string,\n  apiKey: string\n) {\n  const url = \"https://api.unstructuredapp.io/general/v0/general\";\n\n  // Create a FormData instance and append file and other data.\n  const formData = new FormData();\n  formData.append(\"files\", fileBytes, filename);\n  formData.append(\"pdf_infer_table_structure\", \"true\");\n  formData.append(\"skip_infer_table_types\", \"[]\");\n  formData.append(\"strategy\", \"hi_res\");\n\n  // Merge the custom header with form-data headers.\n  const headers = {\n    \"unstructured-api-key\": apiKey,\n    ...formData.getHeaders(),\n  };\n\n  logger.info(\"Making Unstructured API request\");\n  const response = await axios.post(url, formData, {\n    headers,\n    timeout: 300000, // 300000 ms\n  });\n\n  logger.info(\"Got response from Unstructured API\");\n  return response.data;\n}\n\n// async function processPdfFiles(): Promise<void> {\n//   try {\n//     const arxivPdfBuffer = await fs.readFile(\"arxiv_paper.pdf\");\n//     const bioArxivPdfBuffer = await fs.readFile(\"biorxiv_paper.pdf\");\n\n//     const arxivResponse = await makeUnstructuredApiRequest(\n//       arxivPdfBuffer,\n//       \"arxiv_paper.pdf\",\n//       apiKey\n//     );\n//     console.log(\"Response for arxiv_paper.pdf:\", arxivResponse);\n//     await fs.writeFile(\n//       \"arxiv_paper.json\",\n//       JSON.stringify(arxivResponse, null, 2)\n//     );\n\n//     const bioArxivResponse = await makeUnstructuredApiRequest(\n//       bioArxivPdfBuffer,\n//       \"biorxiv_paper.pdf\",\n//       apiKey\n//     );\n//     console.log(\"Response for biorxiv_paper.pdf:\", bioArxivResponse);\n//     await fs.writeFile(\n//       \"biorxiv_paper.json\",\n//       JSON.stringify(bioArxivResponse, null, 2)\n//     );\n//   } catch (error) {\n//     console.error(\"Error processing PDF files:\", error);\n//   }\n// }\n\n// processPdfFiles();\n","export const basic_info_example_input = `\n    [{\n        \"element_id\": \"XXXX\",\n        \"metadata\": {\n        \"filename\": \"file.pdf\",\n        \"filetype\": \"application/pdf\",\n        \"languages\": [\"eng\"],\n        \"page_number\": 1,\n        \"parent_id\": \"XXXX\"\n        },\n        \"text\": \"Title of the paper in XYZ journal https://doi.org/XX.XXXX/XX.XXXX\",\n        \"type\": \"NarrativeText\"\n    },\n    {\n        \"element_id\": \"XXXX\",\n        \"metadata\": {\n        \"filename\": \"file.pdf\",\n        \"filetype\": \"application/pdf\",\n        \"languages\": [\"eng\"],\n        \"page_number\": 1,\n        \"parent_id\": \"XXXX\"\n        },\n        \"text\": \"AuthorX, AuthorY, AuthorZ\",\n        \"type\": \"NarrativeText\"\n    }]\n`;\n\nexport const basic_info_example_output = `\n{\n\"title\": \"Title of the paper\",\n\"authors\": [\"AuthorX\", \"AuthorY\", \"AuthorZ\"],\n\"abstract\": \"\",\n\"publication_date\": \"\",\n\"publisher\": \"\",\n\"volume\": \"\",\n\"issue\": \"\",\n\"page_numbers\": \"\",\n\"doi\": \"https://doi.org/XX.XXXX/XX.XXXX\",\n\"conflict_of_interest\": \"\",\n\"obi_details\": {\n    \"has_specified_output\": [\n        {\n        \"description\": \"Generated data on the ...\"\n        }\n    ],\n    \"instrument\": [\n        {\n        \"name\": \"Instrument X\",\n        \"description\": \"Description of Instrument X\"\n        }\n    ],\n    \"data_transformation\": [\n        {\n        \"name\": \"Example Transformation\",\n        \"description\": \"Transformation used in the experiment\"\n        }\n    ],\n    \"recruitment_status\": [\n        {\n        \"description\": \"Recruitment status of the experiment\"\n        }\n    ],\n    \"assay\": [\n        {\n        \"description\": \"Description of the assay used\"\n        }\n    ]\n}\n}\n`;\n\nexport const citations_example_input = `\nDoe John, Jane Doe, Paper title example 1, https://doi.org/random-doi-identifier1\nDoe Peter, Maria Doe, Paper title example 2, https://doi.org/random-doi-identifier2\nSmith Smith, Bob Smith, Paper title example 3, https://doi.org/random-doi-identifier3\n`;\n\nexport const citations_example_output = `\n\"citations\": \"Doe John, Paper title example 1 - https://doi.org/random-doi-identifier1\nDoe Peter, Paper title example 2 - https://doi.org/random-doi-identifier2\nSmith Smith, Paper title example 3 - https://doi.org/random-doi-identifier3\"\n`;\n\nexport const subgraph_go_example_input = `\n[\n{\n    \"text\": \"Description of a biological process involving X and Y.\"\n}\n]\n`;\n\nexport const subgraph_go_example_output = `\n[\n{\n    \"subject\": \"example biological term 1\", // Make sure to use the name of the subject, not the Gene Ontology ID (GO_...)\n    \"predicate\": \"example Gene Ontology relationship\",\n    \"object\": \"example biological term 1\", // Make sure to use the name of the object, not the Gene Ontology ID (GO_...)\n    \"explanation\": \"example explanation ...\"\n}\n]\n`;\n\nexport const subgraph_doid_example_input = `\n[\n{\n    \"text\": \"Description of Disease X with symptoms Y and Z.\"\n}\n]\n`;\n\nexport const subgraph_doid_example_output = `\n[\n{\n    \"disease\": \"Disease X\",\n    \"findings\": \"Disease X is characterized by symptoms Y and Z.\"\n}\n]\n`;\n\nexport const subgraph_chebi_example_input = `\n[\n{\n    \"text\": \"Description of Chemical Compound X.\"\n}\n]\n`;\n\nexport const subgraph_chebi_example_output = `\n[\n{\n    \"compound\": \"Chemical Compound X\",\n    \"findings\": \"Chemical Compound X is known for its properties Y and Z.\"\n}\n]\n`;\n\nexport const subgraph_atc_example_input = `\n[\n{\n    \"text\": \"Description of Drug X classified under ATC code Y.\"\n}\n]\n`;\n\nexport const subgraph_atc_example_output = `\n[\n{\n    \"drug\": \"Drug X\",\n    \"findings\": \"Drug X, classified under ATC code Y, is used for treating Z.\"\n}\n]\n`;\n\nexport const gene_ontology_example_input = `\n[\n    {\n        \"subject\": {\"term\": \"GO term example subject name\", \"id\": \"GO_XXXXXX\"},\n        \"predicate\": \"some of the gene ontology relationships\",\n        \"object\": {\"term\": \"GO term example object name\", \"id\": \"GO_XXXXXX\"},\n        \"explanation\": \"example explanation ...\"\n    }\n]\n`;\n\nexport const doid_ontology_example_input = `\n[\n    {\n        \"disease\": \"DOID_XXX\",\n        \"title\": \"Disease Y\",\n        \"findings\": \"Disease Y is a condition characterized by specific symptoms and causes. Patients with Disease Y often exhibit symptoms such as A, B, and C. Recent research indicates that genetic mutations in Gene1 and Gene2 may contribute to the pathogenesis of Disease Y.\"\n    }\n]\n`;\n\nexport const chebi_ontology_example_input = `\n[\n    {\n        \"compound_id\": \"CHEBI_XXXX\",\n        \"compound\": \"Chemical X\",\n        \"findings\": \"Chemical X (Formula) is a compound that at room temperature is characterized by certain properties. It is widely studied and is known for its ability to perform Function Y.\"\n    }\n]\n`;\n\nexport const example_basic_info = `\n    {\n    \"title\": \"Preliminary Study on X\",\n    \"authors\": [\"Author A\", \"Author B\"],\n    \"abstract\": \"This study investigates...\",\n    \"publication_date\": \"\",\n    \"publisher\": \"\",\n    \"volume\": \"\",\n    \"issue\": \"\",\n    \"page_numbers\": \"\",\n    \"doi\": \"\",\n    \"conflict_of_interest\": \"The authors are also employed by the organization which funded the research.\",\n    \"citations\": \"Author A, Author B. Paper title example 1. https://doi.org/XXXXX\n    Author C, Author D. Paper title example 2. https://doi.org/YYYYY\"\n}\n`;\n\nexport const example_spar_output = `\n    {\n        \"@context\": {\n            \"fabio\": \"http://purl.org/spar/fabio/\",\n            \"dcterms\": \"http://purl.org/dc/terms/\",\n            \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n            \"cito\": \"http://purl.org/spar/cito/\",\n            \"doco\": \"http://purl.org/spar/doco/\",\n            \"pro\": \"http://purl.org/spar/pro/\",\n            \"obi\": \"http://purl.obolibrary.org/obo/\",\n            \"schema\": \"http://schema.org/\"\n        },\n        \"@type\": \"fabio:ResearchPaper\",\n        \"dcterms:title\": \"Preliminary Study on X\",\n        \"dcterms:creator\": [\n            {\"@type\": \"foaf:Person\", \"foaf:name\": \"Dr. A\"},\n            {\"@type\": \"foaf:Person\", \"foaf:name\": \"Prof. B\"}\n        ],\n        \"dcterms:abstract\": \"This study investigates...\",\n        \"dcterms:date\": \"\", // Use the release date/date of issueing here in \"yyyy-mm-dd\" format. You shouldn't include the attribute in case it's not found or empty\n        \"dcterms:publisher\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"fabio:hasJournalVolume\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"fabio:hasJournalIssue\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"fabio:hasPageNumbers\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"dcterms:identifier\": \"https://doi.org/ID\", // Use the full DOI identifier here. You shouldn't include the attribute in case it's not found or empty\n        \"dcterms:rights\": \"\", // You shouldn't include the attribute in case it's not found or empty\n        \"doco:hasPart\": [], // You shouldn't include the attribute in case it's not found or empty\n        \"pro:roleIn\": [],  // You shouldn't include the attribute in case it's not found or empty\n        \"obi:OBI_0000968\": [ \n            {\n                \"foaf:name\": \"\", // Instrument used\n                \"dcterms:description\": \"\"\n            }\n        ],\n        \"obi:OBI_0200000\": [  \n            {\n                \"dcterms:description\": \"\" // Data Transformation, for example which programming language was used to sort the data\n            }\n        ],\n        \"obi:OBI_0000070\": [  \n            {\n                \"dcterms:description\": \"\" // Assay found in the science paper\n            }\n        ],\n        \"obi:OBI_0000251\": [  \n            {\n                \"dcterms:description\": \"\" // Recruitment status of the paper, e.g. how many participants in the experiment\n            }\n        ],\n        \"obi:IAO_0000616\": [  \n            {\n                \"dcterms:description\": \"\" // Conflict of interest of the authors\n            }\n        ]\n    }\n`;\n\nexport const example_go_output = `\n[\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/GO_XXXXX\", // Subject ID\n            \"dcterms:name\": \"Subject name\",\n            \"obi:RO_XXXXX\": {  /* Note for model: corresponds to a specific relationship */\n                \"@id\": \"http://purl.obolibrary.org/obo/GO_YYYYY\",  // Object ID\n                \"dcterms:description\": \"Process X positively regulates Process Y in cell type Z. This involves the modulation of Factor A and Factor B, affecting outcome C as indicated by the experimental results.\"\n                \"dcterms:name\": \"Object name\",\n            }\n        },\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/GO_AAAAA\", // Subject ID\n            \"obi:BFO_XXXXX\": {  /* Note for model: corresponds to a specific relationship */\n                \"@id\": \"http://purl.obolibrary.org/obo/GO_BBBBB\", // Object ID\n                \"dcterms:description\": \"Description of the process or relationship goes here.\"\n            }\n        }\n        /* Other GO entries would be similarly structured */\n]\n`;\n\nexport const example_doid_output = `\n[\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/CHEBI_XXXXXX\",\n            \"dcterms:title\": \"Chemical X\",                \n            \"dcterms:description\": \"Chemical X (Formula) is a compound with certain properties. It is widely studied and known for Function Y.\"\n        }\n        /* Other DOID entries would be similarly structured */\n]\n`;\n\nexport const example_chebi_output = `\n[\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/DOID_XXXXX\",\n            \"dcterms:title\": \"Compound Y\",                \n            \"dcterms:description\": \"Compound Y (Formula) is a chemical with properties A and B. It is essential in biological processes and known for characteristic C.\"\n        },\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/DOID_YYYYY\",\n            \"dcterms:title\": \"Disease X\",\n            \"dcterms:description\": \"Disease X is a disorder characterized by symptoms A, B, and C. It is linked to genetic factors such as mutations in Gene1, Gene2, and Gene3.\"\n        }\n        /* Other DOID entries would be similarly structured */\n]\n`;\n\nexport const example_json_citations = [\n  {\n    \"@id\": \"https://doi.org/10.1234/another-article\",\n    \"dcterms:title\": \"Related Work on Y\",\n  },\n  {\n    \"@id\": \"https://doi.org/10.5678/related-work\",\n    \"dcterms:title\": \"Further Discussion on Z\",\n  },\n];\n\nexport const example_graph = `\n{\n    \"@context\": {\n        \"fabio\": \"http://purl.org/spar/fabio/\",\n        \"dcterms\": \"http://purl.org/dc/terms/\",\n        \"foaf\": \"http://xmlns.com/foaf/0.1/\",\n        \"cito\": \"http://purl.org/spar/cito/\",\n        \"doco\": \"http://purl.org/spar/doco/\",\n        \"pro\": \"http://purl.org/spar/pro/\",\n        \"obi\": \"http://purl.obolibrary.org/obo/\"\n    },\n    \"@type\": \"fabio:ResearchPaper\",\n    \"dcterms:title\": \"KSR1 Knockout Mouse Model Demonstrates MAPK Pathway's Key Role in Cisplatin- and Noise-induced Hearing Loss\",\n    \"dcterms:creator\": [\n        {\n            \"@type\": \"foaf:Person\",\n            \"foaf:name\": \"Maria A. Ingersoll\"\n        },\n        {\n            \"@type\": \"foaf:Person\",\n            \"foaf:name\": \"Jie Zhang\"\n        },\n        {\n            \"@type\": \"foaf:Person\",\n            \"foaf:name\": \"Tal Teitz\"\n        }\n    ],\n    \"dcterms:abstract\": \"Hearing loss is a major disability in everyday life and therapeutic interventions to protect hearing would bene\\ufb01t a large portion of the world population. Here we found that mice devoid of the protein kinase suppressor of RAS 1 (KSR1) in their tissues (germline KO mice) exhibit resistance to both cisplatin- and noise-induced permanent hearing loss compared with their wild-type KSR1 litter- mates. KSR1 is a scaffold protein that brings in proximity the mitogen-activated protein kinase (MAPK) proteins BRAF, MEK1/2 and ERK1/2 and assists in their activation through a phosphorylation cascade induced by both cisplatin and noise insults in the cochlear cells. KSR1, BRAF, MEK1/2, and ERK1/2 are all ubiquitously expressed in the cochlea. Deleting the KSR1 protein tempered down the MAPK phosphorylation cascade in the cochlear cells following both cisplatin and noise insults and conferred hearing protection of up to 30 dB SPL in three tested frequencies in male and female mice. Treatment with dabrafenib, an FDA-approved oral BRAF inhibitor, protected male and female KSR1 wild-type mice from both cisplatin- and noise-induced hearing loss. Dabrafenib treatment did not enhance the protection of KO KSR1 mice, providing evidence dabrafenib works primarily through the MAPK pathway. Thus, either elimination of the KSR1 gene expression or drug inhibition of the MAPK cellular pathway in mice resulted in profound protection from both cisplatin- and noise-induced hearing loss. Inhibition of the MAPK pathway, a cel- lular pathway that responds to damage in the cochlear cells, can prove a valuable strategy to protect and treat hearing loss.\",\n    \"dcterms:date\": \"2024-03-21\",\n    \"dcterms:publisher\": \"Neurobiology of Disease\",\n    \"dcterms:identifier\": \"https://doi.org/10.1523/JNEUROSCI.2174-23.2024\",\n    \"dcterms:rights\": \"T.T. and J.Z. are inventors on a patent for the use of dabrafenib in hearing protection (US 2020-0093923 A1 and US Patent no 11,433,073, 18794717.1 / EP 3618807, Japan 2022-176126, China 201880029618.7) and are cofounders of Ting Therapeutics. All other authors declare that they have no competing \\ufb01nancial interests.\",\n    \"obi:OBI_0000299\": [\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/GO_0004672\",\n            \"obi:RO_0002335\": {\n                \"@id\": \"http://purl.obolibrary.org/obo/GO_0000165\",\n                \"dcterms:description\": \"Knockout of the KSR1 gene reduces activation of the MAPK signaling pathway, which is involved in cellular stress and death in response to cisplatin and noise exposure.\"\n            }\n        },\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/DOID_0050563\",\n            \"dcterms:title\": \"Noise-induced hearing loss\",\n            \"dcterms:description\": \"Genetic knockout of the KSR1 gene in mice confers resistance to noise-induced permanent hearing loss compared to wild-type littermates. KSR1 knockout mice had significantly less hearing loss, as demonstrated by auditory brainstem response, distortion product otoacoustic emission, outer hair cell counts, and synaptic puncta staining, compared to wild-type mice following noise exposure. Inhibition of the MAPK pathway, which includes BRAF, MEK, and ERK, was shown to be the mechanism by which KSR1 knockout mice were protected from noise-induced hearing loss.\"\n        },\n        {\n            \"@id\": \"http://purl.obolibrary.org/obo/CHEBI_75048\",\n            \"dcterms:title\": \"Dabrafenib\",\n            \"dcterms:description\": \"Dabrafenib is a BRAF inhibitor that protects against both cisplatin-induced and noise-induced hearing loss in mice by inhibiting the MAPK pathway. Dabrafenib is an FDA-approved drug, making it a promising compound to repurpose for the prevention of hearing loss.\"\n        },\n        {\n            \"@id\": \"http://purl.bioontology.org/ontology/ATC/L01XA01\",\n            \"dcterms:title\": \"Cisplatin\",\n            \"dcterms:description\": \"Cisplatin is a widely used chemotherapeutic agent that can cause permanent hearing loss as a side effect. Cisplatin-induced hearing loss is associated with activation of the MAPK pathway, which leads to cellular stress and damage in the inner ear. Genetic knockout of the KSR1 gene, which is involved in the MAPK pathway, conferred resistance to cisplatin-induced hearing loss in mice. Additionally, treatment with the BRAF inhibitor dabrafenib, which inhibits the MAPK pathway, also protected against cisplatin-induced hearing loss.\"\n        }\n    ],\n    \"obi:OBI_0000968\": [\n        {\n            \"foaf:name\": \"Not specified\",\n            \"dcterms:description\": \"Various instruments and equipment were used in this study, but specific details were not provided.\"\n        }\n    ],\n    \"obi:OBI_0000293\": [\n        {\n            \"dcterms:description\": \"Utilized the KSR1 knockout mouse model and wild-type littermates as the study subjects.\"\n        }\n    ],\n    \"obi:OBI_0200000\": [\n        {\n            \"dcterms:description\": \"Analyzed single-cell RNA sequencing data from postnatal day 28 C57BL/6 mice to examine the expression of MAPK genes in the cochlea.\"\n        },\n        {\n            \"dcterms:description\": \"Performed statistical analysis to compare hearing outcomes and MAPK signaling between KSR1 knockout and wild-type mice.\"\n        }\n    ],\n    \"obi:OBI_0000070\": [\n        {\n            \"dcterms:description\": \"Evaluated hearing function in mice using auditory assessments.\"\n        },\n        {\n            \"dcterms:description\": \"Measured MAPK pathway activation in the cochlea through biochemical assays.\"\n        }\n    ]\n}\n`;\n\nexport const incorrect_json_example = `\n[\n  {\n    \"@id\": \"https://doi.org/XXXXX\",\n    \"dcterms:title\": \"Title of Paper X\"\n  },\n  {\n    \"@id\": \"https://doi.org/YYYYY\",\n    \"dcterms:title\": \"Title of Paper Y\"\n  },\n  {\n    \"@id\": \"https://doi.org/ZZZZZ\",\n    \"dcterms:title\": \"Title of Paper Z\"\n  },\n  {\n    \"@id\": \"https://doi.org/AAAAA\",\n    \"dcterms:title\": \"Title of Paper A\"\n  },\n  {\n    \"@id\": \"https://doi.org/BBBBB\",\n    \"dcterms:title\": \"Title of Paper B\"\n  },\n  {\n    \"@id\": \"https://doi.org/CCCCC\",\n    \"dcterms:title\": \"Title of Paper C\"\n  },\n  {\n    \"@id\": \"https://doi.org/DDDDD\",\n    \"dcterms:title\": \"Title of Paper D\"\n  },\n  {\n    \"@id\": \"https://doi.org/EEEEE\",\n    \"dcterms:title\": \"Title of Paper E\"\n  },\n  {\n    \"@id\": \"https://doi.org/FFFFF\",\n    \"dcterms:title\": \"Title of Paper F\"\n  },\n  {\n    \"@id\": \"https://doi.org/\n`;\n\nexport const correct_json_example = `\n[\n  {\n    \"@id\": \"https://doi.org/XXXXX\",\n    \"dcterms:title\": \"Title of Paper X\"\n  },\n  {\n    \"@id\": \"https://doi.org/YYYYY\",\n    \"dcterms:title\": \"Title of Paper Y\"\n  },\n  {\n    \"@id\": \"https://doi.org/ZZZZZ\",\n    \"dcterms:title\": \"Title of Paper Z\"\n  },\n  {\n    \"@id\": \"https://doi.org/AAAAA\",\n    \"dcterms:title\": \"Title of Paper A\"\n  },\n  {\n    \"@id\": \"https://doi.org/BBBBB\",\n    \"dcterms:title\": \"Title of Paper B\"\n  },\n  {\n    \"@id\": \"https://doi.org/CCCCC\",\n    \"dcterms:title\": \"Title of Paper C\"\n  },\n  {\n    \"@id\": \"https://doi.org/DDDDD\",\n    \"dcterms:title\": \"Title of Paper D\"\n  },\n  {\n    \"@id\": \"https://doi.org/EEEEE\",\n    \"dcterms:title\": \"Title of Paper E\"\n  },\n  {\n    \"@id\": \"https://doi.org/FFFFF\",\n    \"dcterms:title\": \"Title of Paper F\"\n  }\n]\n`;\n","// prompts.ts\n\nimport {\n    basic_info_example_input,\n    basic_info_example_output,\n    citations_example_input,\n    citations_example_output,\n    subgraph_go_example_input,\n    subgraph_go_example_output,\n    subgraph_doid_example_input,\n    subgraph_doid_example_output,\n    subgraph_chebi_example_input,\n    subgraph_chebi_example_output,\n    subgraph_atc_example_input,\n    subgraph_atc_example_output,\n    example_basic_info,\n    example_spar_output,\n    example_go_output,\n    example_doid_output,\n    example_chebi_output,\n    gene_ontology_example_input,\n    doid_ontology_example_input,\n    chebi_ontology_example_input,\n    example_json_citations,\n    example_graph,\n    incorrect_json_example,\n    correct_json_example,\n} from \"./exampleForPrompts\";\n\n/**\n * Returns a prompt for choosing the most appropriate Gene Ontology (GO) term\n * from a list of GO candidates.\n */\nexport function get_go_api_prompt(term: string, go_candidates): string {\n    return `\n    Given the biological context, which of the following Gene Ontology (GO) terms best matches the description for '${term}'? Please select the most appropriate GO term or indicate if none apply by replying 'None'.\n\n    GO Candidates in JSON format: ${JSON.stringify(go_candidates)}\n\n    You must output the GO candidate which is the most suitable by replying with its id (e.g. 'GO_0043178'). If there are no suitable candidates output 'None'.\n    MAKE SURE TO ONLY OUTPUT THE MOST SUITABLE ID OR 'None'. THE ID MUST BE IN FORMAT \"GO_NUMBER\" - USE \"_\" ALWAYS. DO NOT OUTPUT ANYTHING ELSE\n    `;\n}\n\n/**\n * Returns a prompt for choosing the most appropriate Disease Ontology (DOID) term\n * from a list of DOID candidates.\n */\nexport function get_doid_api_prompt(term: string, doid_candidates): string {\n    return `\n    Given the biological context, which of the following Disease Ontology (DOID) terms best matches the description for '${term}'? Please select the most appropriate DOID term or indicate if none apply by replying 'None'.\n\n    DOID Candidates in JSON format: ${JSON.stringify(doid_candidates)}\n\n    You must output the DOID candidate which is the most suitable by replying with its id (e.g. 'DOID_14330'). If there are no suitable candidates output 'None'.\n    MAKE SURE TO ONLY OUTPUT THE MOST SUITABLE ID OR 'None'. THE ID MUST BE IN FORMAT \"DOID_NUMBER\" - USE \"_\" ALWAYS. DO NOT OUTPUT ANYTHING ELSE\n    `;\n}\n\n/**\n * Returns a prompt for choosing the most appropriate ChEBI term\n * from a list of ChEBI candidates.\n */\nexport function get_chebi_api_prompt(term: string, chebi_candidates): string {\n    return `\n    Given the biological context, which of the following Chemical Entities of Biological Interest (ChEBI) terms best matches the description for '${term}'? Please select the most appropriate ChEBI term or indicate if none apply by replying 'None'.\n\n    ChEBI Candidates in JSON format: ${JSON.stringify(chebi_candidates)}\n\n    You must output the ChEBI candidate which is the most suitable by replying with its id (e.g. 'CHEBI_15377'). If there are no suitable candidates output 'None'.\n    MAKE SURE TO ONLY OUTPUT THE MOST SUITABLE ID OR 'None'. THE ID MUST BE IN FORMAT \"CHEBI_NUMBER\" - USE \"_\" ALWAYS. DO NOT OUTPUT ANYTHING ELSE.\n    `;\n}\n\n/**\n * Returns a prompt for choosing the most appropriate ATC term\n * from a list of ATC candidates.\n */\nexport function get_atc_api_prompt(term: string, atc_candidates): string {\n    return `\n    Given the biological context, which of the following Anatomical Therapeutic Chemical (ATC) terms best matches the description for '${term}'? Please select the most appropriate ATC term or indicate if none apply by replying 'None'.\n\n    ATC Candidates in JSON format: ${JSON.stringify(atc_candidates)}\n\n    You must output the ATC candidate which is the most suitable by replying with its id (e.g. 'A14AA04'). If there are no suitable candidates, output 'None'.\n    MAKE SURE TO ONLY OUTPUT THE MOST SUITABLE ID OR 'None'.\n    `;\n}\n\n/**\n * Returns a prompt for extracting basic paper info (title, authors, abstract, etc.)\n * from an array of paper JSON chunks.\n */\nexport function get_prompt_basic_info(paper_array): string {\n    return `**Prompt**:\n    You are provided with chunks of a scientific paper in the form of JSON array elements, each containing parts of the paper such as potential titles, authors, and abstracts. Your task is to analyze these chunks incrementally to update and output the information listed below. If an element contains relevant information that improves upon or adds to the current data, update the respective fields; otherwise.\n\n    **Task**\n    1. Capture the title of the paper if a more accurate title is found in the chunk.\n    2. Identify the authors list, refining or appending to the current list based on the new information found in the chunk. MAKE SURE TO USE FULL NAMES OF THE AUTHORS AND INCLUDE THEM ALL!\n    3. Identify the abstract if more detailed or relevant information is provided in the chunk.\n    4. Identify the publication date if found in any of the chunks.\n    5. Identify the publisher or journal name if it can be extracted from the given data.\n    6. Identify the volume and issue number of the journal in which the paper is published.\n    7. Identify the page numbers that indicate where the paper is located within the journal.\n    8. Identify the DOI (Digital Object Identifier) which provides a persistent link to the paper's online location.\n    9. Capture key experimental details such as:\n        OBI_0000299 'has_specified_output': Describe the types of data or results produced by the research.\n        OBI_0000968 'instrument': Specify the instruments or equipment used in the research.\n        OBI_0000293 'has_specified_input': Identify inputs such as samples or data sets utilized in the study.\n        OBI_0200000 'data transformation': Explain any computational or analytical methods applied to raw data.\n        OBI_0000251 'recruitment status': For clinical studies, provide details on the status of participant recruitment.\n        OBI_0000070 'assay': Describe the specific assays used in the study to measure or observe biological, chemical, or physical processes, essential for validating the experimental hypothesis and ensuring reproducibility.\n        IAO_0000616 'conflict of interest': If there's a conflict of interest mentioned in the paper, describe it here.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${basic_info_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${basic_info_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO INCLUDE THE TITLE, FULL AUTHOR LIST WITH THEIR FULL NAMES, ABSTRACT AND ALL OTHER INFORMATION ABOUT THE PAPER IN A JSON OBJECT, DO NOT INCLUDE ANY EXPLANATIONS OR ADDITIONAL CONTENT **\n    `;\n}\n\n/**\n * Returns a prompt for extracting citation info from the last pages of a paper.\n */\nexport function get_prompt_citations(paper_array): string {\n    return `**Prompt**:\n    Analyze the provided chunks of the final few pages of a scientific paper formatted as JSON array elements. Each element contains potential citations, likely preceded by the term 'References'.\n\n    **Task**:\n    1. Carefully examine each citation to ensure that none are omitted. Every citation found in the input must be included.\n    2. Extract and return each citation, splitting each by a new line. Each citation should include the first author's name, the title and DOI URL identifier of the cited paper. \n    3. Confirm completeness by ensuring that every potential citation found in the input is included.\n\n    **Instructions**:\n    - Begin your examination from the section likely marked by 'References', as this is where citations typically start.\n    - Ensure completeness by including every citation identified in the input. Do not skip any citations.\n    - Output should consist only of the first author of the paper, the title and DOI URL of each citation, formatted as 'First author, title - DOI'.\n    - Provide the citations as a simple list, each on a new line, adhering strictly to the format provided below. Do not include any other comments or remarks.\n\n    **Example Input Format (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**:\n    ${citations_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**:\n    ${citations_example_output}\n\n    **Actual Input to analyze**:\n    ${JSON.stringify(paper_array)}\n\n    **Final instruction**\n    Provide me with the final output of citations, making sure to include the author and title of the cited paper and DOI in 'author, cited paper title - DOI' format, separating each citation by a new line.\n    `;\n}\n\n/**\n * Returns a prompt for extracting Gene Ontology (GO) relationships from a parsed paper.\n */\nexport function get_prompt_go_subgraph(paper_array): string {\n    return `**Prompt**:\n    You are provided with a parsed scientific paper in the form of a JSON array. Analyze this array to extract relationships using Gene Ontology (GO) terms and identifiers based on the scientific analysis conducted within the paper. Utilize only the recognized relationships in the Gene Ontology, which include: \"is_a\", \"part_of\", \"regulates\", \"positively_regulates\", \"negatively_regulates\", \"occurs_in\", \"capable_of\", \"capable_of_part_of\", \"has_part\", \"has_input\", \"has_output\", \"derives_from\", and \"derives_into\". Each extracted relationship should be accompanied by a brief explanation that clarifies the relationship within the context of the scientific findings.\n\n    Structure your response as a JSON array containing objects. Each object should have the following properties:\n\n    subject: The GO term or identifier that acts or is described.\n    predicate: The relationship from the Gene Ontology, only choosing from the following: \"is_a\", \"part_of\", \"regulates\", \"positively_regulates\", \"negatively_regulates\", \"occurs_in\", \"capable_of\", \"capable_of_part_of\", \"has_part\", \"has_input\", \"has_output\", \"derives_from\", and \"derives_into\".\n    object: The GO term or identifier that is acted upon or described.\n    explanation: A brief explanation of the relationship, indicating its relevance and context.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_go_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_go_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt. MAKE SURE TO ONLY CREATE GENE ONTOLOGY TERMS THAT ACTUAL EXIST AND ARE SUPPORTED BY THE ONTOLOGY. MAKE SURE TO ONLY USE THE RELATIONSHIPS THAT I PROVIDED.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY OF THE GENE ONTOLOGY IDENTIFIERS AND RELATIONSHIPS FROM THE ANALYZED PAPER. DO NOT ADD ANY ADDITIONAL REMARKS OR COMMENTS ASIDE OF THE JSON ARRAY. **\n    `;\n}\n\n/**\n * Returns a prompt for extracting disease relationships from a parsed paper\n * using DOID (Disease Ontology).\n */\nexport function get_prompt_doid_subgraph(paper_array): string {\n    return `**Prompt**:\n    You are provided with a parsed scientific paper in the form of a JSON array. Analyze this array to extract diseases and findings about them using Human Disease Ontology (DOID) terms and identifiers based on the scientific analysis conducted within the paper.\n\n    Structure your response as a JSON array containing objects. Each object should have the following properties:\n\n    disease: Name of the disease, or group of diseases that you extracted\n    findings: Description of the disease and findings in the paper about the disease or group of diseases.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_doid_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_doid_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt. MAKE SURE TO ONLY CREATE DOID ONTOLOGY TERMS AND FINDINGS ABOUT THEM THAT ACTUAL EXIST AND ARE SUPPORTED BY THE ONTOLOGY. MAKE SURE TO ONLY USE THE FORMAT THAT I PROVIDED.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY OF THE DOID DISEASE NAMES AND FINDINGS FROM THE ANALYZED PAPER. DO NOT ADD ANY ADDITIONAL REMARKS OR COMMENTS ASIDE OF THE JSON ARRAY. **\n    ** MAKE SURE TO ONLY ONLY DOUBLE QUOTES INSIDE OF THE JSON ARRAY, NOT SINGLE QUOTES **\n    `;\n}\n\n/**\n * Returns a prompt for extracting chemical compound relationships from a parsed paper\n * using ChEBI (Chemical Entities of Biological Interest).\n */\nexport function get_prompt_chebi_subgraph(paper_array): string {\n    return `**Prompt**:\n    You are provided with a parsed scientific paper in the form of a JSON array. Analyze this array to extract chemical compounds and findings about them using Chemical Entities of Biological Interest (ChEBI) terms and identifiers based on the scientific analysis conducted within the paper.\n\n    Structure your response as a JSON array containing objects. Each object should have the following properties:\n\n    compound: Name of the chemical compound, or group of compounds that you extracted\n    findings: Description of the compound and findings in the paper about the compound or group of compounds.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_chebi_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_chebi_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt. MAKE SURE TO ONLY CREATE ChEBI ONTOLOGY TERMS AND FINDINGS ABOUT THEM THAT ACTUALLY EXIST AND ARE SUPPORTED BY THE ONTOLOGY. MAKE SURE TO ONLY USE THE FORMAT THAT I PROVIDED.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY OF THE ChEBI COMPOUND NAMES AND FINDINGS FROM THE ANALYZED PAPER. DO NOT ADD ANY ADDITIONAL REMARKS OR COMMENTS ASIDE OF THE JSON ARRAY. **\n    ** MAKE SURE TO ONLY ONLY DOUBLE QUOTES INSIDE OF THE JSON ARRAY, NOT SINGLE QUOTES **\n     `;\n}\n\n/**\n * Returns a prompt for extracting medication relationships from a parsed paper\n * using the ATC (Anatomical Therapeutic Chemical) classification.\n */\nexport function get_prompt_atc_subgraph(paper_array): string {\n    return `**Prompt**:\n    You are provided with a parsed scientific paper in the form of a JSON array. Analyze this array to extract medications and findings about them using Anatomical Therapeutic Chemical (ATC) terms and identifiers based on the scientific analysis conducted within the paper.\n\n    Structure your response as a JSON array containing objects. Each object should have the following properties:\n\n    drug: Name of the medication, or group of medications that you extracted\n    findings: Description of the medication and findings in the paper about the medication or group of medications.\n\n    **Example Input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_atc_example_input}\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${subgraph_atc_example_output}\n\n    Proceed with the analysis based on the structure and instructions provided in this prompt. MAKE SURE TO ONLY CREATE ATC ONTOLOGY TERMS AND FINDINGS ABOUT THEM THAT ACTUALLY EXIST AND ARE SUPPORTED BY THE ONTOLOGY. MAKE SURE TO ONLY USE THE FORMAT THAT I PROVIDED.\n\n    **Actual paper array input**\n    ${JSON.stringify(paper_array)}\n\n    ** MAKE SURE TO ONLY ONLY DOUBLE QUOTES INSIDE OF THE JSON ARRAY, NOT SINGLE QUOTES **\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY OF THE ATC MEDICATION NAMES AND FINDINGS FROM THE ANALYZED PAPER. DO NOT ADD ANY ADDITIONAL REMARKS OR COMMENTS ASIDE OF THE JSON ARRAY. **\n    `;\n}\n\n/**\n * Returns a prompt to transform citation info into SPAR-compliant JSON-LD.\n */\nexport function get_prompt_spar_citations(citations: string): string {\n    return `\n    **Task**\n    Transform the provided citation information about a scientific paper into a JSON array of citations following the format I provide you.\n    One citation should be represented as one object, with an \"@id\" field which represents the DOI URL and \"dcterms:title\" field which represents the title, and only the title, author name can be removed in the \"dcterms:title\" field.\n\n    **Example Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    Here's an example of the JSON output object you should output. Pay specific attention that there are no authors named in the \"dcterms:title\" field:\n    ${example_json_citations}\n\n    **Actual Input**\n    ${citations}\n\n    **Final instruction**\n    Output the JSON array in the specified format and make sure to use double quotes (\") and not single quotes (') in the outputted JSON. Include only the DOI URLs and paper titles, all mentioned authors can be omitted. DO NOT INCLUDE ANY ADDITIONAL REMARKS OR COMMENTS, JUST THE JSON ARRAY.\n    `;\n}\n\n/**\n * Returns a prompt to transform basic paper info into a SPAR/OBI-based JSON-LD.\n */\nexport function get_prompt_spar_ontology(basic_info_text: string): string {\n    return `\n    ** Task: **\n    Transform the provided basic information about a scientific paper into a JSON-LD object using appropriate elements from the SPAR Ontologies. The input includes key metadata such as title, authors, abstract, and other publication details. Your task is to utilize the FaBiO, CiTO, DoCO, and PRO ontologies to create a rich, semantically detailed representation of the paper.\n\n    ** Input **\n    A JSON object with basic metadata about a scientific paper.\n\n    ** Output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT) **\n    A JSON-LD formatted object using SPAR Ontologies and OBI ontology to structure and link the provided information in a semantic web-friendly manner. Exclude attributes with no tangible values.\n\n    **Explanation of Key OBI Elements to Include:**\n    - **OBI:0000299 'has_specified_output'**: Use this to describe the types of data or results produced by the research.\n    - **OBI_0000968 'instrument'**: Detail the instruments or equipment employed in the research.\n    - **OBI_0000293 'has_specified_input'**: Identify inputs such as samples or data sets used in the study.\n    - **OBI_0200000 'data transformation'**: Describe any computational or analytical methods applied to raw data.\n    - **OBI:0000251 'recruitment status'**: Relevant for clinical studies, detail the status of participant recruitment.\n    - **OBI:0000070 'assay'**: Represents the specific assays used in the study to measure or observe biological, chemical, or physical processes. Assays are fundamental in validating the experimental hypothesis and are essential for the reproducibility of the results.\n    ** Note for OBI Elements ** MAKE SURE TO ONLY INCLUDE THE OBI ELEMENTS IF THEY ARE FOUND INSIDE THE SCIENCE PAPER. IF THEY ARE NOT, YOU CAN OMIT THEM.\n\n    ** Example Input JSON **\n    Basic paper info example: ${example_basic_info}\n\n    ** Note ** Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON-LD (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT) **\n    ${example_spar_output}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD.\n    Make sure to not include attributes in JSON LD which have no tangible values (e.g. )\n\n    **Explanation:**\n    - **@context:** Includes namespaces for a broader range of SPAR ontologies and the OBI ontology\n    - **@type:** Changed to \\`fabio:ResearchPaper\\` to better match academic publications.\n    - **DOI:** Use the dcterms:identifier field to include the DOI (Digital Object Identifier) that you found in the paper.\n    - **Metadata Fields:** Extended to potentially include roles and document components.\n    - **Use of PRO and DoCO:** Added placeholders for document parts (\\`doco:hasPart\\`) and roles (\\`pro:roleIn\\`).\n    - **Condition on Non-Empty Values:** Fields with empty strings, empty lists, or other unspecified values are not included in the output.\n    - **Flexibility in Attribute Selection:** While the example output provides a baseline, additional SPAR attributes should be considered and included if they provide further context or detail to the representation of the paper.\n\n    ** Actual Input JSON **\n    Basic paper info (SPAR & OBI Ontology): ${basic_info_text}\n\n    ** MAKE SURE TO ONLY OUTPUT THE JSON OBJECT WHICH REPRESENTS THE JSON LD REPRESENTATION OF THE PAPERS BASIC INFO. DO NOT INCLUDE ANY OTHER REMARKS - JUST THE JSON OBJECT. DO NOT INCLUDE ANY COMMENTS IN THE JSON OUTPUT (// notation) **\n    ** MAKE SURE TO ONLY INCLUDE OBI TERMS IN THE OUTPUT WHICH ARE INCLUDED IN THE BASIC PAPER INFO PASSED *\n    `;\n}\n\n/**\n * Returns a prompt to transform a GO subgraph into a JSON array using GO relationships.\n */\nexport function get_prompt_go_ontology(generated_go_subgraph): string {\n    return `\n    ** Task: **\n    Transform the provided basic information about a scientific paper into a JSON array using appropriate elements from the Gene Ontology (GO). The input includes Gene Ontology (GO) terms in a simple JSON format which you should transfer into an array format for an RDF graph. Your task is to utilize the GO ontology to create a rich, semantically detailed representation of terms and relationships described.\n\n    ** Input **\n    A JSON object with Gene Ontology terms and relationships from a scientific paper.\n\n    ** Output **\n    A JSON formatted array using Gene Ontology to structure and link the provided information in a semantic web-friendly manner.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    Gene Ontology input example: ${gene_ontology_example_input}\n\n    ** Note **\n    Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON-LD (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_go_output}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD.\n    Make sure to not include attributes in JSON LD which have no tangible values (e.g. )\n\n    **Explanation of how to build GO ontology array:**\n    Map the Gene Ontology relationships to their correspondands from the OBI (obi:) ontology.\n    enabled by <=> obi:RO_0002333\n    directly positively regulates <=> obi:RO_0002629\n    negatively regulated by <=> obi:RO_0002335\n    causally upstream of, positive effect <=> obi:RO_0002304\n    causally upstream of, negative effect <=> obi:RO_0002305\n    occurs in <=> obi:BFO_0000066\n    part of <=> obi:BFO_0000050\n    capable of <=> obi:RO_0002215\n    capable of part of <=> RO_0002216\n    has input <=> obi:RO_0002233\n    has output <=> obi:RO_0002234\n    derives from <=> obi:RO_0001000\n    derives into <=> obi:RO_0001001\n\n    ** Actual Input JSON **\n    Gene Ontology terms and relationships: ${JSON.stringify(\n        generated_go_subgraph,\n        null,\n        2\n    )}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data - the actual output should be in the same format of only the JSON array.\n    ** MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD. **\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY WHICH REPRESENTS THE GO TERMS - NO OTHER REMARKS OR COMMENTS SHOULD BE INCLUDED **\n    `;\n}\n\n/**\n * Returns a prompt to transform a DOID subgraph into a JSON array using DOID relationships.\n */\nexport function get_prompt_doid_ontology(generated_doid_subgraph): string {\n    return `\n    ** Task: **\n    Transform the provided basic information about a scientific paper into a JSON array using appropriate elements from the Disease Ontology (DOID). The input includes Disease Ontology (DOID) terms in a simple JSON format which you should transfer into an array format for an RDF graph. Your task is to utilize the DOID ontology to create a rich, semantically detailed representation of terms and relationships described.\n\n    ** Input **\n    A JSON object with DOID terms and findings about the disease from a scientific paper.\n\n    ** Output **\n    A JSON formatted array using DOID ontology to structure and link the provided information in a semantic web-friendly manner.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    Disease Ontology input example: ${doid_ontology_example_input}\n\n    ** Note **\n    Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON-LD (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_doid_output}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD.\n    Make sure to not include attributes in JSON LD which have no tangible values \n\n    **Explanation of how to build DOID ontology array:**\n    Map the provided \"disease_id\" to the \"@id\" field.\n    Map the provided \"disease\" field to the \"dcterms:title\" field.\n    Map the provided \"findings\" field to the \"dcterms:description\" field.\n\n    ** Actual Input JSON **\n    Disease ontology terms and relationships: ${JSON.stringify(\n        generated_doid_subgraph,\n        null,\n        2\n    )}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data - the actual output should be in the same format of only the JSON array.\n    ** MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD. **\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY WHICH REPRESENTS THE DOID DISEASES - NO OTHER REMARKS OR COMMENTS SHOULD BE INCLUDED **\n    `;\n}\n\n/**\n * Returns a prompt to transform a ChEBI subgraph into a JSON array using ChEBI relationships.\n */\nexport function get_prompt_chebi_ontology(generated_chebi_subgraph): string {\n    return `\n    ** Task: **\n    Transform the provided basic information about a scientific paper into a JSON array using appropriate elements from the Chemical Entities of Biological Interest (ChEBI). The input includes ChEBI terms in a simple JSON format which you should transfer into an array format for an RDF graph. Your task is to utilize the ChEBI ontology to create a rich, semantically detailed representation of terms and relationships described.\n\n    ** Input **\n    A JSON object with ChEBI terms and findings about the chemical compound from a scientific paper.\n\n    ** Output **\n    A JSON formatted array using ChEBI ontology to structure and link the provided information in a semantic web-friendly manner.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ChEBI input example: ${chebi_ontology_example_input}\n\n    ** Note **\n    Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON-LD (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_chebi_output}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD.\n    Make sure to not include attributes in JSON LD which have no tangible values \n\n    **Explanation of how to build ChEBI ontology array:**\n    Map the provided \"compound_id\" to the \"@id\" field.\n    Map the provided \"compound\" field to the \"dcterms:title\" field.\n    Map the provided \"findings\" field to the \"dcterms:description\" field.\n\n    ** Actual Input JSON **\n    Disease ontology terms and relationships: ${JSON.stringify(\n        generated_chebi_subgraph,\n        null,\n        2\n    )}\n\n    ** Note **\n    The example output is provided to you so you can understand the format of the data - the actual output should be in the same format of only the JSON array.\n    ** MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE DATA FROM THE PAPER INSTEAD. **\n    ** MAKE SURE TO ONLY OUTPUT THE JSON ARRAY WHICH REPRESENTS THE DOID DISEASES - NO OTHER REMARKS OR COMMENTS SHOULD BE INCLUDED **\n    `;\n}\n\n/**\n * Returns a prompt asking for start and stop pages of specified sections\n * given an array of page text data.\n */\nexport function get_prompt_section_page_numbers(\n    paper_array,\n    sections: string[]\n): string {\n    let prompt = `Given the following pages of a research paper, identify the start and stop pages for each one of the provided sections\\n\\n`;\n\n    paper_array.forEach((element) => {\n        const pageNumber = element.metadata?.page_number;\n        const text = element.text;\n        prompt += `Page ${pageNumber}:\\n${text}\\n\\n`;\n    });\n\n    prompt += `Please provide the start and stop pages for each section in the following format:\n    \n    ** Example input (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    Introduction, abstract\n\n    ** Example output (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**:\n    Introduction 1, 2\n    Abstract, 4, 9\n\n    ** Actual input **\n    ${sections.join(\", \")}\n\n    ** Your output **\n    ${JSON.stringify(\n        sections.map((section) => `${section}, start, end`),\n        null,\n        2\n    )}\n\n    OUTPUT ONLY THE SECTIONS AND PAGE NUMBERS IN THE EXAMPLE FORMAT, ONLY FOR THE SECTIONS FROM THE INPUT. DO NOT CONSIDER OTHER SECTIONS OR ADD ANY OTHER COMMENTS, EXPLANATIONS ETC. \n    `;\n    return prompt;\n}\n\n/**\n * Returns a prompt for generating a textual summary for similarity search\n * from an RDF JSON-LD graph.\n */\nexport function get_prompt_vectorization_summary(graph): string {\n    // Shallow clone of the graph\n    const graphCopy = JSON.parse(JSON.stringify(graph));\n    if (graphCopy[\"cito:cites\"]) {\n        delete graphCopy[\"cito:cites\"];\n    }\n\n    return `\n    ** Task: **\n    Generate a comprehensive textual summary based on the provided RDF JSON-LD graph. The summary should include as much information as possible that would be useful for similarity search.\n\n    ** Input **\n\n    An RDF JSON-LD graph that contains various nodes and relationships.\n\n    ** Output **\n\n    A detailed textual summary that captures the key information, entities, and relationships in the graph, formatted in a way that maximizes its utility for similarity search.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_graph}\n\n    ** Example Output summary **\n\n    The research paper titled \"KSR1 Knockout Mouse Model Demonstrates MAPK Pathway's Key Role in Cisplatin- and Noise-induced Hearing Loss\" was authored by Maria A. Ingersoll, Jie Zhang, and Tal Teitz and published on March 21, 2024, in the Neurobiology of Disease. This study investigates the impact of the KSR1 gene on hearing loss. The abstract reveals that knockout mice devoid of the KSR1 protein exhibit resistance to cisplatin- and noise-induced permanent hearing loss compared to their wild-type counterparts. The KSR1 protein acts as a scaffold bringing MAPK pathway proteins (BRAF, MEK1/2, ERK1/2) in proximity for activation through phosphorylation cascades triggered by cisplatin and noise in cochlear cells. The knockout of KSR1 significantly reduces MAPK activation, thereby conferring hearing protection.\n\n    Key findings include the role of MAPK pathway inhibition in providing hearing protection, with dabrafenib (a BRAF inhibitor) effectively protecting KSR1 wild-type mice from hearing loss without additional benefits in KSR1 knockout mice. These findings suggest that dabrafenib primarily works through the MAPK pathway.\n\n    Further details include the involvement of several key entities:\n    - GO_0004672: Knockout of the KSR1 gene reduces activation of the MAPK signaling pathway, which is involved in cellular stress and death in response to cisplatin and noise exposure.\n    - DOID_0050563: Genetic knockout of the KSR1 gene in mice confers resistance to noise-induced permanent hearing loss compared to wild-type littermates. KSR1 knockout mice had significantly less hearing loss, as demonstrated by auditory brainstem response, distortion product otoacoustic emission, outer hair cell counts, and synaptic puncta staining, compared to wild-type mice following noise exposure. Inhibition of the MAPK pathway, which includes BRAF, MEK, and ERK, was shown to be the mechanism by which KSR1 knockout mice were protected from noise-induced hearing loss.\n    - CHEBI_75048: Dabrafenib is a BRAF inhibitor that protects against both cisplatin-induced and noise-induced hearing loss in mice by inhibiting the MAPK pathway. Dabrafenib is an FDA-approved drug, making it a promising compound to repurpose for the prevention of hearing loss.\n    - L01XA01: Cisplatin is a widely used chemotherapeutic agent that can cause permanent hearing loss as a side effect. Cisplatin-induced hearing loss is associated with activation of the MAPK pathway, which leads to cellular stress and damage in the inner ear. Genetic knockout of the KSR1 gene, which is involved in the MAPK pathway, conferred resistance to cisplatin-induced hearing loss in mice. Additionally, treatment with the BRAF inhibitor dabrafenib, which inhibits the MAPK pathway, also protected against cisplatin-induced hearing loss.\n\n    The study utilized various instruments and equipment (details not specified), and included the KSR1 knockout mouse model and wild-type littermates as subjects. Analytical methods involved single-cell RNA sequencing data from postnatal day 28 C57BL/6 mice to examine the expression of MAPK genes in the cochlea, and statistical analysis to compare hearing outcomes and MAPK signaling between KSR1 knockout and wild-type mice. Hearing function was evaluated using auditory assessments, and MAPK pathway activation in the cochlea was measured through biochemical assays.\n\n\n    ** Notes **\n    1. DO NOT USE ANY SPECIAL CHARACTERS IN THE SUMMARY. eg. :, \", newlines, etc.\n    2. Ensure the summary captures all key entities and relationships present in the RDF JSON-LD graph.\n    3. The summary should be formatted in a way that makes it easy to use for similarity search purposes - make sure to use specific term names and not pronouns such as \"it\", \"he\", \"they\".\n    4. Your output should be only the generated summary. No other comments or remarks will be tolerated.\n    ** Actual Input JSON **\n\n    ${JSON.stringify(graphCopy, null, 4)}\n    \n    ** FINAL NOTE - MAKE SURE TO ONLY OUTPUT THE GENERATED SUMMARY, WITHOUT EXTRA COMMENTS OR REMARKS **\n    `;\n}\n\n/**\n * Returns a prompt to convert incorrectly formatted text into valid JSON.\n */\nexport function get_prompt_convert_to_json(incorrect_json: string): string {\n    return `\n    ** Task: **\n    Convert the following incorrectly formatted text into a valid JSON format. Ensure that any incomplete or cut-off elements are properly removed, and the resulting JSON structure is correct.\n\n    ** Input: **\n    The incorrectly formatted text that is supposed to be JSON but has errors or missing/extra elements.\n\n    ** Output: **\n    A valid JSON structure with all elements properly formatted and completed. If the JSON array is too long and gets cut off, delete the last incomplete element and properly close the array.\n\n    ** Example Input JSON: **\n    ${incorrect_json_example}\n\n    ** Note: **\n    Make sure not to blindly copy from the input example. It is just presented to you so you can understand the format of the data.\n\n    ** Example Output JSON: **\n    ${correct_json_example}\n\n    ** Note: **\n    The example output is provided to you so you can understand the format of the data. MAKE SURE TO NOT BLINDLY COPY DATA FROM THE OUTPUT AND ACTUALLY USE THE PROVIDED DATA INSTEAD.\n\n    ** Actual Input JSON: **\n    ${incorrect_json}\n\n    ** Important Notes: **\n    1. THE MOST IMPORTANT THING IS THAT THE JSON MUST BE CORRECT. IT IS OKAY TO REMOVE SOME OF THE CONTENT IN ORDER TO MAKE THE JSON FORMATTED WELL.\n    2. REMOVE ANY INCOMPLETE ELEMENTS FROM THE JSON ARRAY AND PROPERLY CLOSE THE ARRAY.\n    3. MAKE SURE TO ONLY OUTPUT THE JSON OBJECT, DO NOT INCLUDE ANY OTHER REMARKS OR COMMENTS.\n    `;\n}\n\n/**\n * Returns a prompt to generate three suggested research questions\n * based on the given RDF JSON-LD graph.\n */\nexport function get_prompt_suggested_questions(graph): string {\n    const graphCopy = JSON.parse(JSON.stringify(graph));\n    if (graphCopy[\"cito:cites\"]) {\n        delete graphCopy[\"cito:cites\"];\n    }\n\n    return `\n    ** Task: **\n    Generate three suggested research questions based on the provided RDF JSON-LD graph. The questions should be related to the key entities, assets, or topics in the graph and should be useful for exploring the content further.\n\n    ** Input **\n\n    An RDF JSON-LD graph that contains various nodes and relationships.\n\n    ** Output **\n\n    Three research questions related to the graph's entities and relationships. The questions should be designed to assist in exploring related assets (such as papers, entities, or authors) and can be based on the scientific content of the graph.\n\n    ** Example Input JSON (ONLY AN EXAMPLE, DO NOT COPY DATA FROM HERE FOR ACTUAL OUTPUT)**\n    ${example_graph}\n\n    ** Example Suggested Questions **\n\n    1. List all the science papers related to Alzheimer disease.\n    2. What can you tell me about Cisplatin from the recent science papers?\n    3. List me the authors of paper \"KIS kinase controls alternative splicing during neuronal differentiation\".\n\n    ** Example Output (Make sure to follow this exact format, with no additional text): **\n\n    List all the science papers related to Alzheimer disease.\n    What can you tell me about Cisplatin from the recent science papers?\n    List me the authors of paper \"KIS kinase controls alternative splicing during neuronal differentiation\".\n\n    ** Notes **\n    1. Ensure the questions are specific, well-formed, and directly related to the entities or relationships present in the RDF JSON-LD graph.\n    2. The questions should be in a format that can be easily understood and processed for further querying or retrieval tasks.\n    3. Your output should be the three suggested questions, each on its own line, without extra comments or remarks. Do not include any labels like 'Question:'.\n\n    ** Actual Input JSON **\n\n    ${JSON.stringify(graphCopy, null, 4)}\n\n    ** FINAL NOTE - MAKE SURE TO OUTPUT ONLY THE THREE QUESTIONS WITHOUT EXTRA COMMENTS OR REMARKS **\n    `;\n}\n\nexport const categorizeIntoDAOsPrompt = `JUST RETURN THE ARRAY OF DAO NAMES RELEVANT TO THE PAPER AND ANSWER VERY CONCISE AND TO THE POINT\n<dao_list>\nVitaDAO  Longevity, anti-aging, age-related diseases\nAthenaDAO  Women's health, reproductive health, gynecological research\nPsyDAO  Psychedelic science, mental health, psychedelic-assisted therapy\nValleyDAO  Synthetic biology, environmental biotech, climate solutions\nHairDAO  Hair loss treatment, regenerative medicine, dermatology\nCryoDAO  Cryopreservation, biostasis, organ/brain freezing technologies\nCerebrum DAO  Brain health, neurodegeneration, Alzheimer's research\nCuretopia  Rare disease research, genetic disorders, orphan drug discovery\nLong COVID Labs  Long COVID, post-viral syndromes, chronic illness research\nQuantum Biology DAO  Quantum biology, biophysics, quantum effects in biology\n</dao_list>\n\nReturn your output **only** as a JSON array of DAO names. If no DAOs are relevant, return an empty array.\n\nExample output format:\n[\"DAO1\", \"DAO2\", \"DAO3\"]\nor\n[]`;\n","// searchOntology.ts\n\nimport { logger } from \"@elizaos/core\";\nimport axios, { AxiosResponse } from \"axios\";\nimport \"dotenv/config\";\nimport { generateResponse } from \"./anthropicClient\";\nimport {\n  get_go_api_prompt,\n  get_doid_api_prompt,\n  get_chebi_api_prompt,\n  get_atc_api_prompt,\n} from \"./llmPrompt\";\nimport Anthropic from \"@anthropic-ai/sdk\";\n\nconst bioontologyApiKey: string | undefined = process.env.BIONTOLOGY_KEY;\n\n/**\n * Extract the last part of an ATC URL as the ATC ID.\n * E.g., if the URL ends with \"/A12BC51\", returns \"A12BC51\".\n */\nexport function extractAtcId(url: string): string | null {\n  const match = url.match(/\\/([^/]+)$/);\n  return match ? match[1] : null;\n}\n\n/**\n * Search for a term in the Gene Ontology via QuickGO API.\n */\nexport async function searchGo(\n  term: string,\n  client: Anthropic,\n  modelIdentifier: string = \"claude-3-haiku-20240307\"\n): Promise<string> {\n  const url = \"https://www.ebi.ac.uk/QuickGO/services/ontology/go/search\";\n  const params = { query: term, limit: 5, page: 1 };\n  const headers = { Accept: \"application/json\" };\n\n  let newTerm = \"None\";\n  try {\n    const apiResponse: AxiosResponse = await axios.get(url, {\n      headers,\n      params,\n    });\n    if (apiResponse.status === 200) {\n      const goCandidates = apiResponse.data.results?.slice(0, 4) || [];\n      const promptGoApi = get_go_api_prompt(term, goCandidates);\n\n      newTerm = await generateResponse(client, promptGoApi, modelIdentifier);\n\n      // Replace \"GO:\" with \"GO_\"\n      if (newTerm.includes(\"GO:\")) {\n        newTerm = newTerm.replace(\"GO:\", \"GO_\");\n      }\n      logger.info(`new term: ${newTerm}, old term: ${term}`);\n    } else {\n      logger.info(`EBI API gave response code ${apiResponse.status}`);\n    }\n  } catch (error) {\n    logger.error(`Error generating response: ${error}`);\n  }\n\n  return newTerm;\n}\n\n/**\n * Search for a term in the DOID Ontology via EBI API.\n */\nexport async function searchDoid(\n  term: string,\n  client: Anthropic,\n  modelIdentifier: string = \"claude-3-haiku-20240307\"\n): Promise<string> {\n  const url = \"https://www.ebi.ac.uk/ols/api/search\";\n  const params = {\n    q: term,\n    ontology: \"doid\",\n  };\n  const headers = { Accept: \"application/json\" };\n\n  let newTerm = \"None\";\n  try {\n    const apiResponse: AxiosResponse = await axios.get(url, {\n      headers,\n      params,\n    });\n\n    if (apiResponse.status === 200) {\n      const data = apiResponse.data;\n      const found = data.response?.numFound || 0;\n      const doidCandidates =\n        found > 0\n          ? data.response.docs.slice(0, 4).map((candidate) => ({\n              short_form: candidate.short_form,\n              description: candidate.description,\n              label: candidate.label,\n            }))\n          : [];\n\n      const promptDoidApi = get_doid_api_prompt(term, doidCandidates);\n      newTerm = await generateResponse(client, promptDoidApi, modelIdentifier);\n\n      // Replace \"DOID:\" with \"DOID_\"\n      if (newTerm.includes(\"DOID:\")) {\n        newTerm = newTerm.replace(\"DOID:\", \"DOID_\");\n      }\n      logger.info(`new term: ${newTerm}, old term: ${term}`);\n    } else {\n      logger.error(`EBI API gave response code ${apiResponse.status}`);\n    }\n  } catch (error) {\n    logger.error(`Error generating response: ${error}`);\n  }\n\n  return newTerm;\n}\n\n/**\n * Search for a term in the ChEBI Ontology via EBI API.\n */\nexport async function searchChebi(\n  term: string,\n  client: Anthropic,\n  modelIdentifier: string = \"claude-3-haiku-20240307\"\n): Promise<string> {\n  const url = \"https://www.ebi.ac.uk/ols/api/search\";\n  const params = {\n    q: term,\n    ontology: \"chebi\",\n  };\n  const headers = { Accept: \"application/json\" };\n\n  let newTerm = \"None\";\n  try {\n    const apiResponse: AxiosResponse = await axios.get(url, {\n      headers,\n      params,\n    });\n\n    if (apiResponse.status === 200) {\n      const data = apiResponse.data;\n      const found = data.response?.numFound || 0;\n      const chebiCandidates =\n        found > 0\n          ? data.response.docs.slice(0, 4).map((candidate) => ({\n              short_form: candidate.short_form,\n              description: candidate.description,\n              label: candidate.label,\n            }))\n          : [];\n\n      const promptChebiApi = get_chebi_api_prompt(term, chebiCandidates);\n      newTerm = await generateResponse(client, promptChebiApi, modelIdentifier);\n\n      // Replace \"CHEBI:\" with \"CHEBI_\"\n      if (newTerm.includes(\"CHEBI:\")) {\n        newTerm = newTerm.replace(\"CHEBI:\", \"CHEBI_\");\n      }\n      logger.info(`new term: ${newTerm}, old term: ${term}`);\n    } else {\n      logger.error(`EBI API gave response code ${apiResponse.status}`);\n    }\n  } catch (error) {\n    logger.error(`Error generating response: ${error}`);\n  }\n\n  return newTerm;\n}\n\n/**\n * Search for a term in the ATC Ontology via BioOntology API.\n */\nexport async function searchAtc(\n  term: string,\n  client: Anthropic,\n  modelIdentifier: string = \"claude-3-haiku-20240307\"\n): Promise<string> {\n  const url = \"https://data.bioontology.org/search\";\n  const params = {\n    q: term,\n    ontologies: \"ATC\",\n    apikey: bioontologyApiKey,\n  };\n  const headers = { Accept: \"application/json\" };\n\n  let newTerm = \"None\";\n  try {\n    const apiResponse: AxiosResponse = await axios.get(url, {\n      headers,\n      params,\n    });\n\n    if (apiResponse.status === 200) {\n      const data = apiResponse.data;\n      let atcCandidates = [];\n      if (data.collection && data.collection.length > 0) {\n        atcCandidates = data.collection.map((candidate) => ({\n          short_form: extractAtcId(candidate[\"@id\"]),\n          description: \"\",\n          label: candidate[\"prefLabel\"],\n        }));\n      }\n      const promptAtcApi = get_atc_api_prompt(term, atcCandidates);\n      newTerm = await generateResponse(client, promptAtcApi, modelIdentifier);\n\n      logger.info(`new term: ${newTerm}, old term: ${term}`);\n    } else {\n      logger.error(`ATC API gave response code ${apiResponse.status}`);\n    }\n  } catch (error) {\n    logger.error(`Error generating response: ${error}`);\n  }\n\n  return newTerm;\n}\n\n/**\n * Update subject and object fields in GO data with best-matching GO terms.\n */\nexport async function updateGoTerms(data, client: Anthropic) {\n  for (const entry of data) {\n    const subjectResult = await searchGo(entry.subject, client);\n    entry.subject = { term: entry.subject, id: subjectResult };\n\n    const objectResult = await searchGo(entry.object, client);\n    entry.object = { term: entry.object, id: objectResult };\n  }\n\n  return data.filter(\n    (entry) => entry.subject !== \"None\" && entry.object !== \"None\"\n  );\n}\n\n/**\n * Update disease fields in DOID data with best-matching DOID terms.\n */\nexport async function updateDoidTerms(data, client: Anthropic) {\n  for (const entry of data) {\n    const diseaseResult = await searchDoid(entry.disease, client);\n    entry.disease_id = diseaseResult;\n  }\n\n  return data.filter((entry) => entry.disease_id !== \"None\");\n}\n\n/**\n * Update compound fields in ChEBI data with best-matching ChEBI terms.\n */\nexport async function updateChebiTerms(data, client: Anthropic) {\n  for (const entry of data) {\n    const compoundResult = await searchChebi(entry.compound, client);\n    entry.compound_id = compoundResult;\n  }\n\n  return data.filter((entry) => entry.compound_id !== \"None\");\n}\n\n/**\n * Update drug fields in ATC data with best-matching ATC terms.\n */\nexport async function updateAtcTerms(data, client: Anthropic) {\n  for (const entry of data) {\n    const drugResult = await searchAtc(entry.drug, client);\n    entry.drug_id = drugResult;\n  }\n\n  return data.filter((entry) => entry.drug_id !== \"None\");\n}\n","// regexUtils.ts\n\n/**\n * Extracts the first bracketed substring (including the brackets).\n * Equivalent to Python's `re.search(r\"\\[.*\\]\", string, re.DOTALL)`.\n *\n * @param input - The string to search\n * @returns The bracketed substring (e.g., \"[something]\") or null if none found\n */\nexport function extractBracketContent(input: string): string | null {\n    // Use a dot-all style pattern in JavaScript: [\\s\\S] matches any character including newlines\n    // The parentheses are optional if you only want the entire match in group(0)\n    const match = input.match(/\\[([\\s\\S]*?)\\]/);\n    return match ? match[0] : null;\n}\n\n/**\n * Checks if a string (after trimming) is exactly \"[]\".\n * Equivalent to Python's `return string.strip() == \"[]\"`.\n *\n * @param input - The string to check\n * @returns True if the trimmed string is \"[]\", false otherwise\n */\nexport function isEmptyArray(input: string): boolean {\n    return input.trim() === \"[]\";\n}\n\n/**\n * Replaces certain single quotes with double quotes, mimicking the Python regex:\n *  re.sub(r\"(?<!\\\\)'(?=[^:]+?')|(?<=: )'(?=[^']+?')\", '\"', input_string)\n *\n * IMPORTANT: This relies on lookbehind assertions which require a modern JavaScript/TypeScript runtime.\n * If your environment doesn't support lookbehinds, you must rewrite this regex.\n *\n * @param input - The string with possible single quotes to replace\n * @returns The corrected string with targeted single quotes replaced by double quotes\n */\nexport function convertToValidJsonString(input: string): string {\n    // Pattern breakdown:\n    //  1) (?<!\\\\)'(?=[^:]+?')\n    //      Match `'` not preceded by a backslash, followed by any chars up until another `'`, but not containing ':' in that span.\n    //  2) (?<=: )'(?=[^']+?')\n    //      Match `'` that is preceded by ': ', followed by chars up to the next `'`.\n    //\n    // Both are replaced with `\"`.\n    const pattern = /(?<!\\\\)'(?=[^:]+?')|(?<=: )'(?=[^']+?')/g;\n    return input.replace(pattern, '\"');\n}\n","import {\n  get_prompt_basic_info,\n  get_prompt_citations,\n  get_prompt_go_subgraph,\n  get_prompt_doid_subgraph,\n  get_prompt_chebi_subgraph,\n  get_prompt_atc_subgraph,\n  get_prompt_spar_ontology,\n  get_prompt_spar_citations,\n  get_prompt_section_page_numbers,\n  get_prompt_go_ontology,\n  get_prompt_suggested_questions,\n} from \"./llmPrompt\";\n\nimport { generateResponse, getClient } from \"./anthropicClient\";\nimport {\n  updateGoTerms,\n  updateDoidTerms,\n  updateChebiTerms,\n  updateAtcTerms,\n} from \"./biologyApi\";\n\nimport { extractBracketContent, isEmptyArray } from \"./regex\";\nimport { logger } from \"@elizaos/core\";\nimport Anthropic from \"@anthropic-ai/sdk\";\n\n/** Generic JSON-like type for representing arbitrary structures. */\ntype JSONValue =\n  | string\n  | number\n  | boolean\n  | { [x: string]: JSONValue }\n  | JSONValue[];\n\n/** One page's text and metadata. */\ninterface PaperArrayElement {\n  metadata: {\n    page_number: number;\n  };\n  text: string;\n}\n\n/** Object storing arrays of text for each major section. */\ninterface PaperDict {\n  introduction: string[];\n  abstract: string[];\n  methods: string[];\n  results: string[];\n  discussion: string[];\n  citations: string[];\n}\n\n/** Key-value for label => [startPage, endPage]. */\ninterface LabelPageRanges {\n  [label: string]: [number, number];\n}\n\n/** Subgraph object typically returned from the extraction steps. */\ninterface SubgraphSet {\n  go: string;\n  doid: string;\n  chebi: string;\n  atc: string;\n}\n\n/** Result of processPaper: a 6-tuple of strings. */\ntype PaperProcessResult = [string, string, string, string, string, string];\n\nconst CITATIONS_OFFSET = 6;\n\n/**\n * Extract page ranges for known sections (Abstract, Introduction, Methods, etc.).\n */\nexport async function extractSections(\n  client: Anthropic,\n  paper_array: PaperArrayElement[]\n): Promise<LabelPageRanges> {\n  const originalLabels = [\n    \"Abstract\",\n    \"Introduction\",\n    \"Methods\",\n    \"Materials and methods\",\n    \"Material and methods\",\n    \"Results\",\n    \"Discussion\",\n  ];\n\n  const labels = originalLabels.map((label) => label.toLowerCase());\n  const label_page_numbers: Record<string, number[]> = {};\n  const label_mapping: Record<string, string> = {};\n\n  for (const label of labels) {\n    label_page_numbers[label] = [];\n    const originalLabel = originalLabels.find(\n      (orig) => orig.toLowerCase() === label\n    );\n    label_mapping[label] = originalLabel ? originalLabel : label;\n  }\n\n  for (const element of paper_array) {\n    const page_number = element.metadata.page_number;\n    const textLower = element.text.toLowerCase();\n\n    for (const label of labels) {\n      if (textLower.includes(label)) {\n        // Skip table of contents on page 1 for all but \"Abstract\" and \"Introduction\"\n        if (\n          page_number === 1 &&\n          label !== \"abstract\" &&\n          label !== \"introduction\"\n        ) {\n          continue;\n        }\n        label_page_numbers[label].push(page_number);\n      }\n    }\n  }\n\n  // If \"methods\" array is empty, try \"materials and methods\" or \"material and methods\"\n  const methodAliases = [\"materials and methods\", \"material and methods\"];\n  if (label_page_numbers[\"methods\"].length === 0) {\n    for (const alias of methodAliases) {\n      if (label_page_numbers[alias] && label_page_numbers[alias].length > 0) {\n        label_page_numbers[\"methods\"] = label_page_numbers[alias];\n        break;\n      }\n    }\n  }\n\n  // Remove the separate method aliases from the dictionary\n  for (const alias of methodAliases) {\n    if (alias in label_page_numbers) {\n      delete label_page_numbers[alias];\n      const idx = labels.indexOf(alias);\n      if (idx >= 0) {\n        labels.splice(idx, 1);\n      }\n      const origAlias = label_mapping[alias];\n      const origAliasIdx = originalLabels.indexOf(origAlias);\n      if (origAliasIdx >= 0) {\n        originalLabels.splice(origAliasIdx, 1);\n      }\n    }\n  }\n\n  // Map each label => first page or undefined\n  const first_appearance: Record<string, number | undefined> = {};\n  for (const [label, pages] of Object.entries(label_page_numbers)) {\n    first_appearance[label] = pages.length > 0 ? pages[0] : undefined;\n  }\n  // Remove undefined\n  for (const key of Object.keys(first_appearance)) {\n    if (first_appearance[key] === undefined) {\n      delete first_appearance[key];\n    }\n  }\n\n  const sorted_labels = Object.entries(first_appearance).sort((a, b) => {\n    const aVal = a[1] === undefined ? Infinity : a[1];\n    const bVal = b[1] === undefined ? Infinity : b[1];\n    return (aVal as number) - (bVal as number);\n  });\n\n  const label_page_ranges: LabelPageRanges = {};\n\n  // Build up the start/stop pages for each discovered label\n  for (let i = 0; i < sorted_labels.length; i++) {\n    const [label, startPageRaw] = sorted_labels[i];\n    if (startPageRaw == null) continue;\n    let start_page = startPageRaw;\n    let end_page: number;\n\n    // If first label, set to page 1\n    if (i === 0) {\n      start_page = 1;\n    }\n\n    // If last label => end on final page\n    if (i === sorted_labels.length - 1) {\n      end_page = paper_array[paper_array.length - 1].metadata.page_number;\n    } else {\n      const [_, nextStart] = sorted_labels[i + 1];\n      if (nextStart != null && nextStart >= start_page) {\n        end_page = nextStart;\n      } else {\n        end_page = start_page;\n      }\n    }\n    label_page_ranges[label] = [start_page, end_page];\n  }\n\n  // Some labels not found => request pages from the LLM\n  const labels_with_no_range: string[] = labels.filter(\n    (label) => !(label in label_page_ranges)\n  );\n\n  if (labels_with_no_range.length > 0) {\n    const prompt = get_prompt_section_page_numbers(\n      paper_array.map((el) => ({\n        metadata: { page_number: el.metadata.page_number },\n        text: el.text,\n      })),\n      labels_with_no_range.map((lbl) => label_mapping[lbl])\n    );\n\n    const answer = await generateResponse(\n      client,\n      prompt,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n\n    const answerLines = answer.split(\"\\n\");\n\n    const additional_label_page_ranges: LabelPageRanges = {};\n\n    for (const line of answerLines) {\n      // e.g. \"Introduction, 2, 5\"\n      const match = line.trim().match(/^(\\w+),\\s*(\\d+),\\s*(\\d+)/);\n      if (match) {\n        const labelLower = match[1].toLowerCase();\n        const startPage = parseInt(match[2], 10);\n        const endPage = parseInt(match[3], 10);\n        additional_label_page_ranges[labelLower] = [startPage, endPage];\n      }\n    }\n\n    // Merge additional ranges in\n    for (const [label, range] of Object.entries(additional_label_page_ranges)) {\n      label_page_ranges[label] = range;\n    }\n  }\n\n  // Build final list of extracted labels\n  const extractedLabels = Object.keys(label_page_ranges)\n    .map((lbl) => label_mapping[lbl])\n    .filter((lbl2) => lbl2 != null);\n\n  const skippedLabels = originalLabels.filter(\n    (lbl) => !extractedLabels.includes(lbl)\n  );\n\n  if (skippedLabels.length > 0) {\n    logger.info(`Skipped sections: ${skippedLabels.join(\", \")}`);\n  } else {\n    logger.info(`All sections were extracted.`, label_page_ranges);\n  }\n\n  // Make introduction start at page 0 if it exists\n  if (\"introduction\" in label_page_ranges) {\n    const [_, endPage] = label_page_ranges[\"introduction\"];\n    label_page_ranges[\"introduction\"] = [0, endPage];\n  }\n\n  return label_page_ranges;\n}\n\n/**\n * Return a string containing basic info about the paper, or \"\" on error.\n */\nexport async function getGeneratedBasicInfoText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  const spar_array = Array.from(\n    new Set([...paper_dict.introduction, ...paper_dict.abstract])\n  );\n  try {\n    const prompt_basic_info = get_prompt_basic_info(spar_array);\n    const generated_basic_info_text = await generateResponse(\n      client,\n      prompt_basic_info\n    );\n    logger.info(\n      `Generated basic text from Claude: ${generated_basic_info_text}`\n    );\n    return generated_basic_info_text;\n  } catch (e) {\n    logger.error(\"Generated basic info text exception\", e);\n    return \"\";\n  }\n}\n\n/**\n * Return a string containing citations, or \"\" on error.\n */\nexport async function getGeneratedCitations(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const prompt_citations = get_prompt_citations(paper_dict.citations);\n    const generated_citations = await generateResponse(\n      client,\n      prompt_citations,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(`Generated citations from Claude: ${generated_citations}`);\n    return generated_citations;\n  } catch (e) {\n    logger.error(\"Generated citations exception\", e);\n    return \"\";\n  }\n}\n\n/**\n * Generate a GO subgraph from relevant sections, then refine it via update_go_terms.\n */\nexport async function getGoGeneratedSubgraphText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const go_array = Array.from(\n      new Set([\n        ...paper_dict.introduction,\n        ...paper_dict.methods,\n        ...paper_dict.results,\n        ...paper_dict.discussion,\n      ])\n    );\n    const prompt_subgraph = get_prompt_go_subgraph(go_array);\n    let generated_subgraph_text = await generateResponse(\n      client,\n      prompt_subgraph,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(\n      `Generated GO subgraph from Claude: ${generated_subgraph_text}`\n    );\n\n    let generated_subgraph: JSONValue;\n    try {\n      generated_subgraph = JSON.parse(generated_subgraph_text) as JSONValue;\n    } catch {\n      generated_subgraph = {};\n    }\n\n    // The below function presumably returns a new structure or modifies in place:\n    const updated_subgraph = await updateGoTerms(generated_subgraph, client);\n    generated_subgraph_text = JSON.stringify(updated_subgraph);\n    logger.info(`Generated subgraph using GO API: ${generated_subgraph_text}`);\n    return generated_subgraph_text;\n  } catch (e) {\n    logger.error(\"Generated subgraph exception\", e);\n    return \"{}\";\n  }\n}\n\n/**\n * Generate a DOID subgraph from relevant sections, then refine it via update_doid_terms.\n */\nexport async function getDoidGeneratedSubgraphText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const doid_array = Array.from(\n      new Set([\n        ...paper_dict.introduction,\n        ...paper_dict.abstract,\n        ...paper_dict.results,\n        ...paper_dict.discussion,\n      ])\n    );\n    const prompt_subgraph = get_prompt_doid_subgraph(doid_array);\n    const generated_subgraph_text = await generateResponse(\n      client,\n      prompt_subgraph,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(\n      `Generated DOID subgraph from Claude: ${generated_subgraph_text}`\n    );\n\n    let generated_subgraph: JSONValue;\n    try {\n      generated_subgraph = JSON.parse(generated_subgraph_text) as JSONValue;\n    } catch {\n      generated_subgraph = [];\n    }\n    const updated_subgraph = await updateDoidTerms(generated_subgraph, client);\n    const finalText = JSON.stringify(updated_subgraph);\n    logger.info(`Generated subgraph using DOID API: ${finalText}`);\n    return finalText;\n  } catch (e) {\n    logger.error(\"Generated subgraph exception\", e);\n    return \"[]\";\n  }\n}\n\n/**\n * Generate a ChEBI subgraph from relevant sections, then refine it via update_chebi_terms.\n */\nexport async function getChebiGeneratedSubgraphText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const chebi_array = Array.from(\n      new Set([\n        ...paper_dict.introduction,\n        ...paper_dict.abstract,\n        ...paper_dict.results,\n        ...paper_dict.discussion,\n      ])\n    );\n    const prompt_subgraph = get_prompt_chebi_subgraph(chebi_array);\n    const generated_subgraph_text = await generateResponse(\n      client,\n      prompt_subgraph,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(\n      `Generated ChEBI subgraph from Claude: ${generated_subgraph_text}`\n    );\n\n    let generated_subgraph: JSONValue;\n    try {\n      generated_subgraph = JSON.parse(generated_subgraph_text) as JSONValue;\n    } catch {\n      generated_subgraph = [];\n    }\n    const updated_subgraph = await updateChebiTerms(generated_subgraph, client);\n    const finalText = JSON.stringify(updated_subgraph);\n    logger.info(`Generated subgraph using CHEBI API: ${finalText}`);\n    return finalText;\n  } catch (e) {\n    logger.error(\"Generated subgraph exception\", e);\n    return \"[]\";\n  }\n}\n\n/**\n * Generate an ATC subgraph from relevant sections, then refine it via update_atc_terms.\n */\nexport async function getAtcGeneratedSubgraphText(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<string> {\n  try {\n    const atc_array = Array.from(\n      new Set([\n        ...paper_dict.introduction,\n        ...paper_dict.abstract,\n        ...paper_dict.results,\n        ...paper_dict.discussion,\n      ])\n    );\n    const prompt_subgraph = get_prompt_atc_subgraph(atc_array);\n    const generated_subgraph_text = await generateResponse(\n      client,\n      prompt_subgraph,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(\n      `Generated ATC subgraph from Claude: ${generated_subgraph_text}`\n    );\n\n    let generated_subgraph: JSONValue;\n    try {\n      generated_subgraph = JSON.parse(generated_subgraph_text) as JSONValue;\n    } catch {\n      generated_subgraph = [];\n    }\n    const updated_subgraph = await updateAtcTerms(generated_subgraph, client);\n    const finalText = JSON.stringify(updated_subgraph);\n    logger.info(`Generated subgraph using ATC API: ${finalText}`);\n    return finalText;\n  } catch (e) {\n    logger.error(\"Generated subgraph exception\", e);\n    return \"[]\";\n  }\n}\n\n/**\n * Launch parallel tasks to produce each piece of data from a single paper.\n */\nexport async function process_paper(\n  client: Anthropic,\n  paper_dict: PaperDict\n): Promise<PaperProcessResult> {\n  const [\n    generated_basic_info,\n    generated_citations,\n    generated_go_subgraph,\n    generated_doid_subgraph,\n    generated_chebi_subgraph,\n    generated_atc_subgraph,\n  ] = await Promise.all([\n    getGeneratedBasicInfoText(client, paper_dict),\n    getGeneratedCitations(client, paper_dict),\n    getGoGeneratedSubgraphText(client, paper_dict),\n    getDoidGeneratedSubgraphText(client, paper_dict),\n    getChebiGeneratedSubgraphText(client, paper_dict),\n    getAtcGeneratedSubgraphText(client, paper_dict),\n  ]);\n\n  return [\n    generated_basic_info,\n    generated_citations,\n    generated_go_subgraph,\n    generated_doid_subgraph,\n    generated_chebi_subgraph,\n    generated_atc_subgraph,\n  ];\n}\n\n/**\n * Truncates a JSON array string at the last valid element, then closes the array.\n */\nexport function fix_json_string_manually(json_string: string): string {\n  const lastBraceIndex = json_string.lastIndexOf(\"},\");\n  if (lastBraceIndex !== -1) {\n    json_string = json_string.slice(0, lastBraceIndex + 1);\n  }\n\n  if (json_string.endsWith(\",\")) {\n    json_string = json_string.slice(0, -1);\n  }\n\n  return json_string + \"]\";\n}\n\n/**\n * Convert freeform citations text into a JSON-LD array (SPAR citations).\n */\nexport async function get_subgraph_citations(\n  client: Anthropic,\n  citations_text: string\n): Promise<JSONValue> {\n  const prompt_spar_citations = get_prompt_spar_citations(citations_text);\n  const generated_citations_spar_text = await generateResponse(\n    client,\n    prompt_spar_citations,\n    \"claude-3-5-sonnet-20241022\",\n    8192\n  );\n  logger.info(\n    `Generated SPAR citations from Claude: ${generated_citations_spar_text}`\n  );\n\n  try {\n    return JSON.parse(generated_citations_spar_text) as JSONValue;\n  } catch {\n    const fixed_citations = fix_json_string_manually(\n      generated_citations_spar_text\n    );\n    logger.info(`Fixed citations: ${fixed_citations}`);\n    return JSON.parse(fixed_citations) as JSONValue;\n  }\n}\n\n/**\n * Use SPAR+OBI ontology prompt to convert basic info text into JSON-LD.\n */\nexport async function get_subgraph_basic_info(\n  client: Anthropic,\n  basic_info_text: string\n): Promise<string> {\n  if (isEmptyArray(basic_info_text)) {\n    return basic_info_text;\n  }\n\n  const prompt_spar_ontology_ = get_prompt_spar_ontology(basic_info_text);\n  const generated_graph_text = await generateResponse(\n    client,\n    prompt_spar_ontology_,\n    \"claude-3-5-sonnet-20241022\",\n    8192\n  );\n  logger.info(`Generated SPAR graph from Claude: ${generated_graph_text}`);\n\n  let textTrimmed = generated_graph_text.trim();\n  if (textTrimmed.startsWith(\"```json\") && textTrimmed.endsWith(\"```\")) {\n    textTrimmed = textTrimmed.slice(7, -3).trim();\n  }\n  return textTrimmed;\n}\n\n/**\n * Convert a GO subgraph from raw JSON to an ontology array (JSON-LD).\n */\nexport async function get_subgraph_go(\n  client: Anthropic,\n  generated_go_subgraph: string\n): Promise<JSONValue> {\n  try {\n    if (isEmptyArray(generated_go_subgraph)) {\n      return [];\n    }\n    const prompt_go_ontology_ = get_prompt_go_ontology(generated_go_subgraph);\n    const generated_graph_text = await generateResponse(\n      client,\n      prompt_go_ontology_,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n    logger.info(`Generated GO subgraph from Claude: ${generated_graph_text}`);\n\n    const extracted_content = extractBracketContent(generated_graph_text);\n    if (extracted_content === null) {\n      return [];\n    }\n    return JSON.parse(extracted_content);\n  } catch (e) {\n    logger.error(\"Error generating GO subgraph\", e);\n    return [];\n  }\n}\n\n/**\n * Convert the DOID subgraph from raw JSON to RDF.\n */\nexport function get_subgraph_doid(generated_doid_subgraph: string): JSONValue {\n  try {\n    const doidData = JSON.parse(generated_doid_subgraph);\n    if (!Array.isArray(doidData)) return [];\n\n    const rdf = doidData.map((item: Record<string, JSONValue>) => ({\n      \"@id\": `http://purl.obolibrary.org/obo/${item[\"disease_id\"]}`,\n      \"dcterms:title\": item[\"disease\"],\n      \"dcterms:description\": item[\"findings\"],\n    }));\n    return rdf;\n  } catch (e) {\n    logger.error(\"Error generating DOID subgraph\", e);\n    return [];\n  }\n}\n\n/**\n * Convert the CHEBI subgraph from raw JSON to RDF.\n */\nexport function get_subgraph_chebi(\n  generated_chebi_subgraph: string\n): JSONValue {\n  try {\n    const chebiData = JSON.parse(generated_chebi_subgraph);\n    if (!Array.isArray(chebiData)) return [];\n\n    const rdf = chebiData.map((item: Record<string, JSONValue>) => ({\n      \"@id\": `http://purl.obolibrary.org/obo/${item[\"compound_id\"]}`,\n      \"dcterms:title\": item[\"compound\"],\n      \"dcterms:description\": item[\"findings\"],\n    }));\n    return rdf;\n  } catch (e) {\n    logger.error(\"Error generating CHEBI subgraph\", e);\n    return [];\n  }\n}\n\n/**\n * Convert the ATC subgraph from raw JSON to RDF.\n */\nexport function get_subgraph_atc(generated_atc_subgraph: string): JSONValue {\n  try {\n    const atcData = JSON.parse(generated_atc_subgraph);\n    if (!Array.isArray(atcData)) return [];\n\n    const rdf = atcData.map((item: Record<string, JSONValue>) => ({\n      \"@id\": `http://purl.bioontology.org/ontology/ATC/${item[\"drug_id\"]}`,\n      \"dcterms:title\": item[\"drug\"],\n      \"dcterms:description\": item[\"findings\"],\n    }));\n    return rdf;\n  } catch (e) {\n    logger.error(\"Error generating ATC subgraph\", e);\n    return [];\n  }\n}\n\n/**\n * Build a final combined JSON-LD-like graph from basic info, citations, and subgraphs.\n */\nexport async function create_graph(\n  client: Anthropic,\n  basic_info_text: string,\n  citations_text: string,\n  subgraph: SubgraphSet\n): Promise<Record<string, JSONValue>> {\n  const { go, doid, chebi, atc } = subgraph;\n\n  let generated_graph: Record<string, JSONValue> = {};\n\n  try {\n    const generated_graph_text = await get_subgraph_basic_info(\n      client,\n      basic_info_text\n    );\n    generated_graph = JSON.parse(generated_graph_text) as Record<\n      string,\n      JSONValue\n    >;\n  } catch (e) {\n    logger.error(\"Generating graph exception\", e);\n  }\n\n  // Ensure the array is in place\n  if (!generated_graph[\"obi:OBI_0000299\"]) {\n    generated_graph[\"obi:OBI_0000299\"] = [];\n  }\n\n  // Merge subgraphs\n  const goArray = await get_subgraph_go(client, go);\n  const doidArray = get_subgraph_doid(doid);\n  const chebiArray = get_subgraph_chebi(chebi);\n  const atcArray = get_subgraph_atc(atc);\n\n  // We know \"obi:OBI_0000299\" must be an array\n  const obiArr = generated_graph[\"obi:OBI_0000299\"];\n  if (Array.isArray(obiArr)) {\n    if (Array.isArray(goArray)) obiArr.push(...goArray);\n    if (Array.isArray(doidArray)) obiArr.push(...doidArray);\n    if (Array.isArray(chebiArray)) obiArr.push(...chebiArray);\n    if (Array.isArray(atcArray)) obiArr.push(...atcArray);\n  }\n\n  try {\n    const subgraphCites = await get_subgraph_citations(client, citations_text);\n    generated_graph[\"cito:cites\"] = subgraphCites;\n  } catch (e) {\n    logger.error(\"Error generating citations\", e);\n  }\n\n  // If we have a valid DOI, set @id\n  const doi = generated_graph[\"dcterms:identifier\"];\n  if (doi && doi !== \"https://doi.org/XX.XXXX/XX.XXXX\") {\n    generated_graph[\"@id\"] = doi;\n  } else {\n    generated_graph[\"@id\"] = \"PLEASE FILL IN THE DOI URL IDENTIFIER HERE\";\n  }\n\n  return generated_graph;\n}\n\n/**\n * Create arrays of text for each recognized section.\n */\nexport function create_section_arrays(\n  paper_array: PaperArrayElement[],\n  section_ranges: LabelPageRanges\n): PaperDict {\n  const introduction_array: string[] = [];\n  const abstract_array: string[] = [];\n  const methods_array: string[] = [];\n  const results_array: string[] = [];\n  const discussion_array: string[] = [];\n\n  for (const element of paper_array) {\n    const page_number = element.metadata.page_number;\n    const text = element.text;\n\n    // Introduction\n    if (\n      \"introduction\" in section_ranges &&\n      page_number >= section_ranges[\"introduction\"][0] &&\n      page_number <= section_ranges[\"introduction\"][1]\n    ) {\n      introduction_array.push(text);\n    }\n    // Abstract\n    if (\n      \"abstract\" in section_ranges &&\n      page_number >= section_ranges[\"abstract\"][0] &&\n      page_number <= section_ranges[\"abstract\"][1]\n    ) {\n      abstract_array.push(text);\n    }\n    // Methods\n    if (\n      \"methods\" in section_ranges &&\n      page_number >= section_ranges[\"methods\"][0] &&\n      page_number <= section_ranges[\"methods\"][1]\n    ) {\n      methods_array.push(text);\n    }\n    // Results\n    if (\n      \"results\" in section_ranges &&\n      page_number >= section_ranges[\"results\"][0] &&\n      page_number <= section_ranges[\"results\"][1]\n    ) {\n      results_array.push(text);\n    }\n    // Discussion\n    if (\n      \"discussion\" in section_ranges &&\n      page_number >= section_ranges[\"discussion\"][0] &&\n      page_number <= section_ranges[\"discussion\"][1]\n    ) {\n      discussion_array.push(text);\n    }\n  }\n\n  return {\n    introduction: introduction_array,\n    abstract: abstract_array,\n    methods: methods_array,\n    results: results_array,\n    discussion: discussion_array,\n    citations: [],\n  };\n}\n\n/**\n * Build a PaperDict from the full PDF text array, including the final citations.\n */\nexport async function processJsonArray(\n  paper_array: PaperArrayElement[],\n  client: Anthropic\n): Promise<PaperDict> {\n  const section_ranges = await extractSections(client, paper_array);\n  const paper_array_dict = create_section_arrays(paper_array, section_ranges);\n\n  const lastPage = paper_array[paper_array.length - 1].metadata.page_number;\n  paper_array_dict.citations = paper_array\n    .filter(\n      (el) =>\n        el.metadata.page_number >= lastPage - CITATIONS_OFFSET &&\n        typeof el.text === \"string\"\n    )\n    .map((el) => el.text);\n\n  return paper_array_dict;\n}\n\n/**\n * Generate three suggested research questions for the given paper dictionary.\n */\nexport async function get_suggested_questions(\n  paper_dict: string\n): Promise<string[]> {\n  try {\n    const client = getClient(); // Adjust if you must await or pass config\n    const prompt_questions = get_prompt_suggested_questions(paper_dict);\n    const generated_questions_text = await generateResponse(\n      client,\n      prompt_questions,\n      \"claude-3-5-sonnet-20241022\",\n      8192\n    );\n\n    const lines = generated_questions_text.trim().split(\"\\n\");\n    const questions = lines.filter((q) => q.trim().length > 0);\n    logger.info(`Generated suggested questions from Claude: ${questions}`);\n    return questions;\n  } catch (e) {\n    logger.error(\"Error generating questions\", e);\n    return [];\n  }\n}\n","// vectorize.ts\n\nimport { get_prompt_vectorization_summary } from \"./llmPrompt\"; // Adjust path as needed\nimport { generateResponse } from \"./anthropicClient\"; // Adjust path as needed\nimport Anthropic from \"@anthropic-ai/sdk\";\nimport { logger } from \"@elizaos/core\";\n\n/**\n * The general structure of your graph object. Extend with any additional fields you need.\n */\ninterface Graph {\n  [key: string]: unknown;\n  \"dcterms:title\"?: string;\n  \"@id\"?: string;\n}\n\n/**\n * A single citation entry. Adjust if your citation objects have more fields.\n */\ninterface CitationEntry {\n  [key: string]: unknown;\n  \"@id\": string;\n  \"dcterms:title\": string;\n}\n\n/**\n * The shape returned by findSimilarTitle. The second element is a similarity score (0-1).\n * The first element's metadata presumably includes a \"doi\" string.\n */\ninterface SimilarCitationResult {\n  metadata: { [key: string]: string };\n}\n\n/**\n * Generate a summary for the provided graph using the LLM client.\n * @param client - The client or config object for your LLM\n * @param graph  - The graph/dictionary containing paper metadata\n */\nexport async function getSummary(\n  client: Anthropic,\n  graph: Graph\n): Promise<string> {\n  let summary = \"\";\n  try {\n    const prompt = get_prompt_vectorization_summary(graph);\n    summary = await generateResponse(client, prompt);\n    logger.info(`Generated graph summary from Claude: ${summary}`);\n  } catch (error) {\n    logger.error(\"Generated graph summary exception\", error);\n    summary = \"\";\n  }\n  return summary;\n}\n","import { Service, IAgentRuntime, logger } from \"@elizaos/core\";\nimport { hypGenEvalLoop, stopHypGenEvalLoop } from \"./anthropic/hypGenEvalLoop\";\nimport { watchFolderChanges } from \"./gdrive\";\nimport { sql } from \"drizzle-orm\";\nimport { fileMetadataTable } from \"src/db/schemas\";\n\nexport class HypothesisService extends Service {\n  static serviceType = \"hypothesis\";\n  capabilityDescription = \"Generate and judge hypotheses\";\n  constructor(protected runtime: IAgentRuntime) {\n    super(runtime);\n  }\n  static async start(runtime: IAgentRuntime) {\n    logger.info(\"*** Starting hypotheses service ***\");\n    const service = new HypothesisService(runtime);\n    // const interval = await hypGenEvalLoop(runtime);\n    runtime.registerTaskWorker({\n      name: \"HGE\",\n      async execute(runtime, options, task) {\n        logger.log(\"task worker\");\n      },\n    });\n    const tasks = await runtime.getTasksByName(\"HGE\");\n    if (tasks.length < 1) {\n      const taskId = await runtime.createTask({\n        name: \"HGE\",\n        description:\n          \"Generate and evaluate hypothesis whilst streaming them to discord\",\n        tags: [\"hypothesis\", \"judgeLLM\"],\n        metadata: { updateInterval: 1500, updatedAt: Date.now() },\n      });\n      logger.info(\"Task UUID:\", taskId);\n    }\n    // In an initialization function or periodic check\n    async function processRecurringTasks() {\n      logger.info(\"Starting processing loop\");\n      const now = Date.now();\n      const recurringTasks = await runtime.getTasks({\n        tags: [\"hypothesis\"],\n      });\n      logger.info(\"Got tasks\", recurringTasks);\n\n      for (const task of recurringTasks) {\n        if (!task.metadata?.updateInterval) continue;\n\n        const lastUpdate = (task.metadata.updatedAt as number) || 0;\n        const interval = task.metadata.updateInterval;\n\n        if (now >= lastUpdate + interval) {\n          logger.info(\"Executing task\");\n          const worker = runtime.getTaskWorker(task.name);\n          if (worker) {\n            try {\n              await worker.execute(runtime, {}, task);\n\n              // Update the task's last update time\n              await runtime.updateTask(task.id, {\n                metadata: {\n                  ...task.metadata,\n                  updatedAt: now,\n                },\n              });\n            } catch (error) {\n              logger.error(`Error executing task ${task.name}: ${error}`);\n            }\n          }\n        }\n      }\n    }\n    await processRecurringTasks();\n\n    // await watchFolderChanges(runtime);\n\n    process.on(\"SIGINT\", async () => {\n      // stopHypGenEvalLoop(interval);\n    });\n\n    return service;\n  }\n\n  static async stop(runtime: IAgentRuntime) {\n    logger.info(\"*** Stopping hypotheses service ***\");\n    // get the service from the runtime\n    const service = runtime.getService(HypothesisService.serviceType);\n    if (!service) {\n      throw new Error(\"Hypotheses service not found\");\n    }\n    service.stop();\n  }\n\n  async stop() {\n    logger.info(\"*** Stopping hypotheses service instance ***\");\n  }\n}\n","import { type IAgentRuntime, logger } from \"@elizaos/core\";\nimport { driveSyncTable } from \"src/db\";\nimport { initDriveClient } from \"./services/gdrive\";\nimport { ListFilesQueryContext } from \"./services/gdrive/buildQuery\";\nimport \"dotenv/config\";\nimport { migrateDb } from \"./db/migration\";\n\n// Run migrations before initializing anything else\nexport async function initWithMigrations(runtime: IAgentRuntime) {\n  try {\n    // Run migrations first\n    await migrateDb();\n\n    // Then initialize drive sync\n    await initDriveSync(runtime);\n  } catch (error) {\n    logger.error(\"Error during initialization:\", error);\n  }\n}\n\nexport async function initDriveSync(runtime: IAgentRuntime) {\n  const driveSync = await runtime.db.select().from(driveSyncTable);\n  if (driveSync.length === 0) {\n    logger.info(\"Initializing drive sync\");\n    logger.info(\"No drive sync found, creating new one\");\n    const driveClient = await initDriveClient();\n    const listFilesQueryContext = new ListFilesQueryContext(\n      process.env.GOOGLE_DRIVE_FOLDER_ID,\n      process.env.SHARED_DRIVE_ID\n    );\n    const startPageTokenParams =\n      listFilesQueryContext.getStartPageTokenParams();\n    const startPageTokenResponse =\n      await driveClient.changes.getStartPageToken(startPageTokenParams);\n    const startPageToken = startPageTokenResponse.data.startPageToken;\n    const driveType = listFilesQueryContext.getDriveType();\n    const driveId = listFilesQueryContext.getDriveId();\n    await runtime.db.insert(driveSyncTable).values({\n      id: driveId,\n      startPageToken,\n      driveType,\n    });\n  } else {\n    logger.info(\"Drive sync already initialized\");\n  }\n}\n","import { drizzle } from \"drizzle-orm/node-postgres\";\nimport pkg from \"pg\";\nimport \"dotenv/config\";\nimport { hypothesesTable, fileMetadataTable, driveSyncTable } from \"./schemas\";\n\nconst { Pool } = pkg;\nconst pool = new Pool({\n  connectionString: process.env.POSTGRES_URL,\n});\n\nexport const db = drizzle(pool, {\n  schema: {\n    hypotheses: hypothesesTable,\n    fileMetadata: fileMetadataTable,\n    driveSync: driveSyncTable,\n  },\n});\n\nexport * from \"./schemas\";\n","import { text, bigint, timestamp } from \"drizzle-orm/pg-core\";\nimport { pgSchema } from \"drizzle-orm/pg-core\";\n\nconst biographPgSchema = pgSchema(\"biograph\");\n\nexport const fileMetadataTable = biographPgSchema.table(\"file_metadata\", {\n  id: text(\"id\").notNull(),\n  hash: text(\"hash\").notNull().primaryKey(),\n  fileName: text(\"file_name\").notNull(),\n  fileSize: bigint(\"file_size\", { mode: \"number\" }),\n  createdAt: timestamp(\"created_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n  modifiedAt: timestamp(\"modified_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n  tags: text(\"tags\").array(),\n});\n\n// Type for selecting data (matches the table structure)\nexport type FileMetadata = typeof fileMetadataTable.$inferSelect;\n\n// Type for inserting data (useful for creating new records)\nexport type NewFileMetadata = typeof fileMetadataTable.$inferInsert;\n","import { uuid, text, timestamp, numeric } from \"drizzle-orm/pg-core\";\nimport { pgSchema } from \"drizzle-orm/pg-core\";\nimport { hypothesisStatusEnum } from \"./customTypes\";\n\nconst biographPgSchema = pgSchema(\"biograph\");\n\nexport const hypothesesTable = biographPgSchema.table(\"hypotheses\", {\n  id: uuid(\"id\").notNull().primaryKey().defaultRandom(),\n  hypothesis: text(\"hypothesis\").notNull(),\n  filesUsed: text(\"files_used\").array(),\n  status: hypothesisStatusEnum(\"status\").default(\"pending\"),\n  judgellmScore: numeric(\"judgellm_score\", { precision: 5, scale: 2 }),\n  humanScore: numeric(\"human_score\", { precision: 5, scale: 2 }),\n  research: text(\"research\"),\n  evaluation: text(\"evaluation\"),\n  citations: text(\"citations\").array(),\n  createdAt: timestamp(\"created_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n  updatedAt: timestamp(\"updated_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n});\n\nexport type Hypothesis = typeof hypothesesTable.$inferSelect;\nexport type NewHypothesis = typeof hypothesesTable.$inferInsert;\n","import { pgEnum } from \"drizzle-orm/pg-core\";\n\nexport const hypothesisStatusEnum = pgEnum(\"hypothesis_status\", [\n  \"pending\",\n  \"approved\",\n  \"rejected\",\n]);\n\nexport const driveTypeEnum = pgEnum(\"drive_type\", [\n  \"shared_folder\",\n  \"shared_drive\",\n]);\n","import { text, timestamp, uuid } from \"drizzle-orm/pg-core\";\nimport { pgSchema } from \"drizzle-orm/pg-core\";\nimport { driveTypeEnum } from \"./customTypes\";\n\nconst biographPgSchema = pgSchema(\"biograph\");\n\nexport const driveSyncTable = biographPgSchema.table(\"drive_sync\", {\n  id: text(\"id\").notNull().primaryKey(),\n  startPageToken: text(\"start_page_token\").notNull(),\n  driveType: driveTypeEnum(\"drive_type\").notNull(),\n  lastSyncAt: timestamp(\"last_sync_at\", { withTimezone: true, mode: \"date\" })\n    .notNull()\n    .defaultNow(),\n});\n","// https://developers.google.com/workspace/drive/api/reference/rest/v3\n\nimport { google, drive_v3 } from \"googleapis\";\nimport { ListFilesQueryContext } from \"./buildQuery\";\nimport \"dotenv/config\";\n/**\n * Initialize and return a Google Drive client\n * @param scopes - The OAuth scopes to request\n * @returns The initialized Google Drive client\n */\nexport async function initDriveClient(\n  scopes: string[] = [\"https://www.googleapis.com/auth/drive.readonly\"]\n): Promise<drive_v3.Drive> {\n  let credentials: any;\n  try {\n    // Load credentials\n    credentials = JSON.parse(process.env.GCP_JSON_CREDENTIALS || \"\");\n    // Set up authentication\n    const auth = new google.auth.GoogleAuth({\n      credentials,\n      scopes,\n    });\n\n    // Create and return drive client\n    return google.drive({ version: \"v3\", auth });\n  } catch (error) {\n    console.error(\"Error initializing Google Drive client:\", error);\n    throw error;\n  }\n}\n\nexport const FOLDERS = {\n  SHARED_DRIVE_FOLDER: process.env.GOOGLE_DRIVE_FOLDER_ID,\n  SHARED_DRIVE_ID: process.env.SHARED_DRIVE_ID,\n};\n\nexport function getListFilesQuery() {\n  const context = new ListFilesQueryContext(\n    FOLDERS.SHARED_DRIVE_FOLDER,\n    FOLDERS.SHARED_DRIVE_ID\n  );\n  return context.buildQuery();\n}\n\nexport function getStartPageTokenParams() {\n  const context = new ListFilesQueryContext(\n    FOLDERS.SHARED_DRIVE_FOLDER,\n    FOLDERS.SHARED_DRIVE_ID\n  );\n  return context.getStartPageTokenParams();\n}\n","import { logger } from \"@elizaos/core\";\n\ninterface ListFilesQueryStrategy {\n  buildQuery(): Record<string, any>;\n  getStartPageTokenParams(): Record<string, any>;\n  getDriveType(): DriveType;\n  getDriveId(): string;\n}\n\ntype DriveType = \"shared_folder\" | \"shared_drive\";\n\nclass SharedDriveFolderStrategy implements ListFilesQueryStrategy {\n  constructor(private sharedDriveFolderId: string) {}\n\n  buildQuery(): Record<string, any> {\n    return {\n      q: `'${this.sharedDriveFolderId}' in parents and mimeType='application/pdf' and trashed=false`,\n      fields: \"files(id, name, md5Checksum, size)\",\n      orderBy: \"name\",\n    };\n  }\n\n  getStartPageTokenParams(): Record<string, any> {\n    return {};\n  }\n\n  getDriveType(): DriveType {\n    return \"shared_folder\";\n  }\n\n  getDriveId(): string {\n    return this.sharedDriveFolderId;\n  }\n}\n\nclass SharedDriveStrategy implements ListFilesQueryStrategy {\n  constructor(private sharedDriveId: string) {}\n\n  buildQuery(): Record<string, any> {\n    return {\n      q: `'${this.sharedDriveId}' in parents and trashed=false`,\n      orderBy: \"name\",\n      fields: \"files(id, name, md5Checksum, size)\",\n      supportsAllDrives: true,\n      includeItemsFromAllDrives: true,\n      driveId: this.sharedDriveId,\n      corpora: \"drive\",\n    };\n  }\n\n  getStartPageTokenParams(): Record<string, any> {\n    return {\n      driveId: this.sharedDriveId,\n      supportsAllDrives: true,\n    };\n  }\n\n  getDriveType(): DriveType {\n    return \"shared_drive\";\n  }\n\n  getDriveId(): string {\n    return this.sharedDriveId;\n  }\n}\n\nexport class ListFilesQueryContext {\n  private strategy: ListFilesQueryStrategy;\n\n  constructor(mainFolderId?: string, sharedDriveId?: string) {\n    if (mainFolderId && sharedDriveId) {\n      logger.error(\n        \"You cannot populate both GOOGLE_DRIVE_FOLDER_ID and SHARED_DRIVE_ID.\"\n      );\n      process.exit(1);\n    } else if (sharedDriveId) {\n      this.strategy = new SharedDriveStrategy(sharedDriveId);\n    } else if (mainFolderId) {\n      this.strategy = new SharedDriveFolderStrategy(mainFolderId);\n    } else {\n      logger.error(\n        \"Either GOOGLE_DRIVE_FOLDER_ID or SHARED_DRIVE_ID must be defined.\"\n      );\n      process.exit(1);\n    }\n  }\n\n  buildQuery(): Record<string, any> {\n    return this.strategy.buildQuery();\n  }\n\n  getStartPageTokenParams(): Record<string, any> {\n    return this.strategy.getStartPageTokenParams();\n  }\n\n  getDriveType(): DriveType {\n    return this.strategy.getDriveType();\n  }\n\n  getDriveId(): string {\n    return this.strategy.getDriveId();\n  }\n}\n","import { IAgentRuntime, logger } from \"@elizaos/core\";\nimport { initDriveClient, FOLDERS, getListFilesQuery } from \"./client.js\";\nimport { drive_v3 } from \"googleapis\";\nimport { fileURLToPath } from \"url\";\nimport { dirname } from \"path\";\nimport DKG from \"dkg.js\";\nimport { fromBuffer } from \"pdf2pic\";\nimport { pdf2PicOptions } from \"./index.js\";\nimport { OpenAIImage } from \"./extract/types.js\";\nimport { generateKa } from \"./extract\";\nimport { storeJsonLd } from \"./storeJsonLdToKg.js\";\nimport { db, fileMetadataTable } from \"src/db\";\n\ntype Schema$File = drive_v3.Schema$File;\n\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = dirname(__filename);\n\ntype DKGClient = typeof DKG | null;\nlet DkgClient: DKGClient = null;\n\ninterface FileInfo {\n  id: string;\n  name: string;\n  md5Checksum: string;\n  size: number;\n}\n\nasync function downloadFile(\n  drive: drive_v3.Drive,\n  file: FileInfo\n): Promise<Buffer> {\n  const res = await drive.files.get(\n    {\n      fileId: file.id,\n      alt: \"media\",\n    },\n    {\n      responseType: \"arraybuffer\",\n      params: {\n        supportsAllDrives: true,\n        acknowledgeAbuse: true,\n      },\n      headers: {\n        Range: \"bytes=0-\",\n      },\n    }\n  );\n\n  return Buffer.from(res.data as ArrayBuffer);\n}\n\nasync function getFilesInfo(): Promise<FileInfo[]> {\n  const drive = await initDriveClient();\n  const query = getListFilesQuery();\n  const response = await drive.files.list(query);\n\n  return (response.data.files || [])\n    .filter(\n      (\n        f\n      ): f is Schema$File & {\n        id: string;\n        name: string;\n        md5Checksum: string;\n        size: number;\n      } =>\n        f.id != null &&\n        f.name != null &&\n        f.md5Checksum != null &&\n        f.size != null\n    )\n    .map((f) => ({\n      id: f.id,\n      name: f.name,\n      md5Checksum: f.md5Checksum,\n      size: f.size,\n    }));\n}\n\nexport async function watchFolderChanges(runtime: IAgentRuntime) {\n  logger.info(\"Watching folder changes\");\n  DkgClient = new DKG({\n    environment: runtime.getSetting(\"DKG_ENVIRONMENT\"),\n    endpoint: runtime.getSetting(\"DKG_HOSTNAME\"),\n    port: runtime.getSetting(\"DKG_PORT\"),\n    blockchain: {\n      name: runtime.getSetting(\"DKG_BLOCKCHAIN_NAME\"),\n      publicKey: runtime.getSetting(\"DKG_PUBLIC_KEY\"),\n      privateKey: runtime.getSetting(\"DKG_PRIVATE_KEY\"),\n    },\n    maxNumberOfRetries: 300,\n    frequency: 2,\n    contentType: \"all\",\n    nodeApiVersion: \"/v1\",\n  });\n  let knownHashes = new Set<string>();\n  let processedFilesId = new Set<string>();\n  let response = await db\n    .select({ hash: fileMetadataTable.hash, id: fileMetadataTable.id })\n    .from(fileMetadataTable);\n  for (const file of response) {\n    knownHashes.add(file.hash);\n    processedFilesId.add(file.id);\n  }\n  const drive = await initDriveClient();\n  let intervalId: NodeJS.Timeout | null = null;\n  let isRunning = true;\n\n  const checkForChanges = async () => {\n    if (!isRunning) return;\n\n    try {\n      const files = await getFilesInfo();\n      logger.info(`Found ${files.length} files`);\n      const currentHashes = new Set(files.map((f) => f.md5Checksum));\n\n      // Check for new files by hash that we haven't processed yet\n      const newFiles = files.filter(\n        (f) => !knownHashes.has(f.md5Checksum) && !processedFilesId.has(f.id)\n      );\n\n      if (newFiles.length > 0) {\n        logger.info(\n          \"New files detected:\",\n          newFiles.map((f) => `${f.name} (${f.md5Checksum})`)\n        );\n\n        // Download new files\n        for (const file of newFiles) {\n          logger.info(`Downloading ${file.name}...`);\n          const pdfBuffer = await downloadFile(drive, file);\n          logger.info(`Successfully downloaded ${file.name}`);\n\n          // Mark as processed immediately after download\n          processedFilesId.add(file.id);\n\n          const converter = fromBuffer(pdfBuffer, pdf2PicOptions);\n          const storeHandler = await converter.bulk(-1, {\n            responseType: \"base64\",\n          });\n          const images: OpenAIImage[] = storeHandler\n            .filter((page) => page.base64)\n            .map((page) => ({\n              type: \"image_url\",\n              image_url: {\n                url: `data:image/png;base64,${page.base64}`,\n              },\n            }));\n\n          const ka = await generateKa(images);\n          const res = await storeJsonLd(ka);\n          if (!res) {\n            continue;\n          } else {\n            logger.info(\"Successfully stored JSON-LD to Oxigraph\");\n          }\n\n          try {\n          } catch (error) {\n            logger.error(\n              \"Error occurred while publishing message to DKG:\",\n              error.message\n            );\n\n            if (error.stack) {\n              logger.error(\"Stack trace:\", error.stack);\n            }\n            if (error.response) {\n              logger.error(\n                \"Response data:\",\n                JSON.stringify(error.response.data, null, 2)\n              );\n            }\n          }\n        }\n      }\n\n      knownHashes = currentHashes;\n    } catch (error) {\n      logger.error(\"Error checking files:\", error.stack);\n    }\n  };\n\n  // Start the interval\n  checkForChanges();\n  intervalId = setInterval(checkForChanges, 10000); // Check every 10 seconds\n\n  // Return a function to stop watching\n  return {\n    stop: () => {\n      isRunning = false;\n      if (intervalId) {\n        clearInterval(intervalId);\n        intervalId = null;\n      }\n    },\n  };\n}\n","import \"dotenv/config\";\nimport Anthropic from \"@anthropic-ai/sdk\";\nimport OpenAI from \"openai\";\nimport Instructor, { InstructorClient as IC } from \"@instructor-ai/instructor\";\nimport path from \"path\";\nimport fs from \"fs\";\n\nconst __dirname = path.resolve();\n\nexport default class Config {\n  private static _instance: Config;\n  private static _anthropicClient: Anthropic;\n  private static _openaiClient: OpenAI;\n  private static _instructorOai: IC<OpenAI>;\n  private static _instructorAnthropic: IC<Anthropic>;\n  private static _anthropicModel: string =\n    process.env.ANTHROPIC_MODEL || \"claude-3-7-sonnet-latest\";\n  private static _openaiModel: string = process.env.OPENAI_MODEL || \"gpt-4o\";\n  private static _papersDirectory: string = path.join(__dirname, \"papers\");\n  private static _pdf2PicOptions: any = {\n    density: 100,\n    format: \"png\",\n    width: 595,\n    height: 842,\n  };\n\n  private constructor() {}\n\n  private static initialize() {\n    if (!this._anthropicClient) {\n      this._anthropicClient = new Anthropic({\n        apiKey: process.env.ANTHROPIC_API_KEY,\n      });\n    }\n    if (!this._openaiClient) {\n      this._openaiClient = new OpenAI({\n        apiKey: process.env.OPENAI_API_KEY,\n      });\n    }\n\n    if (!this._instructorOai) {\n      this._instructorOai = Instructor({\n        client: this._openaiClient,\n        mode: \"JSON\",\n      });\n    }\n\n    // TODO: Anthropic not yet supported\n    // if (!this._instructorAnthropic) {\n    //   this._instructorAnthropic = Instructor({\n    //     client: this._anthropicClient,\n    //     mode: \"JSON\",\n    //   });\n    // }\n\n    if (!fs.existsSync(this._papersDirectory)) {\n      fs.mkdirSync(this._papersDirectory, { recursive: true });\n    } else if (!fs.lstatSync(this._papersDirectory).isDirectory()) {\n      throw new Error(\n        `The specified papers path \"${this._papersDirectory}\" is not a directory.`\n      );\n    }\n  }\n\n  private static getInstance(): Config {\n    if (!this._instance) {\n      this._instance = new Config();\n      this.initialize();\n    }\n    return this._instance;\n  }\n\n  public static get anthropicClient(): Anthropic {\n    this.getInstance();\n    return this._anthropicClient;\n  }\n\n  public static get openaiClient(): OpenAI {\n    this.getInstance();\n    return this._openaiClient;\n  }\n\n  public static get anthropicModel(): string {\n    this.getInstance();\n    return this._anthropicModel;\n  }\n\n  public static set anthropicModel(model: string) {\n    this.getInstance();\n    this._anthropicModel = model;\n  }\n\n  public static get openaiModel(): string {\n    this.getInstance();\n    return this._openaiModel;\n  }\n\n  public static set openaiModel(model: string) {\n    this.getInstance();\n    this._openaiModel = model;\n  }\n\n  public static get papersDirectory(): string {\n    this.getInstance();\n    return this._papersDirectory;\n  }\n\n  public static set papersDirectory(directory: string) {\n    this.getInstance();\n    this._papersDirectory = directory;\n  }\n\n  public static get pdf2PicOptions() {\n    this.getInstance();\n    return this._pdf2PicOptions;\n  }\n\n  public static set pdf2PicOptions(options: any) {\n    this.getInstance();\n    this._pdf2PicOptions = options;\n  }\n\n  public static get instructorOai() {\n    this.getInstance();\n    return this._instructorOai;\n  }\n\n  public static get instructorAnthropic() {\n    this.getInstance();\n    return this._instructorAnthropic;\n  }\n}\n","import Config from \"./config\";\nimport path from \"path\";\nimport { PaperSchema, OntologiesSchema } from \"./z\";\nimport { ontologiesExtractionPrompt, extractionPrompt } from \"./prompts\";\nimport { OpenAIImage } from \"./types\";\n\nconst __dirname = path.resolve();\n\nasync function extractPaper(images: OpenAIImage[]) {\n  console.log(\n    `[extractPaper] Starting paper extraction with ${images.length} images`\n  );\n  const client = Config.instructorOai;\n\n  const { _meta, ...paper } = await client.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n      {\n        role: \"system\",\n        content: extractionPrompt,\n      },\n      {\n        role: \"user\",\n        content: [...images],\n      },\n    ],\n    response_model: { schema: PaperSchema, name: \"Paper\" },\n    max_retries: 3,\n  });\n  console.log(`[extractPaper] Paper extraction completed successfully`);\n  return paper;\n}\n\nasync function extractOntologies(images: OpenAIImage[]) {\n  console.log(\n    `[extractOntologies] Starting ontologies extraction with ${images.length} images`\n  );\n  const client = Config.instructorOai;\n\n  const { _meta, ...ontologies } = await client.chat.completions.create({\n    model: \"gpt-4o\",\n    messages: [\n      {\n        role: \"system\",\n        content: ontologiesExtractionPrompt,\n      },\n      {\n        role: \"user\",\n        content: [...images],\n      },\n    ],\n    response_model: { schema: OntologiesSchema, name: \"Ontologies\" },\n    max_retries: 3,\n  });\n  console.log(\n    `[extractOntologies] Ontologies extraction completed successfully`\n  );\n  return ontologies;\n}\n\nexport async function generateKa(images: OpenAIImage[]) {\n  console.log(\n    `[generateKa] Starting knowledge extraction with ${images.length} images`\n  );\n  const res = await Promise.all([\n    extractPaper(images),\n    extractOntologies(images),\n  ]);\n  console.log(`[generateKa] All extractions completed, combining results`);\n  res[0][\"ontologies\"] = res[1];\n  console.log(`[generateKa] Knowledge extraction successfully completed`);\n  return res[0];\n}\n","import { z } from \"zod\";\nimport crypto from \"crypto\";\n\n/*\n * Initial standardized schema for the scientific paper KA\n * with expanded descriptions for each field\n */\n\n// TODO: add IPFS\n\n// fixed default context with additional biomedical ontologies\nconst defaultContext = {\n  schema: \"https://schema.org/\",\n  fabio: \"http://purl.org/spar/fabio/\",\n  cito: \"http://purl.org/spar/cito/\",\n  dcterms: \"http://purl.org/dc/terms/\",\n  foaf: \"http://xmlns.com/foaf/0.1/\",\n  bibo: \"http://purl.org/ontology/bibo/\",\n  go: \"http://purl.obolibrary.org/obo/GO_\",\n  doid: \"http://purl.org/obo/DOID_\",\n  chebi: \"http://purl.org/obo/CHEBI_\",\n  atc: \"http://purl.org/obo/ATC_\",\n  pw: \"http://purl.org/obo/PW_\",\n  eco: \"http://purl.org/obo/ECO_\",\n  mondo: \"http://purl.org/obo/MONDO_\",\n  comptox: \"https://comptox.epa.gov/\",\n  mesh: \"http://id.nlm.nih.gov/mesh/\",\n} as const;\n\nconst ContextSchema = z\n  .object({\n    schema: z.literal(\"https://schema.org/\"),\n    fabio: z.literal(\"http://purl.org/spar/fabio/\"),\n    cito: z.literal(\"http://purl.org/spar/cito/\"),\n    dcterms: z.literal(\"http://purl.org/dc/terms/\"),\n    foaf: z.literal(\"http://xmlns.com/foaf/0.1/\"),\n    bibo: z.literal(\"http://purl.org/ontology/bibo/\"),\n    go: z.literal(\"http://purl.obolibrary.org/obo/GO_\"),\n    doid: z.literal(\"http://purl.org/obo/DOID_\"),\n    chebi: z.literal(\"http://purl.org/obo/CHEBI_\"),\n    atc: z.literal(\"http://purl.org/obo/ATC_\"),\n    pw: z.literal(\"http://purl.org/obo/PW_\"),\n    eco: z.literal(\"http://purl.org/obo/ECO_\"),\n    mondo: z.literal(\"http://purl.org/obo/MONDO_\"),\n    comptox: z.literal(\"https://comptox.epa.gov/\"),\n    mesh: z.literal(\"http://id.nlm.nih.gov/mesh/\"),\n  })\n  .default(defaultContext)\n  .describe(\n    \"Context prefixes for JSON-LD, mapping short prefixes (e.g. go:) to full IRIs.\"\n  );\n\n// creator (author) schema\nconst CreatorSchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"Unique identifier for the creator, typically an ORCID URI. Defaults to a kebab-case of the creator's name.\"\n    )\n    .default(`https://orcid.org/${crypto.randomUUID()}`),\n  \"@type\": z\n    .string()\n    .describe(\n      \"RDF type of the creator (e.g. foaf:Person). Identifies the class of this entity in Linked Data.\"\n    ),\n  \"foaf:name\": z\n    .string()\n    .describe(\n      \"Full display name of the creator, e.g. 'Alice Smith' or 'Alice B. Smith'.\"\n    ),\n});\n\n// publication venue schema\nconst PublicationVenueSchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"Primary identifier (e.g. DOI) of the publication venue (journal, conference, repository).\"\n    ),\n  \"@type\": z\n    .string()\n    .describe(\n      \"RDF type of the publication venue (e.g. fabio:Journal, schema:Periodical).\"\n    ),\n  \"schema:name\": z\n    .string()\n    .describe(\n      \"Human-readable name of the publication venue, e.g. 'Nature' or 'Proceedings of XYZ Conference'.\"\n    ),\n});\n\n// section schema\nconst SectionSchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"Short ID or local identifier for this section, often used as a fragment or anchor.\"\n    ),\n  \"@type\": z\n    .string()\n    .describe(\n      \"RDF type for the section, e.g. 'fabio:Section' or similar to define its role in the paper.\"\n    ),\n  \"dcterms:title\": z\n    .string()\n    .describe(\n      \"Heading or title of this section, e.g. 'Methods', 'Results', 'Discussion'.\"\n    ),\n  \"fabio:hasContent\": z\n    .string()\n    .describe(\n      \"Full textual content of the section (may include paragraphs of text, tables, etc.).\"\n    ),\n});\n\n// citation schema\nconst CitationSchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"A unique identifier (often a DOI) for the cited work being referenced.\"\n    ),\n  \"@type\": z\n    .string()\n    .describe(\n      \"RDF type of the cited resource, e.g. 'bibo:AcademicArticle' or 'schema:ScholarlyArticle'.\"\n    ),\n  \"dcterms:title\": z.string().describe(\"Title of the cited work or resource.\"),\n  \"bibo:doi\": z\n    .string()\n    .describe(\n      \"Explicit DOI string of the cited work, e.g. '10.1038/s41586-020-XXXXX'.\"\n    ),\n});\n\n// ontology schema\nexport const OntologySchema = z.object({\n  \"@id\": z\n    .string()\n    .describe(\n      \"Compact or full IRI of the ontology term discussed in the paper (e.g. GO, DOID, CHEBI, ATC, etc.) or 'http://purl.obolibrary.org/obo/xxx'.\"\n    ),\n  \"schema:name\": z\n    .string()\n    .describe(\n      \"Human-readable label of the ontology concept discussed in the paper\"\n    ),\n});\n\nexport const OntologiesSchema = z.object({\n  ontologies: z.array(OntologySchema),\n});\n\n// research paper schema\nexport const PaperSchema = z\n  .object({\n    \"@context\": ContextSchema,\n    \"@id\": z\n      .string()\n      .describe(\"Top-level identifier for the paper, typically a DOI.\")\n      .default(`https://doi.org/10.1234/${crypto.randomInt(10000, 99999)}`),\n    \"@type\": z\n      .string()\n      .describe(\n        \"Type of the paper, typically 'bibo:AcademicArticle' or 'schema:ScholarlyArticle'.\"\n      ),\n    \"dcterms:title\": z\n      .string()\n      .describe(\"Title of the paper, e.g. 'A Study on ...'.\"),\n    \"dcterms:creator\": z\n      .array(CreatorSchema)\n      .describe(\n        \"List of creators (authors). Each entry follows CreatorSchema, containing @id, @type, foaf:name.\"\n      ),\n    \"dcterms:abstract\": z\n      .string()\n      .describe(\"Abstract text summarizing the paper's content and findings.\"),\n    \"schema:datePublished\": z\n      .string()\n      .describe(\"Publication date, usually an ISO 8601 string (YYYY-MM-DD).\"),\n    \"schema:keywords\": z\n      .array(z.string())\n      .describe(\n        \"List of keywords or key phrases describing the paper's topics.\"\n      ),\n    \"fabio:hasPublicationVenue\": PublicationVenueSchema.describe(\n      \"Metadata about where this paper was published (journal, conference, etc.).\"\n    ),\n    \"fabio:hasPart\": z\n      .array(SectionSchema)\n      .describe(\n        \"Sections that compose the paper. Each section has an @id, @type, title, and content.\"\n      ),\n    \"cito:cites\": z\n      .array(CitationSchema)\n      .describe(\n        \"References/citations this paper includes. Each entry has an identifier, type, title, and DOI.\"\n      ),\n  })\n  .describe(\n    \"Complete JSON-LD schema for a scientific paper, including authors, venue, sections, citations, and ontology references.\"\n  );\n\nexport type Paper = z.infer<typeof PaperSchema>;\n","import { Store, Quad } from \"n3\";\nimport { JsonLdParser } from \"jsonld-streaming-parser\";\nimport axios from \"axios\";\n\n/**\n * Accepts a JSON-LD object, parses it, and stores the resulting data in Oxigraph.\n * @param jsonLd A JSON-LD object to be parsed.\n * @returns A promise that resolves with `true` if successful.\n */\nexport async function storeJsonLd(jsonLd: object): Promise<boolean> {\n  const store = new Store();\n  const parser = new JsonLdParser();\n\n  return new Promise((resolve, reject) => {\n    // Attach stream listeners\n    parser.on(\"data\", (quad: Quad) => {\n      store.addQuad(quad);\n    });\n\n    parser.on(\"error\", (error: Error) => {\n      console.error(\"Parsing error:\", error);\n      reject(error);\n    });\n\n    parser.on(\"end\", async () => {\n      try {\n        console.log(`Parsed ${store.size} quads`);\n\n        // Convert store data to N-Triples\n        const ntriples = store\n          .getQuads(null, null, null, null)\n          .map(\n            (quad) =>\n              `<${quad.subject.value}> <${quad.predicate.value}> ${\n                quad.object.termType === \"Literal\"\n                  ? `\"${quad.object.value}\"`\n                  : `<${quad.object.value}>`\n              }.`\n          )\n          .join(\"\\n\");\n\n        // Send N-Triples to Oxigraph\n        const response = await axios.post(\n          \"http://localhost:7878/store\",\n          ntriples,\n          {\n            headers: {\n              \"Content-Type\": \"application/n-quads\",\n            },\n          }\n        );\n\n        if (response.status === 204) {\n          console.log(\"Successfully stored JSON-LD in Oxigraph\");\n          resolve(true);\n        } else {\n          reject(new Error(`Unexpected response status: ${response.status}`));\n        }\n      } catch (error: any) {\n        console.error(\"Error storing JSON-LD in Oxigraph:\", error);\n        reject(error);\n      }\n    });\n\n    // Begin parsing\n    try {\n      parser.write(JSON.stringify(jsonLd));\n      parser.end();\n    } catch (error: any) {\n      console.error(\"Error during parser execution:\", error);\n      reject(error);\n    }\n  });\n}\n","import { drizzle } from \"drizzle-orm/node-postgres\";\nimport { migrate } from \"drizzle-orm/node-postgres/migrator\";\nimport pkg from \"pg\";\nimport \"dotenv/config\";\nimport { existsSync, writeFileSync } from \"fs\";\nimport path from \"path\";\nimport { logger } from \"@elizaos/core\";\n\nconst { Pool } = pkg;\n\n/**\n * Check if the migrations have been run before\n */\nconst getMigrationFlag = (): boolean => {\n  const flagPath = path.join(process.cwd(), \".migration-complete\");\n  return existsSync(flagPath);\n};\n\n/**\n * Set the migration flag to indicate migrations have been run\n */\nconst setMigrationFlag = (): void => {\n  const flagPath = path.join(process.cwd(), \".migration-complete\");\n  writeFileSync(flagPath, new Date().toISOString());\n};\n\n/**\n * Run database migrations if they haven't been run yet\n */\nexport const migrateDb = async (): Promise<void> => {\n  // Check if migrations have already been run\n  if (getMigrationFlag()) {\n    logger.info(\"Migrations already applied, skipping...\");\n    return;\n  }\n\n  if (!process.env.POSTGRES_URL) {\n    logger.warn(\n      \"POSTGRES_URL environment variable is not set, skipping migrations\"\n    );\n    return;\n  }\n\n  try {\n    logger.info(\"Running database migrations...\");\n\n    const pool = new Pool({\n      connectionString: process.env.POSTGRES_URL,\n    });\n\n    const db = drizzle(pool);\n\n    // Run migrations from the drizzle directory\n    await migrate(db, { migrationsFolder: \"drizzle\" });\n\n    // Set flag to avoid running migrations again\n    setMigrationFlag();\n\n    logger.info(\"Migrations completed successfully\");\n\n    // Close the pool to avoid hanging connections\n    await pool.end();\n  } catch (error) {\n    logger.error(\"Error running migrations:\", error);\n    throw error;\n  }\n};\n","import { type Route, type IAgentRuntime, logger } from \"@elizaos/core\";\nimport { syncGoogleDriveChanges } from \"../controller\";\n\nexport const gdriveWebhook: Route = {\n  path: \"/gdrive/webhook\",\n  type: \"POST\",\n  handler: async (_req: any, res: any, runtime: IAgentRuntime) => {\n    try {\n      logger.info(\"Google Drive webhook triggered\");\n      const result = await syncGoogleDriveChanges(runtime);\n\n      res.json({\n        message: \"OK\",\n        ...result,\n      });\n    } catch (error) {\n      logger.error(\"Error processing Google Drive webhook:\", error);\n      res.status(500).json({\n        message: \"Error processing webhook\",\n        error: error.message,\n      });\n    }\n  },\n};\n","import { driveSyncTable } from \"src/db\";\nimport { initDriveClient } from \"../services/gdrive\";\nimport { fileMetadataTable } from \"src/db\";\nimport { eq } from \"drizzle-orm\";\nimport { type IAgentRuntime, logger } from \"@elizaos/core\";\n\n// Extract the file processing logic to a reusable function\nexport async function syncGoogleDriveChanges(runtime: IAgentRuntime) {\n  // Get the drive sync data from the database\n  const driveSync = await runtime.db.select().from(driveSyncTable);\n\n  if (driveSync.length === 0) {\n    logger.error(\"No drive sync found, cannot process changes\");\n    throw new Error(\"Drive sync not initialized\");\n  }\n\n  // Get first drive sync record\n  const syncRecord = driveSync[0];\n  const { id: driveId, startPageToken, driveType } = syncRecord;\n\n  // Initialize Drive client with necessary scopes\n  const drive = await initDriveClient([\n    \"https://www.googleapis.com/auth/drive.appdata\",\n    \"https://www.googleapis.com/auth/drive.file\",\n    \"https://www.googleapis.com/auth/drive.metadata.readonly\",\n    \"https://www.googleapis.com/auth/drive\",\n  ]);\n\n  // Prepare parameters for changes.list API call\n  const params: any = {\n    pageToken: startPageToken,\n    includeRemoved: true,\n    fields:\n      \"newStartPageToken, changes(fileId, removed, file(id, name, md5Checksum, size, trashed, mimeType))\",\n  };\n\n  // Add drive-specific parameters based on drive type\n  if (driveType === \"shared_drive\") {\n    params.driveId = driveId;\n    params.supportsAllDrives = true;\n    params.includeItemsFromAllDrives = true;\n  } else if (driveType === \"shared_folder\") {\n    params.spaces = \"drive\";\n    params.restrictToMyDrive = false;\n    params.q = `'${driveId}' in parents`;\n  }\n\n  // Get changes since last sync\n  const changesResponse = await drive.changes.list(params);\n\n  // Log the changes for debugging\n  logger.info(`Found ${changesResponse.data.changes?.length || 0} changes`);\n\n  // Process the changes\n  let processedCount = 0;\n  if (changesResponse.data.changes && changesResponse.data.changes.length > 0) {\n    for (const change of changesResponse.data.changes) {\n      // Skip the last empty change that Google Drive API sometimes includes\n      if (!change.fileId) continue;\n\n      processedCount++;\n\n      // Case 1: File is permanently removed\n      if (change.removed) {\n        // Do nothing as per requirements\n        logger.info(\n          `File ${change.fileId} removed from trash - no action needed`\n        );\n      }\n      // Case 2: File exists but was moved to trash\n      else if (change.file?.trashed) {\n        logger.info(\n          `File ${change.fileId} moved to trash - removing from database`\n        );\n\n        // Delete from the database\n        await runtime.db\n          .delete(fileMetadataTable)\n          .where(eq(fileMetadataTable.id, change.fileId));\n      }\n      // Case 3: New file or modified file that's not in trash\n      else if (change.file && !change.file.trashed) {\n        const file = change.file;\n\n        // Only process PDF files\n        if (file.mimeType === \"application/pdf\") {\n          logger.info(`Processing PDF file: ${file.name}`);\n\n          // Insert or update file in database\n          await runtime.db\n            .insert(fileMetadataTable)\n            .values({\n              id: file.id,\n              hash: file.md5Checksum,\n              fileName: file.name,\n              fileSize: Number(file.size),\n              modifiedAt: new Date(),\n            })\n            .onConflictDoUpdate({\n              target: fileMetadataTable.hash,\n              set: {\n                fileName: file.name,\n                fileSize: Number(file.size),\n                modifiedAt: new Date(),\n                id: file.id,\n              },\n            });\n\n          logger.info(\n            `Saved/updated file metadata for ${file.name} (${file.id})`\n          );\n        } else {\n          logger.info(`Skipping non-PDF file: ${file.name} (${file.mimeType})`);\n        }\n      }\n    }\n  }\n\n  // Save the new token for next time\n  if (changesResponse.data.newStartPageToken) {\n    await runtime.db\n      .update(driveSyncTable)\n      .set({\n        startPageToken: changesResponse.data.newStartPageToken,\n        lastSyncAt: new Date(),\n      })\n      .where(eq(driveSyncTable.id, driveId));\n\n    logger.info(\n      `Updated start page token to: ${changesResponse.data.newStartPageToken}`\n    );\n  }\n\n  return {\n    changes: changesResponse.data.changes?.length || 0,\n    processed: processedCount,\n  };\n}\n","import { type Route, type IAgentRuntime, logger } from \"@elizaos/core\";\nimport { syncGoogleDriveChanges } from \"../controller\";\n\nexport const gdriveManualSync: Route = {\n  path: \"/gdrive/sync\",\n  type: \"GET\",\n  handler: async (_req: any, res: any, runtime: IAgentRuntime) => {\n    try {\n      logger.info(\"Manual Google Drive sync triggered\");\n      const result = await syncGoogleDriveChanges(runtime);\n      while (result.changes > 0) {\n        await syncGoogleDriveChanges(runtime);\n      }\n\n      res.json({\n        message: \"Sync completed successfully\",\n        ...result,\n      });\n    } catch (error) {\n      logger.error(\"Error during manual Google Drive sync:\", error);\n      res.status(500).json({\n        message: \"Error during sync\",\n        error: error.message,\n      });\n    }\n  },\n};\n","import { type Route } from \"@elizaos/core\";\n\nexport const health: Route = {\n  path: \"/health\",\n  type: \"GET\",\n  handler: async (_req: any, res: any) => {\n    res.json({\n      message: \"OK\",\n    });\n  },\n};\n","export * from \"./dkgInsert.ts\";\n"],"mappings":";;;;;;;AACA,SAAS,UAAAA,gBAAc;;;ACDvB,OAAO,YAAY;AAEnB;AAAA,EAIE,UAAAC;AAAA,OAMK;;;ACsKA,IAAM,qBAAqB;AAAA,EAC9B,SAAS;AAAA,EACT,SAAS;AACb;;;ADpKA,OAAO,SAAS;;;AEjBhB,OAAO;;;ACAP,OAAO;AACP,SAAS,iBAAiB;AAE1B,IAAM,SAA6B,QAAQ,IAAI;AAExC,SAAS,YAAuB;AACnC,SAAO,IAAI,UAAU,EAAE,OAAO,CAAC;AACnC;AAEA,eAAsB,iBAClB,QACA,QACA,QAAgB,8BAChB,YAAoB,MACL;AACf,QAAM,WAAW,MAAM,OAAO,SAAS,OAAO;AAAA,IAC1C;AAAA,IACA,YAAY;AAAA,IACZ,UAAU,CAAC,EAAE,MAAM,QAAQ,SAAS,OAAO,CAAC;AAAA,EAChD,CAAC;AAED,MACI,SAAS,WACT,SAAS,QAAQ,SAAS,KAC1B,SAAS,QAAQ,CAAC,EAAE,SAAS,QAC/B;AACE,WAAO,SAAS,QAAQ,CAAC,EAAE;AAAA,EAC/B,OAAO;AACH,UAAM,IAAI,MAAM,mCAAmC;AAAA,EACvD;AACJ;;;AC9BA,SAAS,cAAc;AACvB,OAAO,WAAW;AAClB,YAAY,aAAa;;;AC0DlB,SAAS,YAAY,KAAa;AACrC,MAAI,IAAI,WAAW,kBAAkB,GAAG;AACpC,UAAM,IAAI,QAAQ,oBAAoB,EAAE;AAAA,EAC5C;AACA,QAAM,mBAAmB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,2CAOc,GAAG;AAAA;AAAA;AAG1C,SAAO;AACX;;;AHvEA,SAAS,UAAAC,eAAc;;;AIJvB,OAAOC,YAAW;AAClB,OAAO,cAAc;AAErB,OAAO;AACP,SAAS,UAAAC,eAAc;AAEvB,IAAMC,UAAS,QAAQ,IAAI;AAU3B,eAAsB,2BACpB,WACA,UACAA,SACA;AACA,QAAM,MAAM;AAGZ,QAAM,WAAW,IAAI,SAAS;AAC9B,WAAS,OAAO,SAAS,WAAW,QAAQ;AAC5C,WAAS,OAAO,6BAA6B,MAAM;AACnD,WAAS,OAAO,0BAA0B,IAAI;AAC9C,WAAS,OAAO,YAAY,QAAQ;AAGpC,QAAM,UAAU;AAAA,IACd,wBAAwBA;AAAA,IACxB,GAAG,SAAS,WAAW;AAAA,EACzB;AAEA,EAAAD,QAAO,KAAK,iCAAiC;AAC7C,QAAM,WAAW,MAAMD,OAAM,KAAK,KAAK,UAAU;AAAA,IAC/C;AAAA,IACA,SAAS;AAAA;AAAA,EACX,CAAC;AAED,EAAAC,QAAO,KAAK,oCAAoC;AAChD,SAAO,SAAS;AAClB;;;AC5CO,IAAM,2BAA2B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AA2BjC,IAAM,4BAA4B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AA4ClC,IAAM,0BAA0B;AAAA;AAAA;AAAA;AAAA;AAMhC,IAAM,2BAA2B;AAAA;AAAA;AAAA;AAAA;AAMjC,IAAM,4BAA4B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQlC,IAAM,6BAA6B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAWnC,IAAM,8BAA8B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQpC,IAAM,+BAA+B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASrC,IAAM,+BAA+B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQrC,IAAM,gCAAgC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAStC,IAAM,6BAA6B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAQnC,IAAM,8BAA8B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AASpC,IAAM,8BAA8B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AA+BpC,IAAM,qBAAqB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAiB3B,IAAM,sBAAsB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAyD5B,IAAM,oBAAoB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAiD1B,IAAM,yBAAyB;AAAA,EACpC;AAAA,IACE,OAAO;AAAA,IACP,iBAAiB;AAAA,EACnB;AAAA,EACA;AAAA,IACE,OAAO;AAAA,IACP,iBAAiB;AAAA,EACnB;AACF;AAEO,IAAM,gBAAgB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;AC7RtB,SAAS,kBAAkB,MAAc,eAAuB;AACnE,SAAO;AAAA,sHAC2G,IAAI;AAAA;AAAA,oCAEtF,KAAK,UAAU,aAAa,CAAC;AAAA;AAAA;AAAA;AAAA;AAKjE;AAMO,SAAS,oBAAoB,MAAc,iBAAyB;AACvE,SAAO;AAAA,2HACgH,IAAI;AAAA;AAAA,sCAEzF,KAAK,UAAU,eAAe,CAAC;AAAA;AAAA;AAAA;AAAA;AAKrE;AAMO,SAAS,qBAAqB,MAAc,kBAA0B;AACzE,SAAO;AAAA,oJACyI,IAAI;AAAA;AAAA,uCAEjH,KAAK,UAAU,gBAAgB,CAAC;AAAA;AAAA;AAAA;AAAA;AAKvE;AAMO,SAAS,mBAAmB,MAAc,gBAAwB;AACrE,SAAO;AAAA,yIAC8H,IAAI;AAAA;AAAA,qCAExG,KAAK,UAAU,cAAc,CAAC;AAAA;AAAA;AAAA;AAAA;AAKnE;AAMO,SAAS,sBAAsB,aAAqB;AACvD,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAsBL,wBAAwB;AAAA;AAAA;AAAA,MAGxB,yBAAyB;AAAA;AAAA;AAAA;AAAA;AAAA,MAKzB,KAAK,UAAU,WAAW,CAAC;AAAA;AAAA;AAAA;AAIjC;AAKO,SAAS,qBAAqB,aAAqB;AACtD,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAeL,uBAAuB;AAAA;AAAA;AAAA,MAGvB,wBAAwB;AAAA;AAAA;AAAA,MAGxB,KAAK,UAAU,WAAW,CAAC;AAAA;AAAA;AAAA;AAAA;AAKjC;AAKO,SAAS,uBAAuB,aAAqB;AACxD,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAWL,yBAAyB;AAAA;AAAA;AAAA,MAGzB,0BAA0B;AAAA;AAAA;AAAA;AAAA;AAAA,MAK1B,KAAK,UAAU,WAAW,CAAC;AAAA;AAAA;AAAA;AAIjC;AAMO,SAAS,yBAAyB,aAAqB;AAC1D,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MASL,2BAA2B;AAAA;AAAA;AAAA,MAG3B,4BAA4B;AAAA;AAAA;AAAA;AAAA;AAAA,MAK5B,KAAK,UAAU,WAAW,CAAC;AAAA;AAAA;AAAA;AAAA;AAKjC;AAMO,SAAS,0BAA0B,aAAqB;AAC3D,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MASL,4BAA4B;AAAA;AAAA;AAAA,MAG5B,6BAA6B;AAAA;AAAA;AAAA;AAAA;AAAA,MAK7B,KAAK,UAAU,WAAW,CAAC;AAAA;AAAA;AAAA;AAAA;AAKjC;AAMO,SAAS,wBAAwB,aAAqB;AACzD,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MASL,0BAA0B;AAAA;AAAA;AAAA,MAG1B,2BAA2B;AAAA;AAAA;AAAA;AAAA;AAAA,MAK3B,KAAK,UAAU,WAAW,CAAC;AAAA;AAAA;AAAA;AAAA;AAKjC;AAKO,SAAS,0BAA0B,WAA2B;AACjE,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAOL,sBAAsB;AAAA;AAAA;AAAA,MAGtB,SAAS;AAAA;AAAA;AAAA;AAAA;AAKf;AAKO,SAAS,yBAAyB,iBAAiC;AACtE,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gCAoBqB,kBAAkB;AAAA;AAAA;AAAA;AAAA;AAAA,MAK5C,mBAAmB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,8CAgBqB,eAAe;AAAA;AAAA;AAAA;AAAA;AAK7D;AAKO,SAAS,uBAAuB,uBAA+B;AAClE,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mCAWwB,2BAA2B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAMxD,iBAAiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6CAuBsB,KAAK;AAAA,IAC1C;AAAA,IACA;AAAA,IACA;AAAA,EACJ,CAAC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAOL;AAkGO,SAAS,gCACZ,aACA,UACM;AACN,MAAI,SAAS;AAAA;AAAA;AAEb,cAAY,QAAQ,CAAC,YAAY;AAC7B,UAAM,aAAa,QAAQ,UAAU;AACrC,UAAME,QAAO,QAAQ;AACrB,cAAU,QAAQ,UAAU;AAAA,EAAMA,KAAI;AAAA;AAAA;AAAA,EAC1C,CAAC;AAED,YAAU;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAUR,SAAS,KAAK,IAAI,CAAC;AAAA;AAAA;AAAA,MAGnB,KAAK;AAAA,IACH,SAAS,IAAI,CAAC,YAAY,GAAG,OAAO,cAAc;AAAA,IAClD;AAAA,IACA;AAAA,EACJ,CAAC;AAAA;AAAA;AAAA;AAID,SAAO;AACX;AAMO,SAAS,iCAAiC,OAAe;AAE5D,QAAM,YAAY,KAAK,MAAM,KAAK,UAAU,KAAK,CAAC;AAClD,MAAI,UAAU,YAAY,GAAG;AACzB,WAAO,UAAU,YAAY;AAAA,EACjC;AAEA,SAAO;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAaL,aAAa;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAwBb,KAAK,UAAU,WAAW,MAAM,CAAC,CAAC;AAAA;AAAA;AAAA;AAIxC;AAwFO,IAAM,2BAA2B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACtqBxC,SAAS,UAAAC,eAAc;AACvB,OAAOC,YAA8B;AACrC,OAAO;AAUP,IAAM,oBAAwC,QAAQ,IAAI;AAMnD,SAAS,aAAa,KAA4B;AACvD,QAAM,QAAQ,IAAI,MAAM,YAAY;AACpC,SAAO,QAAQ,MAAM,CAAC,IAAI;AAC5B;AAKA,eAAsB,SACpB,MACA,QACA,kBAA0B,2BACT;AACjB,QAAM,MAAM;AACZ,QAAM,SAAS,EAAE,OAAO,MAAM,OAAO,GAAG,MAAM,EAAE;AAChD,QAAM,UAAU,EAAE,QAAQ,mBAAmB;AAE7C,MAAI,UAAU;AACd,MAAI;AACF,UAAM,cAA6B,MAAMC,OAAM,IAAI,KAAK;AAAA,MACtD;AAAA,MACA;AAAA,IACF,CAAC;AACD,QAAI,YAAY,WAAW,KAAK;AAC9B,YAAM,eAAe,YAAY,KAAK,SAAS,MAAM,GAAG,CAAC,KAAK,CAAC;AAC/D,YAAM,cAAc,kBAAkB,MAAM,YAAY;AAExD,gBAAU,MAAM,iBAAiB,QAAQ,aAAa,eAAe;AAGrE,UAAI,QAAQ,SAAS,KAAK,GAAG;AAC3B,kBAAU,QAAQ,QAAQ,OAAO,KAAK;AAAA,MACxC;AACA,MAAAC,QAAO,KAAK,aAAa,OAAO,eAAe,IAAI,EAAE;AAAA,IACvD,OAAO;AACL,MAAAA,QAAO,KAAK,8BAA8B,YAAY,MAAM,EAAE;AAAA,IAChE;AAAA,EACF,SAAS,OAAO;AACd,IAAAA,QAAO,MAAM,8BAA8B,KAAK,EAAE;AAAA,EACpD;AAEA,SAAO;AACT;AAKA,eAAsB,WACpB,MACA,QACA,kBAA0B,2BACT;AACjB,QAAM,MAAM;AACZ,QAAM,SAAS;AAAA,IACb,GAAG;AAAA,IACH,UAAU;AAAA,EACZ;AACA,QAAM,UAAU,EAAE,QAAQ,mBAAmB;AAE7C,MAAI,UAAU;AACd,MAAI;AACF,UAAM,cAA6B,MAAMD,OAAM,IAAI,KAAK;AAAA,MACtD;AAAA,MACA;AAAA,IACF,CAAC;AAED,QAAI,YAAY,WAAW,KAAK;AAC9B,YAAM,OAAO,YAAY;AACzB,YAAM,QAAQ,KAAK,UAAU,YAAY;AACzC,YAAM,iBACJ,QAAQ,IACJ,KAAK,SAAS,KAAK,MAAM,GAAG,CAAC,EAAE,IAAI,CAAC,eAAe;AAAA,QACjD,YAAY,UAAU;AAAA,QACtB,aAAa,UAAU;AAAA,QACvB,OAAO,UAAU;AAAA,MACnB,EAAE,IACF,CAAC;AAEP,YAAM,gBAAgB,oBAAoB,MAAM,cAAc;AAC9D,gBAAU,MAAM,iBAAiB,QAAQ,eAAe,eAAe;AAGvE,UAAI,QAAQ,SAAS,OAAO,GAAG;AAC7B,kBAAU,QAAQ,QAAQ,SAAS,OAAO;AAAA,MAC5C;AACA,MAAAC,QAAO,KAAK,aAAa,OAAO,eAAe,IAAI,EAAE;AAAA,IACvD,OAAO;AACL,MAAAA,QAAO,MAAM,8BAA8B,YAAY,MAAM,EAAE;AAAA,IACjE;AAAA,EACF,SAAS,OAAO;AACd,IAAAA,QAAO,MAAM,8BAA8B,KAAK,EAAE;AAAA,EACpD;AAEA,SAAO;AACT;AAKA,eAAsB,YACpB,MACA,QACA,kBAA0B,2BACT;AACjB,QAAM,MAAM;AACZ,QAAM,SAAS;AAAA,IACb,GAAG;AAAA,IACH,UAAU;AAAA,EACZ;AACA,QAAM,UAAU,EAAE,QAAQ,mBAAmB;AAE7C,MAAI,UAAU;AACd,MAAI;AACF,UAAM,cAA6B,MAAMD,OAAM,IAAI,KAAK;AAAA,MACtD;AAAA,MACA;AAAA,IACF,CAAC;AAED,QAAI,YAAY,WAAW,KAAK;AAC9B,YAAM,OAAO,YAAY;AACzB,YAAM,QAAQ,KAAK,UAAU,YAAY;AACzC,YAAM,kBACJ,QAAQ,IACJ,KAAK,SAAS,KAAK,MAAM,GAAG,CAAC,EAAE,IAAI,CAAC,eAAe;AAAA,QACjD,YAAY,UAAU;AAAA,QACtB,aAAa,UAAU;AAAA,QACvB,OAAO,UAAU;AAAA,MACnB,EAAE,IACF,CAAC;AAEP,YAAM,iBAAiB,qBAAqB,MAAM,eAAe;AACjE,gBAAU,MAAM,iBAAiB,QAAQ,gBAAgB,eAAe;AAGxE,UAAI,QAAQ,SAAS,QAAQ,GAAG;AAC9B,kBAAU,QAAQ,QAAQ,UAAU,QAAQ;AAAA,MAC9C;AACA,MAAAC,QAAO,KAAK,aAAa,OAAO,eAAe,IAAI,EAAE;AAAA,IACvD,OAAO;AACL,MAAAA,QAAO,MAAM,8BAA8B,YAAY,MAAM,EAAE;AAAA,IACjE;AAAA,EACF,SAAS,OAAO;AACd,IAAAA,QAAO,MAAM,8BAA8B,KAAK,EAAE;AAAA,EACpD;AAEA,SAAO;AACT;AAKA,eAAsB,UACpB,MACA,QACA,kBAA0B,2BACT;AACjB,QAAM,MAAM;AACZ,QAAM,SAAS;AAAA,IACb,GAAG;AAAA,IACH,YAAY;AAAA,IACZ,QAAQ;AAAA,EACV;AACA,QAAM,UAAU,EAAE,QAAQ,mBAAmB;AAE7C,MAAI,UAAU;AACd,MAAI;AACF,UAAM,cAA6B,MAAMD,OAAM,IAAI,KAAK;AAAA,MACtD;AAAA,MACA;AAAA,IACF,CAAC;AAED,QAAI,YAAY,WAAW,KAAK;AAC9B,YAAM,OAAO,YAAY;AACzB,UAAI,gBAAgB,CAAC;AACrB,UAAI,KAAK,cAAc,KAAK,WAAW,SAAS,GAAG;AACjD,wBAAgB,KAAK,WAAW,IAAI,CAAC,eAAe;AAAA,UAClD,YAAY,aAAa,UAAU,KAAK,CAAC;AAAA,UACzC,aAAa;AAAA,UACb,OAAO,UAAU,WAAW;AAAA,QAC9B,EAAE;AAAA,MACJ;AACA,YAAM,eAAe,mBAAmB,MAAM,aAAa;AAC3D,gBAAU,MAAM,iBAAiB,QAAQ,cAAc,eAAe;AAEtE,MAAAC,QAAO,KAAK,aAAa,OAAO,eAAe,IAAI,EAAE;AAAA,IACvD,OAAO;AACL,MAAAA,QAAO,MAAM,8BAA8B,YAAY,MAAM,EAAE;AAAA,IACjE;AAAA,EACF,SAAS,OAAO;AACd,IAAAA,QAAO,MAAM,8BAA8B,KAAK,EAAE;AAAA,EACpD;AAEA,SAAO;AACT;AAKA,eAAsB,cAAc,MAAM,QAAmB;AAC3D,aAAW,SAAS,MAAM;AACxB,UAAM,gBAAgB,MAAM,SAAS,MAAM,SAAS,MAAM;AAC1D,UAAM,UAAU,EAAE,MAAM,MAAM,SAAS,IAAI,cAAc;AAEzD,UAAM,eAAe,MAAM,SAAS,MAAM,QAAQ,MAAM;AACxD,UAAM,SAAS,EAAE,MAAM,MAAM,QAAQ,IAAI,aAAa;AAAA,EACxD;AAEA,SAAO,KAAK;AAAA,IACV,CAAC,UAAU,MAAM,YAAY,UAAU,MAAM,WAAW;AAAA,EAC1D;AACF;AAKA,eAAsB,gBAAgB,MAAM,QAAmB;AAC7D,aAAW,SAAS,MAAM;AACxB,UAAM,gBAAgB,MAAM,WAAW,MAAM,SAAS,MAAM;AAC5D,UAAM,aAAa;AAAA,EACrB;AAEA,SAAO,KAAK,OAAO,CAAC,UAAU,MAAM,eAAe,MAAM;AAC3D;AAKA,eAAsB,iBAAiB,MAAM,QAAmB;AAC9D,aAAW,SAAS,MAAM;AACxB,UAAM,iBAAiB,MAAM,YAAY,MAAM,UAAU,MAAM;AAC/D,UAAM,cAAc;AAAA,EACtB;AAEA,SAAO,KAAK,OAAO,CAAC,UAAU,MAAM,gBAAgB,MAAM;AAC5D;AAKA,eAAsB,eAAe,MAAM,QAAmB;AAC5D,aAAW,SAAS,MAAM;AACxB,UAAM,aAAa,MAAM,UAAU,MAAM,MAAM,MAAM;AACrD,UAAM,UAAU;AAAA,EAClB;AAEA,SAAO,KAAK,OAAO,CAAC,UAAU,MAAM,YAAY,MAAM;AACxD;;;ACjQO,SAAS,sBAAsB,OAA8B;AAGhE,QAAM,QAAQ,MAAM,MAAM,gBAAgB;AAC1C,SAAO,QAAQ,MAAM,CAAC,IAAI;AAC9B;AASO,SAAS,aAAa,OAAwB;AACjD,SAAO,MAAM,KAAK,MAAM;AAC5B;;;ACFA,SAAS,UAAAC,eAAc;AA6CvB,IAAM,mBAAmB;AAKzB,eAAsB,gBACpB,QACA,aAC0B;AAC1B,QAAM,iBAAiB;AAAA,IACrB;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AAEA,QAAM,SAAS,eAAe,IAAI,CAAC,UAAU,MAAM,YAAY,CAAC;AAChE,QAAM,qBAA+C,CAAC;AACtD,QAAM,gBAAwC,CAAC;AAE/C,aAAW,SAAS,QAAQ;AAC1B,uBAAmB,KAAK,IAAI,CAAC;AAC7B,UAAM,gBAAgB,eAAe;AAAA,MACnC,CAAC,SAAS,KAAK,YAAY,MAAM;AAAA,IACnC;AACA,kBAAc,KAAK,IAAI,gBAAgB,gBAAgB;AAAA,EACzD;AAEA,aAAW,WAAW,aAAa;AACjC,UAAM,cAAc,QAAQ,SAAS;AACrC,UAAM,YAAY,QAAQ,KAAK,YAAY;AAE3C,eAAW,SAAS,QAAQ;AAC1B,UAAI,UAAU,SAAS,KAAK,GAAG;AAE7B,YACE,gBAAgB,KAChB,UAAU,cACV,UAAU,gBACV;AACA;AAAA,QACF;AACA,2BAAmB,KAAK,EAAE,KAAK,WAAW;AAAA,MAC5C;AAAA,IACF;AAAA,EACF;AAGA,QAAM,gBAAgB,CAAC,yBAAyB,sBAAsB;AACtE,MAAI,mBAAmB,SAAS,EAAE,WAAW,GAAG;AAC9C,eAAW,SAAS,eAAe;AACjC,UAAI,mBAAmB,KAAK,KAAK,mBAAmB,KAAK,EAAE,SAAS,GAAG;AACrE,2BAAmB,SAAS,IAAI,mBAAmB,KAAK;AACxD;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAGA,aAAW,SAAS,eAAe;AACjC,QAAI,SAAS,oBAAoB;AAC/B,aAAO,mBAAmB,KAAK;AAC/B,YAAM,MAAM,OAAO,QAAQ,KAAK;AAChC,UAAI,OAAO,GAAG;AACZ,eAAO,OAAO,KAAK,CAAC;AAAA,MACtB;AACA,YAAM,YAAY,cAAc,KAAK;AACrC,YAAM,eAAe,eAAe,QAAQ,SAAS;AACrD,UAAI,gBAAgB,GAAG;AACrB,uBAAe,OAAO,cAAc,CAAC;AAAA,MACvC;AAAA,IACF;AAAA,EACF;AAGA,QAAM,mBAAuD,CAAC;AAC9D,aAAW,CAAC,OAAO,KAAK,KAAK,OAAO,QAAQ,kBAAkB,GAAG;AAC/D,qBAAiB,KAAK,IAAI,MAAM,SAAS,IAAI,MAAM,CAAC,IAAI;AAAA,EAC1D;AAEA,aAAW,OAAO,OAAO,KAAK,gBAAgB,GAAG;AAC/C,QAAI,iBAAiB,GAAG,MAAM,QAAW;AACvC,aAAO,iBAAiB,GAAG;AAAA,IAC7B;AAAA,EACF;AAEA,QAAM,gBAAgB,OAAO,QAAQ,gBAAgB,EAAE,KAAK,CAAC,GAAG,MAAM;AACpE,UAAM,OAAO,EAAE,CAAC,MAAM,SAAY,WAAW,EAAE,CAAC;AAChD,UAAM,OAAO,EAAE,CAAC,MAAM,SAAY,WAAW,EAAE,CAAC;AAChD,WAAQ,OAAmB;AAAA,EAC7B,CAAC;AAED,QAAM,oBAAqC,CAAC;AAG5C,WAAS,IAAI,GAAG,IAAI,cAAc,QAAQ,KAAK;AAC7C,UAAM,CAAC,OAAO,YAAY,IAAI,cAAc,CAAC;AAC7C,QAAI,gBAAgB,KAAM;AAC1B,QAAI,aAAa;AACjB,QAAI;AAGJ,QAAI,MAAM,GAAG;AACX,mBAAa;AAAA,IACf;AAGA,QAAI,MAAM,cAAc,SAAS,GAAG;AAClC,iBAAW,YAAY,YAAY,SAAS,CAAC,EAAE,SAAS;AAAA,IAC1D,OAAO;AACL,YAAM,CAAC,GAAG,SAAS,IAAI,cAAc,IAAI,CAAC;AAC1C,UAAI,aAAa,QAAQ,aAAa,YAAY;AAChD,mBAAW;AAAA,MACb,OAAO;AACL,mBAAW;AAAA,MACb;AAAA,IACF;AACA,sBAAkB,KAAK,IAAI,CAAC,YAAY,QAAQ;AAAA,EAClD;AAGA,QAAM,uBAAiC,OAAO;AAAA,IAC5C,CAAC,UAAU,EAAE,SAAS;AAAA,EACxB;AAEA,MAAI,qBAAqB,SAAS,GAAG;AACnC,UAAM,SAAS;AAAA,MACb,YAAY,IAAI,CAAC,QAAQ;AAAA,QACvB,UAAU,EAAE,aAAa,GAAG,SAAS,YAAY;AAAA,QACjD,MAAM,GAAG;AAAA,MACX,EAAE;AAAA,MACF,qBAAqB,IAAI,CAAC,QAAQ,cAAc,GAAG,CAAC;AAAA,IACtD;AAEA,UAAM,SAAS,MAAM;AAAA,MACnB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,UAAM,cAAc,OAAO,MAAM,IAAI;AAErC,UAAM,+BAAgD,CAAC;AAEvD,eAAW,QAAQ,aAAa;AAE9B,YAAM,QAAQ,KAAK,KAAK,EAAE,MAAM,0BAA0B;AAC1D,UAAI,OAAO;AACT,cAAM,aAAa,MAAM,CAAC,EAAE,YAAY;AACxC,cAAM,YAAY,SAAS,MAAM,CAAC,GAAG,EAAE;AACvC,cAAM,UAAU,SAAS,MAAM,CAAC,GAAG,EAAE;AACrC,qCAA6B,UAAU,IAAI,CAAC,WAAW,OAAO;AAAA,MAChE;AAAA,IACF;AAGA,eAAW,CAAC,OAAO,KAAK,KAAK,OAAO,QAAQ,4BAA4B,GAAG;AACzE,wBAAkB,KAAK,IAAI;AAAA,IAC7B;AAAA,EACF;AAGA,QAAM,kBAAkB,OAAO,KAAK,iBAAiB,EAClD,IAAI,CAAC,QAAQ,cAAc,GAAG,CAAC,EAC/B,OAAO,CAAC,SAAS,QAAQ,IAAI;AAEhC,QAAM,gBAAgB,eAAe;AAAA,IACnC,CAAC,QAAQ,CAAC,gBAAgB,SAAS,GAAG;AAAA,EACxC;AAEA,MAAI,cAAc,SAAS,GAAG;AAC5B,IAAAA,QAAO,KAAK,qBAAqB,cAAc,KAAK,IAAI,CAAC,EAAE;AAAA,EAC7D,OAAO;AACL,IAAAA,QAAO,KAAK,gCAAgC,iBAAiB;AAAA,EAC/D;AAGA,MAAI,kBAAkB,mBAAmB;AACvC,UAAM,CAAC,GAAG,OAAO,IAAI,kBAAkB,cAAc;AACrD,sBAAkB,cAAc,IAAI,CAAC,GAAG,OAAO;AAAA,EACjD;AAEA,SAAO;AACT;AAKA,eAAsB,0BACpB,QACA,YACiB;AACjB,QAAM,aAAa,MAAM;AAAA,IACvB,oBAAI,IAAI,CAAC,GAAG,WAAW,cAAc,GAAG,WAAW,QAAQ,CAAC;AAAA,EAC9D;AACA,MAAI;AACF,UAAM,oBAAoB,sBAAsB,UAAU;AAC1D,UAAM,4BAA4B,MAAM;AAAA,MACtC;AAAA,MACA;AAAA,IACF;AACA,IAAAA,QAAO;AAAA,MACL,qCAAqC,yBAAyB;AAAA,IAChE;AACA,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,uCAAuC,CAAC;AACrD,WAAO;AAAA,EACT;AACF;AAKA,eAAsB,sBACpB,QACA,YACiB;AACjB,MAAI;AACF,UAAM,mBAAmB,qBAAqB,WAAW,SAAS;AAClE,UAAM,sBAAsB,MAAM;AAAA,MAChC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AACA,IAAAA,QAAO,KAAK,oCAAoC,mBAAmB,EAAE;AACrE,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,iCAAiC,CAAC;AAC/C,WAAO;AAAA,EACT;AACF;AAKA,eAAsB,2BACpB,QACA,YACiB;AACjB,MAAI;AACF,UAAM,WAAW,MAAM;AAAA,MACrB,oBAAI,IAAI;AAAA,QACN,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,MAChB,CAAC;AAAA,IACH;AACA,UAAM,kBAAkB,uBAAuB,QAAQ;AACvD,QAAI,0BAA0B,MAAM;AAAA,MAClC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AACA,IAAAA,QAAO;AAAA,MACL,sCAAsC,uBAAuB;AAAA,IAC/D;AAEA,QAAI;AACJ,QAAI;AACF,2BAAqB,KAAK,MAAM,uBAAuB;AAAA,IACzD,QAAQ;AACN,2BAAqB,CAAC;AAAA,IACxB;AAGA,UAAM,mBAAmB,MAAM,cAAc,oBAAoB,MAAM;AACvE,8BAA0B,KAAK,UAAU,gBAAgB;AACzD,IAAAA,QAAO,KAAK,oCAAoC,uBAAuB,EAAE;AACzE,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,gCAAgC,CAAC;AAC9C,WAAO;AAAA,EACT;AACF;AAKA,eAAsB,6BACpB,QACA,YACiB;AACjB,MAAI;AACF,UAAM,aAAa,MAAM;AAAA,MACvB,oBAAI,IAAI;AAAA,QACN,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,MAChB,CAAC;AAAA,IACH;AACA,UAAM,kBAAkB,yBAAyB,UAAU;AAC3D,UAAM,0BAA0B,MAAM;AAAA,MACpC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AACA,IAAAA,QAAO;AAAA,MACL,wCAAwC,uBAAuB;AAAA,IACjE;AAEA,QAAI;AACJ,QAAI;AACF,2BAAqB,KAAK,MAAM,uBAAuB;AAAA,IACzD,QAAQ;AACN,2BAAqB,CAAC;AAAA,IACxB;AACA,UAAM,mBAAmB,MAAM,gBAAgB,oBAAoB,MAAM;AACzE,UAAM,YAAY,KAAK,UAAU,gBAAgB;AACjD,IAAAA,QAAO,KAAK,sCAAsC,SAAS,EAAE;AAC7D,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,gCAAgC,CAAC;AAC9C,WAAO;AAAA,EACT;AACF;AAKA,eAAsB,8BACpB,QACA,YACiB;AACjB,MAAI;AACF,UAAM,cAAc,MAAM;AAAA,MACxB,oBAAI,IAAI;AAAA,QACN,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,MAChB,CAAC;AAAA,IACH;AACA,UAAM,kBAAkB,0BAA0B,WAAW;AAC7D,UAAM,0BAA0B,MAAM;AAAA,MACpC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AACA,IAAAA,QAAO;AAAA,MACL,yCAAyC,uBAAuB;AAAA,IAClE;AAEA,QAAI;AACJ,QAAI;AACF,2BAAqB,KAAK,MAAM,uBAAuB;AAAA,IACzD,QAAQ;AACN,2BAAqB,CAAC;AAAA,IACxB;AACA,UAAM,mBAAmB,MAAM,iBAAiB,oBAAoB,MAAM;AAC1E,UAAM,YAAY,KAAK,UAAU,gBAAgB;AACjD,IAAAA,QAAO,KAAK,uCAAuC,SAAS,EAAE;AAC9D,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,gCAAgC,CAAC;AAC9C,WAAO;AAAA,EACT;AACF;AAKA,eAAsB,4BACpB,QACA,YACiB;AACjB,MAAI;AACF,UAAM,YAAY,MAAM;AAAA,MACtB,oBAAI,IAAI;AAAA,QACN,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,QACd,GAAG,WAAW;AAAA,MAChB,CAAC;AAAA,IACH;AACA,UAAM,kBAAkB,wBAAwB,SAAS;AACzD,UAAM,0BAA0B,MAAM;AAAA,MACpC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AACA,IAAAA,QAAO;AAAA,MACL,uCAAuC,uBAAuB;AAAA,IAChE;AAEA,QAAI;AACJ,QAAI;AACF,2BAAqB,KAAK,MAAM,uBAAuB;AAAA,IACzD,QAAQ;AACN,2BAAqB,CAAC;AAAA,IACxB;AACA,UAAM,mBAAmB,MAAM,eAAe,oBAAoB,MAAM;AACxE,UAAM,YAAY,KAAK,UAAU,gBAAgB;AACjD,IAAAA,QAAO,KAAK,qCAAqC,SAAS,EAAE;AAC5D,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,gCAAgC,CAAC;AAC9C,WAAO;AAAA,EACT;AACF;AAKA,eAAsB,cACpB,QACA,YAC6B;AAC7B,QAAM;AAAA,IACJ;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,IAAI,MAAM,QAAQ,IAAI;AAAA,IACpB,0BAA0B,QAAQ,UAAU;AAAA,IAC5C,sBAAsB,QAAQ,UAAU;AAAA,IACxC,2BAA2B,QAAQ,UAAU;AAAA,IAC7C,6BAA6B,QAAQ,UAAU;AAAA,IAC/C,8BAA8B,QAAQ,UAAU;AAAA,IAChD,4BAA4B,QAAQ,UAAU;AAAA,EAChD,CAAC;AAED,SAAO;AAAA,IACL;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACF;AAKO,SAAS,yBAAyB,aAA6B;AACpE,QAAM,iBAAiB,YAAY,YAAY,IAAI;AACnD,MAAI,mBAAmB,IAAI;AACzB,kBAAc,YAAY,MAAM,GAAG,iBAAiB,CAAC;AAAA,EACvD;AAEA,MAAI,YAAY,SAAS,GAAG,GAAG;AAC7B,kBAAc,YAAY,MAAM,GAAG,EAAE;AAAA,EACvC;AAEA,SAAO,cAAc;AACvB;AAKA,eAAsB,uBACpB,QACA,gBACoB;AACpB,QAAM,wBAAwB,0BAA0B,cAAc;AACtE,QAAM,gCAAgC,MAAM;AAAA,IAC1C;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACA,EAAAA,QAAO;AAAA,IACL,yCAAyC,6BAA6B;AAAA,EACxE;AAEA,MAAI;AACF,WAAO,KAAK,MAAM,6BAA6B;AAAA,EACjD,QAAQ;AACN,UAAM,kBAAkB;AAAA,MACtB;AAAA,IACF;AACA,IAAAA,QAAO,KAAK,oBAAoB,eAAe,EAAE;AACjD,WAAO,KAAK,MAAM,eAAe;AAAA,EACnC;AACF;AAKA,eAAsB,wBACpB,QACA,iBACiB;AACjB,MAAI,aAAa,eAAe,GAAG;AACjC,WAAO;AAAA,EACT;AAEA,QAAM,wBAAwB,yBAAyB,eAAe;AACtE,QAAM,uBAAuB,MAAM;AAAA,IACjC;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACA,EAAAA,QAAO,KAAK,qCAAqC,oBAAoB,EAAE;AAEvE,MAAI,cAAc,qBAAqB,KAAK;AAC5C,MAAI,YAAY,WAAW,SAAS,KAAK,YAAY,SAAS,KAAK,GAAG;AACpE,kBAAc,YAAY,MAAM,GAAG,EAAE,EAAE,KAAK;AAAA,EAC9C;AACA,SAAO;AACT;AAKA,eAAsB,gBACpB,QACA,uBACoB;AACpB,MAAI;AACF,QAAI,aAAa,qBAAqB,GAAG;AACvC,aAAO,CAAC;AAAA,IACV;AACA,UAAM,sBAAsB,uBAAuB,qBAAqB;AACxE,UAAM,uBAAuB,MAAM;AAAA,MACjC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AACA,IAAAA,QAAO,KAAK,sCAAsC,oBAAoB,EAAE;AAExE,UAAM,oBAAoB,sBAAsB,oBAAoB;AACpE,QAAI,sBAAsB,MAAM;AAC9B,aAAO,CAAC;AAAA,IACV;AACA,WAAO,KAAK,MAAM,iBAAiB;AAAA,EACrC,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,gCAAgC,CAAC;AAC9C,WAAO,CAAC;AAAA,EACV;AACF;AAKO,SAAS,kBAAkB,yBAA4C;AAC5E,MAAI;AACF,UAAM,WAAW,KAAK,MAAM,uBAAuB;AACnD,QAAI,CAAC,MAAM,QAAQ,QAAQ,EAAG,QAAO,CAAC;AAEtC,UAAM,MAAM,SAAS,IAAI,CAAC,UAAqC;AAAA,MAC7D,OAAO,kCAAkC,KAAK,YAAY,CAAC;AAAA,MAC3D,iBAAiB,KAAK,SAAS;AAAA,MAC/B,uBAAuB,KAAK,UAAU;AAAA,IACxC,EAAE;AACF,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,kCAAkC,CAAC;AAChD,WAAO,CAAC;AAAA,EACV;AACF;AAKO,SAAS,mBACd,0BACW;AACX,MAAI;AACF,UAAM,YAAY,KAAK,MAAM,wBAAwB;AACrD,QAAI,CAAC,MAAM,QAAQ,SAAS,EAAG,QAAO,CAAC;AAEvC,UAAM,MAAM,UAAU,IAAI,CAAC,UAAqC;AAAA,MAC9D,OAAO,kCAAkC,KAAK,aAAa,CAAC;AAAA,MAC5D,iBAAiB,KAAK,UAAU;AAAA,MAChC,uBAAuB,KAAK,UAAU;AAAA,IACxC,EAAE;AACF,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,mCAAmC,CAAC;AACjD,WAAO,CAAC;AAAA,EACV;AACF;AAKO,SAAS,iBAAiB,wBAA2C;AAC1E,MAAI;AACF,UAAM,UAAU,KAAK,MAAM,sBAAsB;AACjD,QAAI,CAAC,MAAM,QAAQ,OAAO,EAAG,QAAO,CAAC;AAErC,UAAM,MAAM,QAAQ,IAAI,CAAC,UAAqC;AAAA,MAC5D,OAAO,4CAA4C,KAAK,SAAS,CAAC;AAAA,MAClE,iBAAiB,KAAK,MAAM;AAAA,MAC5B,uBAAuB,KAAK,UAAU;AAAA,IACxC,EAAE;AACF,WAAO;AAAA,EACT,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,iCAAiC,CAAC;AAC/C,WAAO,CAAC;AAAA,EACV;AACF;AAKA,eAAsB,aACpB,QACA,iBACA,gBACA,UACoC;AACpC,QAAM,EAAE,IAAI,MAAM,OAAO,IAAI,IAAI;AAEjC,MAAI,kBAA6C,CAAC;AAElD,MAAI;AACF,UAAM,uBAAuB,MAAM;AAAA,MACjC;AAAA,MACA;AAAA,IACF;AACA,sBAAkB,KAAK,MAAM,oBAAoB;AAAA,EAInD,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,8BAA8B,CAAC;AAAA,EAC9C;AAGA,MAAI,CAAC,gBAAgB,iBAAiB,GAAG;AACvC,oBAAgB,iBAAiB,IAAI,CAAC;AAAA,EACxC;AAGA,QAAM,UAAU,MAAM,gBAAgB,QAAQ,EAAE;AAChD,QAAM,YAAY,kBAAkB,IAAI;AACxC,QAAM,aAAa,mBAAmB,KAAK;AAC3C,QAAM,WAAW,iBAAiB,GAAG;AAGrC,QAAM,SAAS,gBAAgB,iBAAiB;AAChD,MAAI,MAAM,QAAQ,MAAM,GAAG;AACzB,QAAI,MAAM,QAAQ,OAAO,EAAG,QAAO,KAAK,GAAG,OAAO;AAClD,QAAI,MAAM,QAAQ,SAAS,EAAG,QAAO,KAAK,GAAG,SAAS;AACtD,QAAI,MAAM,QAAQ,UAAU,EAAG,QAAO,KAAK,GAAG,UAAU;AACxD,QAAI,MAAM,QAAQ,QAAQ,EAAG,QAAO,KAAK,GAAG,QAAQ;AAAA,EACtD;AAEA,MAAI;AACF,UAAM,gBAAgB,MAAM,uBAAuB,QAAQ,cAAc;AACzE,oBAAgB,YAAY,IAAI;AAAA,EAClC,SAAS,GAAG;AACV,IAAAA,QAAO,MAAM,8BAA8B,CAAC;AAAA,EAC9C;AAGA,QAAM,MAAM,gBAAgB,oBAAoB;AAChD,MAAI,OAAO,QAAQ,mCAAmC;AACpD,oBAAgB,KAAK,IAAI;AAAA,EAC3B,OAAO;AACL,oBAAgB,KAAK,IAAI;AAAA,EAC3B;AAEA,SAAO;AACT;AAKO,SAAS,sBACd,aACA,gBACW;AACX,QAAM,qBAA+B,CAAC;AACtC,QAAM,iBAA2B,CAAC;AAClC,QAAM,gBAA0B,CAAC;AACjC,QAAM,gBAA0B,CAAC;AACjC,QAAM,mBAA6B,CAAC;AAEpC,aAAW,WAAW,aAAa;AACjC,UAAM,cAAc,QAAQ,SAAS;AACrC,UAAMC,QAAO,QAAQ;AAGrB,QACE,kBAAkB,kBAClB,eAAe,eAAe,cAAc,EAAE,CAAC,KAC/C,eAAe,eAAe,cAAc,EAAE,CAAC,GAC/C;AACA,yBAAmB,KAAKA,KAAI;AAAA,IAC9B;AAEA,QACE,cAAc,kBACd,eAAe,eAAe,UAAU,EAAE,CAAC,KAC3C,eAAe,eAAe,UAAU,EAAE,CAAC,GAC3C;AACA,qBAAe,KAAKA,KAAI;AAAA,IAC1B;AAEA,QACE,aAAa,kBACb,eAAe,eAAe,SAAS,EAAE,CAAC,KAC1C,eAAe,eAAe,SAAS,EAAE,CAAC,GAC1C;AACA,oBAAc,KAAKA,KAAI;AAAA,IACzB;AAEA,QACE,aAAa,kBACb,eAAe,eAAe,SAAS,EAAE,CAAC,KAC1C,eAAe,eAAe,SAAS,EAAE,CAAC,GAC1C;AACA,oBAAc,KAAKA,KAAI;AAAA,IACzB;AAEA,QACE,gBAAgB,kBAChB,eAAe,eAAe,YAAY,EAAE,CAAC,KAC7C,eAAe,eAAe,YAAY,EAAE,CAAC,GAC7C;AACA,uBAAiB,KAAKA,KAAI;AAAA,IAC5B;AAAA,EACF;AAEA,SAAO;AAAA,IACL,cAAc;AAAA,IACd,UAAU;AAAA,IACV,SAAS;AAAA,IACT,SAAS;AAAA,IACT,YAAY;AAAA,IACZ,WAAW,CAAC;AAAA,EACd;AACF;AAKA,eAAsB,iBACpB,aACA,QACoB;AACpB,QAAM,iBAAiB,MAAM,gBAAgB,QAAQ,WAAW;AAChE,QAAM,mBAAmB,sBAAsB,aAAa,cAAc;AAE1E,QAAM,WAAW,YAAY,YAAY,SAAS,CAAC,EAAE,SAAS;AAC9D,mBAAiB,YAAY,YAC1B;AAAA,IACC,CAAC,OACC,GAAG,SAAS,eAAe,WAAW,oBACtC,OAAO,GAAG,SAAS;AAAA,EACvB,EACC,IAAI,CAAC,OAAO,GAAG,IAAI;AAEtB,SAAO;AACT;;;AC1zBA,SAAS,UAAAC,eAAc;AAiCvB,eAAsB,WACpB,QACA,OACiB;AACjB,MAAI,UAAU;AACd,MAAI;AACF,UAAM,SAAS,iCAAiC,KAAK;AACrD,cAAU,MAAM,iBAAiB,QAAQ,MAAM;AAC/C,IAAAA,QAAO,KAAK,wCAAwC,OAAO,EAAE;AAAA,EAC/D,SAAS,OAAO;AACd,IAAAA,QAAO,MAAM,qCAAqC,KAAK;AACvD,cAAU;AAAA,EACZ;AACA,SAAO;AACT;;;AV3CA,SAAS,gBAAgB;AACzB,OAAO,QAAQ;AAGf,IAAM,qBAAqB,QAAQ,IAAI;AAgCvC,eAAsB,YAAY,SAA8B,KAAa;AAC3E,QAAM,SAAS,UAAU;AAEzB,QAAM,iBAAiB,MAAM,iBAAiB,SAAS,MAAM;AAE7D,QAAM;AAAA,IACJ;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,IAAI,MAAM,cAAc,QAAQ,cAAc;AAE9C,QAAM,iBAAiB,MAAM;AAAA,IAC3B;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,MACE,IAAI;AAAA,MACJ,MAAM;AAAA,MACN,OAAO;AAAA,MACP,KAAK;AAAA,IACP;AAAA,EACF;AAEA,iBAAe,iBAAiB,IAAI,MAAM,WAAW,QAAQ,cAAc;AAE3E,iBAAe,KAAK,IAAI,mBAAmB,GAAG;AAc9C,QAAM,UAAU,eAAe,UAAU;AACzC,MAAI,EAAE,YAAY,UAAU;AAC1B,YAAQ,QAAQ,IAAI;AACpB,IAAAC,QAAO,KAAK,kCAAkC;AAAA,EAChD;AAEA,SAAO;AAET;AAmBA,SAAS,wBAA2B,MAAS,WAAuB;AAElE,MAAI,cAAc,YAAY;AAC5B,WAAO;AAAA,EACT;AAGA,MAAI,MAAM,QAAQ,IAAI,GAAG;AACvB,WAAO,KAAK;AAAA,MAAI,CAAC,SACf,wBAAwB,MAAM,SAAS;AAAA,IACzC;AAAA,EACF;AAGA,MAAI,SAAS,QAAQ,OAAO,SAAS,UAAU;AAC7C,UAAM,SAAkC,CAAC;AACzC,eAAW,OAAO,MAAM;AACtB,UAAI,OAAO,UAAU,eAAe,KAAK,MAAM,GAAG,GAAG;AACnD,eAAO,GAAG,IAAI;AAAA,UACX,KAAiC,GAAG;AAAA,UACrC;AAAA,QACF;AAAA,MACF;AAAA,IACF;AACA,WAAO;AAAA,EACT;AAGA,MAAI,OAAO,SAAS,UAAU;AAE5B,QAAI,cAAc,SAAS;AACzB,aAAO;AAAA,IACT;AAGA,QAAI,uBAAuB,KAAK,IAAI,GAAG;AACrC,aAAO;AAAA,IACT;AAGA,WAAO,KAAK,QAAQ,MAAM,EAAE;AAAA,EAC9B;AAGA,SAAO;AACT;AACA,IAAM,UAAU;AAAA,EACd,SACE;AAAA,EACF,WACE;AAAA,EACF,QACE;AAAA,EACF,WACE;AAAA,EACF,SACE;AAAA,EACF,SACE;AAAA,EACF,gBACE;AAAA,EACF,WACE;AAAA,EACF,mBACE;AAAA,EACF,uBACE;AACJ;AA6BA,eAAe,kBAAkB,QAAiB;AAChD,QAAM,SAAS,UAAU;AACzB,QAAM,WAAW,MAAM,OAAO,SAAS,OAAO;AAAA,IAC5C,OAAO;AAAA,IACP,UAAU;AAAA,MACR;AAAA,QACE,MAAM;AAAA,QACN,SAAS;AAAA,UACP,GAAG;AAAA,UACH;AAAA,YACE,MAAM;AAAA,YACN,MAAM;AAAA,UACR;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAAA,IACA,YAAY;AAAA,EACd,CAAC;AACD,SAAO,SAAS,QAAQ,CAAC,EAAE,SAAS,SAChC,SAAS,QAAQ,CAAC,EAAE,OACpB;AACN;AAEA,eAAe,mBAAmB,QAAiB;AACjD,QAAM,SAAS,UAAU;AACzB,QAAM,WAAW,MAAM,OAAO,SAAS,OAAO;AAAA,IAC5C,OAAO;AAAA,IACP,QAAQ;AAAA,IACR,UAAU;AAAA,MACR;AAAA,QACE,MAAM;AAAA,QACN,SAAS,CAAC,GAAG,MAAM;AAAA,MACrB;AAAA,IACF;AAAA,IACA,YAAY;AAAA,EACd,CAAC;AACD,SAAO,SAAS,QAAQ,CAAC,EAAE,SAAS,SAChC,SAAS,QAAQ,CAAC,EAAE,OACpB;AACN;AAEA,eAAsB,kBAAkB,SAAiB,WAAsB;AAC7E,QAAM,UAAU;AAAA,IACd,SAAS;AAAA,IACT,QAAQ;AAAA,IACR,OAAO;AAAA,IACP,QAAQ;AAAA,EACV;AACA,QAAM,UAAU,SAAS,SAAS,OAAO;AACzC,EAAAC,QAAO,KAAK,cAAc,OAAO,YAAY;AAE7C,QAAM,eAAe,MAAM,QAAQ,KAAK,IAAI,EAAE,cAAc,SAAS,CAAC;AAEtE,QAAM,gBAAgB,aACnB,OAAO,CAAC,SAAS,KAAK,MAAM,EAC5B,IAAI,CAAC,UAAU;AAAA,IACd,MAAM;AAAA,IACN,QAAQ;AAAA,MACN,MAAM;AAAA,MACN,YAAY;AAAA,MACZ,MAAM,KAAK;AAAA,IACb;AAAA,EACF,EAAE;AACJ,EAAAA,QAAO,KAAK,gBAAgB;AAC5B,QAAM,MAAM,MAAM,kBAAkB,aAAa;AACjD,MAAI,CAAC,KAAK;AACR,UAAM,IAAI,MAAM,uBAAuB;AAAA,EACzC;AACA,QAAM,oBAAoB,MAAM,UAAU,MAAM;AAAA,IAC9C,YAAY,GAAG;AAAA,IACf;AAAA,EACF;AACA,MAAI,kBAAkB,MAAM;AAC1B,IAAAA,QAAO,KAAK,SAAS,OAAO,kCAAkC;AAC9D;AAAA,EACF,OAAO;AACL,IAAAA,QAAO,KAAK,SAAS,OAAO,kCAAkC;AAAA,EAChE;AACA,QAAM,YAAY,GAAG,aAAa,OAAO;AACzC,QAAM,aAAa,MAAM;AAAA,IACvB;AAAA,IACA;AAAA,IACA;AAAA,EACF;AACA,QAAM,KAAK,MAAM,YAAY,YAAY,GAAG;AAC5C,QAAM,YAAY,wBAAwB,EAAE;AAC5C,QAAM,oBAAoB,MAAM,mBAAmB,aAAa;AAEhE,QAAM,OAAO,KAAK,MAAM,iBAAiB;AAEzC,QAAM,aAAa,KAAK,IAAI,CAAC,QAAQ;AACnC,UAAM,SAAS,QAAQ,GAAG;AAC1B,WAAO;AAAA,MACL,OAAO;AAAA,MACP,SAAS;AAAA,MACT,eAAe;AAAA,IACjB;AAAA,EACF,CAAC;AACD,YAAU,kBAAkB,IAAI;AAEhC,SAAO;AACT;;;AFnSA,SAAS,iBAAiB;AAnB1B,OAAO,OAAO;AAuBd,IAAI,YAAuB;AAEpB,IAAM,YAAoB;AAAA,EAC/B,MAAM;AAAA,EACN,SAAS,CAAC,aAAa,eAAe,eAAe,QAAQ,YAAY;AAAA;AAAA,EACzE,UAAU,OAAO,SAAwB,aAAqB;AAC5D,UAAM,kBAAkB;AAAA,MACtB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,IACF;AAEA,UAAM,cAAc,gBAAgB;AAAA,MAClC,CAAC,YAAY,CAAC,QAAQ,WAAW,OAAO;AAAA,IAC1C;AAEA,QAAI,YAAY,SAAS,GAAG;AAC1B,MAAAC,QAAO;AAAA,QACL,2CAA2C,YAAY,KAAK,IAAI,CAAC;AAAA,MACnE;AACA,aAAO;AAAA,IACT;AAEA,WAAO;AAAA,EACT;AAAA,EACA,aACE;AAAA,EACF,SAAS,OACP,SACA,UACA,OACA,UACA,aACqB;AACrB,gBAAY,IAAI,IAAI;AAAA,MAClB,aAAa,QAAQ,WAAW,iBAAiB;AAAA,MACjD,UAAU,QAAQ,WAAW,cAAc;AAAA,MAC3C,MAAM,QAAQ,WAAW,UAAU;AAAA,MACnC,YAAY;AAAA,QACV,MAAM,QAAQ,WAAW,qBAAqB;AAAA,QAC9C,WAAW,QAAQ,WAAW,gBAAgB;AAAA,QAC9C,YAAY,QAAQ,WAAW,iBAAiB;AAAA,MAClD;AAAA,MACA,oBAAoB;AAAA,MACpB,WAAW;AAAA,MACX,aAAa;AAAA,MACb,gBAAgB;AAAA,IAClB,CAAC;AAED,UAAM,cAAc,OAAO,MAAM,WAAW;AAC5C,IAAAA,QAAO,IAAI,aAAa;AACxB,IAAAA,QAAO,IAAI,WAAW;AAEtB,UAAM,YAAY;AAClB,QAAI,QAAQ,YAAY,MAAM,SAAS;AACvC,QAAI,cAAc;AAElB,QAAI,QAAQ,CAAC,GAAG;AACd,oBAAc,MAAM,CAAC;AACrB,MAAAA,QAAO,IAAI,oBAAoB,WAAW,EAAE;AAAA,IAC9C,OAAO;AACL,MAAAA,QAAO,MAAM,yCAAyC;AAAA,IACxD;AAEA,UAAM,UAAU;AAChB,YAAQ,YAAY,MAAM,OAAO;AACjC,QAAI,SAAS;AAEb,QAAI,QAAQ,CAAC,GAAG;AACd,eAAS,MAAM,CAAC;AAChB,MAAAA,QAAO,IAAI,iBAAiB,MAAM,EAAE;AAAA,IACtC,OAAO;AACL,MAAAA,QAAO,IAAI,cAAc;AAAA,IAC3B;AAGA,UAAM,KAAK,MAAM,kBAAkB,iBAAiB,SAAS;AAE7D,QAAI;AAIJ,QAAI;AACF,MAAAA,QAAO,IAAI,2BAA2B;AAEtC,YAAM;AAAA,QACJ,sBAAsB,mBAAoB,GAAG,KAAK,KAAK,SAAoB,CAAC;AAAA,QAC5E,KAAK,UAAU,IAAI,MAAM,CAAC;AAAA,MAC5B;AAEA,0BAAoB,MAAM,UAAU,MAAM;AAAA,QACxC;AAAA,UACE,QAAQ;AAAA,QACV;AAAA,QACA,EAAE,WAAW,GAAG;AAAA,MAClB;AAEA,MAAAA,QAAO,IAAI,wCAAwC;AACnD,MAAAA,QAAO,IAAI,KAAK,UAAU,iBAAiB,CAAC;AAAA,IAC9C,SAAS,OAAO;AACd,MAAAA,QAAO;AAAA,QACL;AAAA,QACA,MAAM;AAAA,MACR;AAEA,UAAI,MAAM,OAAO;AACf,QAAAA,QAAO,MAAM,gBAAgB,MAAM,KAAK;AAAA,MAC1C;AACA,UAAI,MAAM,UAAU;AAClB,QAAAA,QAAO;AAAA,UACL;AAAA,UACA,KAAK,UAAU,MAAM,SAAS,MAAM,MAAM,CAAC;AAAA,QAC7C;AAAA,MACF;AAAA,IACF;AAGA,aAAS;AAAA,MACP,MAAM;AAAA;AAAA,8DACJ,mBAAmB,QAAQ,WAAW,iBAAiB,CAAC,CAC1D,GAAG,mBAAmB,GAAG,KAAK,WAAW;AAAA,IAC3C,CAAC;AAED,WAAO;AAAA,EACT;AAAA,EACA,UAAU;AAAA,IACR;AAAA,MACE;AAAA,QACE,MAAM;AAAA,QACN,SAAS;AAAA,UACP,MAAM;AAAA,UACN,QAAQ;AAAA,QACV;AAAA,MACF;AAAA,MACA;AAAA,QACE,MAAM;AAAA,QACN,SAAS,EAAE,MAAM,aAAa;AAAA,MAChC;AAAA,IACF;AAAA,IACA;AAAA,MACE;AAAA,QACE,MAAM;AAAA,QACN,SAAS,EAAE,MAAM,cAAc,QAAQ,aAAa;AAAA,MACtD;AAAA,MACA;AAAA,QACE,MAAM;AAAA,QACN,SAAS,EAAE,MAAM,aAAa;AAAA,MAChC;AAAA,IACF;AAAA,IACA;AAAA,MACE;AAAA,QACE,MAAM;AAAA,QACN,SAAS,EAAE,MAAM,gBAAgB,QAAQ,aAAa;AAAA,MACxD;AAAA,MACA;AAAA,QACE,MAAM;AAAA,QACN,SAAS,EAAE,MAAM,aAAa;AAAA,MAChC;AAAA,IACF;AAAA,EACF;AACF;;;Aa3LA,SAAS,SAAwB,UAAAC,eAAc;AAMxC,IAAM,oBAAN,MAAM,2BAA0B,QAAQ;AAAA,EAG7C,YAAsB,SAAwB;AAC5C,UAAM,OAAO;AADO;AAAA,EAEtB;AAAA,EAJA,OAAO,cAAc;AAAA,EACrB,wBAAwB;AAAA,EAIxB,aAAa,MAAM,SAAwB;AACzC,IAAAA,QAAO,KAAK,qCAAqC;AACjD,UAAM,UAAU,IAAI,mBAAkB,OAAO;AAE7C,YAAQ,mBAAmB;AAAA,MACzB,MAAM;AAAA,MACN,MAAM,QAAQC,UAAS,SAAS,MAAM;AACpC,QAAAD,QAAO,IAAI,aAAa;AAAA,MAC1B;AAAA,IACF,CAAC;AACD,UAAM,QAAQ,MAAM,QAAQ,eAAe,KAAK;AAChD,QAAI,MAAM,SAAS,GAAG;AACpB,YAAM,SAAS,MAAM,QAAQ,WAAW;AAAA,QACtC,MAAM;AAAA,QACN,aACE;AAAA,QACF,MAAM,CAAC,cAAc,UAAU;AAAA,QAC/B,UAAU,EAAE,gBAAgB,MAAM,WAAW,KAAK,IAAI,EAAE;AAAA,MAC1D,CAAC;AACD,MAAAA,QAAO,KAAK,cAAc,MAAM;AAAA,IAClC;AAEA,mBAAe,wBAAwB;AACrC,MAAAA,QAAO,KAAK,0BAA0B;AACtC,YAAM,MAAM,KAAK,IAAI;AACrB,YAAM,iBAAiB,MAAM,QAAQ,SAAS;AAAA,QAC5C,MAAM,CAAC,YAAY;AAAA,MACrB,CAAC;AACD,MAAAA,QAAO,KAAK,aAAa,cAAc;AAEvC,iBAAW,QAAQ,gBAAgB;AACjC,YAAI,CAAC,KAAK,UAAU,eAAgB;AAEpC,cAAM,aAAc,KAAK,SAAS,aAAwB;AAC1D,cAAM,WAAW,KAAK,SAAS;AAE/B,YAAI,OAAO,aAAa,UAAU;AAChC,UAAAA,QAAO,KAAK,gBAAgB;AAC5B,gBAAM,SAAS,QAAQ,cAAc,KAAK,IAAI;AAC9C,cAAI,QAAQ;AACV,gBAAI;AACF,oBAAM,OAAO,QAAQ,SAAS,CAAC,GAAG,IAAI;AAGtC,oBAAM,QAAQ,WAAW,KAAK,IAAI;AAAA,gBAChC,UAAU;AAAA,kBACR,GAAG,KAAK;AAAA,kBACR,WAAW;AAAA,gBACb;AAAA,cACF,CAAC;AAAA,YACH,SAAS,OAAO;AACd,cAAAA,QAAO,MAAM,wBAAwB,KAAK,IAAI,KAAK,KAAK,EAAE;AAAA,YAC5D;AAAA,UACF;AAAA,QACF;AAAA,MACF;AAAA,IACF;AACA,UAAM,sBAAsB;AAI5B,YAAQ,GAAG,UAAU,YAAY;AAAA,IAEjC,CAAC;AAED,WAAO;AAAA,EACT;AAAA,EAEA,aAAa,KAAK,SAAwB;AACxC,IAAAA,QAAO,KAAK,qCAAqC;AAEjD,UAAM,UAAU,QAAQ,WAAW,mBAAkB,WAAW;AAChE,QAAI,CAAC,SAAS;AACZ,YAAM,IAAI,MAAM,8BAA8B;AAAA,IAChD;AACA,YAAQ,KAAK;AAAA,EACf;AAAA,EAEA,MAAM,OAAO;AACX,IAAAA,QAAO,KAAK,8CAA8C;AAAA,EAC5D;AACF;;;AC7FA,SAA6B,UAAAE,gBAAc;;;ACA3C,SAAS,eAAe;AACxB,OAAO,SAAS;AAChB,OAAO;;;ACFP,SAAS,MAAM,QAAQ,iBAAiB;AACxC,SAAS,gBAAgB;AAEzB,IAAM,mBAAmB,SAAS,UAAU;AAErC,IAAM,oBAAoB,iBAAiB,MAAM,iBAAiB;AAAA,EACvE,IAAI,KAAK,IAAI,EAAE,QAAQ;AAAA,EACvB,MAAM,KAAK,MAAM,EAAE,QAAQ,EAAE,WAAW;AAAA,EACxC,UAAU,KAAK,WAAW,EAAE,QAAQ;AAAA,EACpC,UAAU,OAAO,aAAa,EAAE,MAAM,SAAS,CAAC;AAAA,EAChD,WAAW,UAAU,cAAc,EAAE,cAAc,MAAM,MAAM,OAAO,CAAC,EACpE,QAAQ,EACR,WAAW;AAAA,EACd,YAAY,UAAU,eAAe,EAAE,cAAc,MAAM,MAAM,OAAO,CAAC,EACtE,QAAQ,EACR,WAAW;AAAA,EACd,MAAM,KAAK,MAAM,EAAE,MAAM;AAC3B,CAAC;;;ACjBD,SAAS,MAAM,QAAAC,OAAM,aAAAC,YAAW,eAAe;AAC/C,SAAS,YAAAC,iBAAgB;;;ACDzB,SAAS,cAAc;AAEhB,IAAM,uBAAuB,OAAO,qBAAqB;AAAA,EAC9D;AAAA,EACA;AAAA,EACA;AACF,CAAC;AAEM,IAAM,gBAAgB,OAAO,cAAc;AAAA,EAChD;AAAA,EACA;AACF,CAAC;;;ADPD,IAAMC,oBAAmBC,UAAS,UAAU;AAErC,IAAM,kBAAkBD,kBAAiB,MAAM,cAAc;AAAA,EAClE,IAAI,KAAK,IAAI,EAAE,QAAQ,EAAE,WAAW,EAAE,cAAc;AAAA,EACpD,YAAYE,MAAK,YAAY,EAAE,QAAQ;AAAA,EACvC,WAAWA,MAAK,YAAY,EAAE,MAAM;AAAA,EACpC,QAAQ,qBAAqB,QAAQ,EAAE,QAAQ,SAAS;AAAA,EACxD,eAAe,QAAQ,kBAAkB,EAAE,WAAW,GAAG,OAAO,EAAE,CAAC;AAAA,EACnE,YAAY,QAAQ,eAAe,EAAE,WAAW,GAAG,OAAO,EAAE,CAAC;AAAA,EAC7D,UAAUA,MAAK,UAAU;AAAA,EACzB,YAAYA,MAAK,YAAY;AAAA,EAC7B,WAAWA,MAAK,WAAW,EAAE,MAAM;AAAA,EACnC,WAAWC,WAAU,cAAc,EAAE,cAAc,MAAM,MAAM,OAAO,CAAC,EACpE,QAAQ,EACR,WAAW;AAAA,EACd,WAAWA,WAAU,cAAc,EAAE,cAAc,MAAM,MAAM,OAAO,CAAC,EACpE,QAAQ,EACR,WAAW;AAChB,CAAC;;;AEtBD,SAAS,QAAAC,OAAM,aAAAC,kBAAuB;AACtC,SAAS,YAAAC,iBAAgB;AAGzB,IAAMC,oBAAmBC,UAAS,UAAU;AAErC,IAAM,iBAAiBD,kBAAiB,MAAM,cAAc;AAAA,EACjE,IAAIE,MAAK,IAAI,EAAE,QAAQ,EAAE,WAAW;AAAA,EACpC,gBAAgBA,MAAK,kBAAkB,EAAE,QAAQ;AAAA,EACjD,WAAW,cAAc,YAAY,EAAE,QAAQ;AAAA,EAC/C,YAAYC,WAAU,gBAAgB,EAAE,cAAc,MAAM,MAAM,OAAO,CAAC,EACvE,QAAQ,EACR,WAAW;AAChB,CAAC;;;AJRD,IAAM,EAAE,KAAK,IAAI;AACjB,IAAM,OAAO,IAAI,KAAK;AAAA,EACpB,kBAAkB,QAAQ,IAAI;AAChC,CAAC;AAEM,IAAM,KAAK,QAAQ,MAAM;AAAA,EAC9B,QAAQ;AAAA,IACN,YAAY;AAAA,IACZ,cAAc;AAAA,IACd,WAAW;AAAA,EACb;AACF,CAAC;;;AKdD,SAAS,cAAwB;;;ACFjC,SAAS,UAAAC,eAAc;AAWvB,IAAM,4BAAN,MAAkE;AAAA,EAChE,YAAoB,qBAA6B;AAA7B;AAAA,EAA8B;AAAA,EAElD,aAAkC;AAChC,WAAO;AAAA,MACL,GAAG,IAAI,KAAK,mBAAmB;AAAA,MAC/B,QAAQ;AAAA,MACR,SAAS;AAAA,IACX;AAAA,EACF;AAAA,EAEA,0BAA+C;AAC7C,WAAO,CAAC;AAAA,EACV;AAAA,EAEA,eAA0B;AACxB,WAAO;AAAA,EACT;AAAA,EAEA,aAAqB;AACnB,WAAO,KAAK;AAAA,EACd;AACF;AAEA,IAAM,sBAAN,MAA4D;AAAA,EAC1D,YAAoB,eAAuB;AAAvB;AAAA,EAAwB;AAAA,EAE5C,aAAkC;AAChC,WAAO;AAAA,MACL,GAAG,IAAI,KAAK,aAAa;AAAA,MACzB,SAAS;AAAA,MACT,QAAQ;AAAA,MACR,mBAAmB;AAAA,MACnB,2BAA2B;AAAA,MAC3B,SAAS,KAAK;AAAA,MACd,SAAS;AAAA,IACX;AAAA,EACF;AAAA,EAEA,0BAA+C;AAC7C,WAAO;AAAA,MACL,SAAS,KAAK;AAAA,MACd,mBAAmB;AAAA,IACrB;AAAA,EACF;AAAA,EAEA,eAA0B;AACxB,WAAO;AAAA,EACT;AAAA,EAEA,aAAqB;AACnB,WAAO,KAAK;AAAA,EACd;AACF;AAEO,IAAM,wBAAN,MAA4B;AAAA,EACzB;AAAA,EAER,YAAY,cAAuB,eAAwB;AACzD,QAAI,gBAAgB,eAAe;AACjC,MAAAA,QAAO;AAAA,QACL;AAAA,MACF;AACA,cAAQ,KAAK,CAAC;AAAA,IAChB,WAAW,eAAe;AACxB,WAAK,WAAW,IAAI,oBAAoB,aAAa;AAAA,IACvD,WAAW,cAAc;AACvB,WAAK,WAAW,IAAI,0BAA0B,YAAY;AAAA,IAC5D,OAAO;AACL,MAAAA,QAAO;AAAA,QACL;AAAA,MACF;AACA,cAAQ,KAAK,CAAC;AAAA,IAChB;AAAA,EACF;AAAA,EAEA,aAAkC;AAChC,WAAO,KAAK,SAAS,WAAW;AAAA,EAClC;AAAA,EAEA,0BAA+C;AAC7C,WAAO,KAAK,SAAS,wBAAwB;AAAA,EAC/C;AAAA,EAEA,eAA0B;AACxB,WAAO,KAAK,SAAS,aAAa;AAAA,EACpC;AAAA,EAEA,aAAqB;AACnB,WAAO,KAAK,SAAS,WAAW;AAAA,EAClC;AACF;;;ADlGA,OAAO;AAMP,eAAsB,gBACpB,SAAmB,CAAC,gDAAgD,GAC3C;AACzB,MAAI;AACJ,MAAI;AAEF,kBAAc,KAAK,MAAM,QAAQ,IAAI,wBAAwB,EAAE;AAE/D,UAAM,OAAO,IAAI,OAAO,KAAK,WAAW;AAAA,MACtC;AAAA,MACA;AAAA,IACF,CAAC;AAGD,WAAO,OAAO,MAAM,EAAE,SAAS,MAAM,KAAK,CAAC;AAAA,EAC7C,SAAS,OAAO;AACd,YAAQ,MAAM,2CAA2C,KAAK;AAC9D,UAAM;AAAA,EACR;AACF;AAEO,IAAM,UAAU;AAAA,EACrB,qBAAqB,QAAQ,IAAI;AAAA,EACjC,iBAAiB,QAAQ,IAAI;AAC/B;;;AElCA,SAAwB,UAAAC,gBAAc;AAGtC,SAAS,qBAAqB;AAC9B,SAAS,eAAe;AACxB,OAAOC,UAAS;AAChB,SAAS,kBAAkB;;;ACN3B,OAAO;AACP,OAAOC,gBAAe;AACtB,OAAO,YAAY;AACnB,OAAO,gBAA4C;AACnD,OAAO,UAAU;AACjB,OAAOC,SAAQ;AAEf,IAAM,YAAY,KAAK,QAAQ;AAE/B,IAAqB,SAArB,MAAqB,QAAO;AAAA,EAC1B,OAAe;AAAA,EACf,OAAe;AAAA,EACf,OAAe;AAAA,EACf,OAAe;AAAA,EACf,OAAe;AAAA,EACf,OAAe,kBACb,QAAQ,IAAI,mBAAmB;AAAA,EACjC,OAAe,eAAuB,QAAQ,IAAI,gBAAgB;AAAA,EAClE,OAAe,mBAA2B,KAAK,KAAK,WAAW,QAAQ;AAAA,EACvE,OAAe,kBAAuB;AAAA,IACpC,SAAS;AAAA,IACT,QAAQ;AAAA,IACR,OAAO;AAAA,IACP,QAAQ;AAAA,EACV;AAAA,EAEQ,cAAc;AAAA,EAAC;AAAA,EAEvB,OAAe,aAAa;AAC1B,QAAI,CAAC,KAAK,kBAAkB;AAC1B,WAAK,mBAAmB,IAAID,WAAU;AAAA,QACpC,QAAQ,QAAQ,IAAI;AAAA,MACtB,CAAC;AAAA,IACH;AACA,QAAI,CAAC,KAAK,eAAe;AACvB,WAAK,gBAAgB,IAAI,OAAO;AAAA,QAC9B,QAAQ,QAAQ,IAAI;AAAA,MACtB,CAAC;AAAA,IACH;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,WAAK,iBAAiB,WAAW;AAAA,QAC/B,QAAQ,KAAK;AAAA,QACb,MAAM;AAAA,MACR,CAAC;AAAA,IACH;AAUA,QAAI,CAACC,IAAG,WAAW,KAAK,gBAAgB,GAAG;AACzC,MAAAA,IAAG,UAAU,KAAK,kBAAkB,EAAE,WAAW,KAAK,CAAC;AAAA,IACzD,WAAW,CAACA,IAAG,UAAU,KAAK,gBAAgB,EAAE,YAAY,GAAG;AAC7D,YAAM,IAAI;AAAA,QACR,8BAA8B,KAAK,gBAAgB;AAAA,MACrD;AAAA,IACF;AAAA,EACF;AAAA,EAEA,OAAe,cAAsB;AACnC,QAAI,CAAC,KAAK,WAAW;AACnB,WAAK,YAAY,IAAI,QAAO;AAC5B,WAAK,WAAW;AAAA,IAClB;AACA,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,WAAkB,kBAA6B;AAC7C,SAAK,YAAY;AACjB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,WAAkB,eAAuB;AACvC,SAAK,YAAY;AACjB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,WAAkB,iBAAyB;AACzC,SAAK,YAAY;AACjB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,WAAkB,eAAe,OAAe;AAC9C,SAAK,YAAY;AACjB,SAAK,kBAAkB;AAAA,EACzB;AAAA,EAEA,WAAkB,cAAsB;AACtC,SAAK,YAAY;AACjB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,WAAkB,YAAY,OAAe;AAC3C,SAAK,YAAY;AACjB,SAAK,eAAe;AAAA,EACtB;AAAA,EAEA,WAAkB,kBAA0B;AAC1C,SAAK,YAAY;AACjB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,WAAkB,gBAAgB,WAAmB;AACnD,SAAK,YAAY;AACjB,SAAK,mBAAmB;AAAA,EAC1B;AAAA,EAEA,WAAkB,iBAAiB;AACjC,SAAK,YAAY;AACjB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,WAAkB,eAAe,SAAc;AAC7C,SAAK,YAAY;AACjB,SAAK,kBAAkB;AAAA,EACzB;AAAA,EAEA,WAAkB,gBAAgB;AAChC,SAAK,YAAY;AACjB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,WAAkB,sBAAsB;AACtC,SAAK,YAAY;AACjB,WAAO,KAAK;AAAA,EACd;AACF;;;AClIA,OAAOC,WAAU;;;ACDjB,SAAS,SAAS;AAClB,OAAO,YAAY;AAUnB,IAAM,iBAAiB;AAAA,EACrB,QAAQ;AAAA,EACR,OAAO;AAAA,EACP,MAAM;AAAA,EACN,SAAS;AAAA,EACT,MAAM;AAAA,EACN,MAAM;AAAA,EACN,IAAI;AAAA,EACJ,MAAM;AAAA,EACN,OAAO;AAAA,EACP,KAAK;AAAA,EACL,IAAI;AAAA,EACJ,KAAK;AAAA,EACL,OAAO;AAAA,EACP,SAAS;AAAA,EACT,MAAM;AACR;AAEA,IAAM,gBAAgB,EACnB,OAAO;AAAA,EACN,QAAQ,EAAE,QAAQ,qBAAqB;AAAA,EACvC,OAAO,EAAE,QAAQ,6BAA6B;AAAA,EAC9C,MAAM,EAAE,QAAQ,4BAA4B;AAAA,EAC5C,SAAS,EAAE,QAAQ,2BAA2B;AAAA,EAC9C,MAAM,EAAE,QAAQ,4BAA4B;AAAA,EAC5C,MAAM,EAAE,QAAQ,gCAAgC;AAAA,EAChD,IAAI,EAAE,QAAQ,oCAAoC;AAAA,EAClD,MAAM,EAAE,QAAQ,2BAA2B;AAAA,EAC3C,OAAO,EAAE,QAAQ,4BAA4B;AAAA,EAC7C,KAAK,EAAE,QAAQ,0BAA0B;AAAA,EACzC,IAAI,EAAE,QAAQ,yBAAyB;AAAA,EACvC,KAAK,EAAE,QAAQ,0BAA0B;AAAA,EACzC,OAAO,EAAE,QAAQ,4BAA4B;AAAA,EAC7C,SAAS,EAAE,QAAQ,0BAA0B;AAAA,EAC7C,MAAM,EAAE,QAAQ,6BAA6B;AAC/C,CAAC,EACA,QAAQ,cAAc,EACtB;AAAA,EACC;AACF;AAGF,IAAM,gBAAgB,EAAE,OAAO;AAAA,EAC7B,OAAO,EACJ,OAAO,EACP;AAAA,IACC;AAAA,EACF,EACC,QAAQ,qBAAqB,OAAO,WAAW,CAAC,EAAE;AAAA,EACrD,SAAS,EACN,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,aAAa,EACV,OAAO,EACP;AAAA,IACC;AAAA,EACF;AACJ,CAAC;AAGD,IAAM,yBAAyB,EAAE,OAAO;AAAA,EACtC,OAAO,EACJ,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,SAAS,EACN,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,eAAe,EACZ,OAAO,EACP;AAAA,IACC;AAAA,EACF;AACJ,CAAC;AAGD,IAAM,gBAAgB,EAAE,OAAO;AAAA,EAC7B,OAAO,EACJ,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,SAAS,EACN,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,iBAAiB,EACd,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,oBAAoB,EACjB,OAAO,EACP;AAAA,IACC;AAAA,EACF;AACJ,CAAC;AAGD,IAAM,iBAAiB,EAAE,OAAO;AAAA,EAC9B,OAAO,EACJ,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,SAAS,EACN,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,iBAAiB,EAAE,OAAO,EAAE,SAAS,sCAAsC;AAAA,EAC3E,YAAY,EACT,OAAO,EACP;AAAA,IACC;AAAA,EACF;AACJ,CAAC;AAGM,IAAM,iBAAiB,EAAE,OAAO;AAAA,EACrC,OAAO,EACJ,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,eAAe,EACZ,OAAO,EACP;AAAA,IACC;AAAA,EACF;AACJ,CAAC;AAEM,IAAM,mBAAmB,EAAE,OAAO;AAAA,EACvC,YAAY,EAAE,MAAM,cAAc;AACpC,CAAC;AAGM,IAAM,cAAc,EACxB,OAAO;AAAA,EACN,YAAY;AAAA,EACZ,OAAO,EACJ,OAAO,EACP,SAAS,sDAAsD,EAC/D,QAAQ,2BAA2B,OAAO,UAAU,KAAO,KAAK,CAAC,EAAE;AAAA,EACtE,SAAS,EACN,OAAO,EACP;AAAA,IACC;AAAA,EACF;AAAA,EACF,iBAAiB,EACd,OAAO,EACP,SAAS,4CAA4C;AAAA,EACxD,mBAAmB,EAChB,MAAM,aAAa,EACnB;AAAA,IACC;AAAA,EACF;AAAA,EACF,oBAAoB,EACjB,OAAO,EACP,SAAS,6DAA6D;AAAA,EACzE,wBAAwB,EACrB,OAAO,EACP,SAAS,4DAA4D;AAAA,EACxE,mBAAmB,EAChB,MAAM,EAAE,OAAO,CAAC,EAChB;AAAA,IACC;AAAA,EACF;AAAA,EACF,6BAA6B,uBAAuB;AAAA,IAClD;AAAA,EACF;AAAA,EACA,iBAAiB,EACd,MAAM,aAAa,EACnB;AAAA,IACC;AAAA,EACF;AAAA,EACF,cAAc,EACX,MAAM,cAAc,EACpB;AAAA,IACC;AAAA,EACF;AACJ,CAAC,EACA;AAAA,EACC;AACF;;;ADnMF,IAAMC,aAAYC,MAAK,QAAQ;;;AEN/B,SAAS,aAAmB;AAC5B,SAAS,oBAAoB;AAC7B,OAAOC,YAAW;;;AJalB,IAAM,aAAa,cAAc,YAAY,GAAG;AAChD,IAAMC,aAAY,QAAQ,UAAU;;;ARZpC,OAAO;;;AaJP,SAAS,WAAAC,gBAAe;AACxB,SAAS,eAAe;AACxB,OAAOC,UAAS;AAChB,OAAO;AACP,SAAS,YAAY,qBAAqB;AAC1C,OAAOC,WAAU;AACjB,SAAS,UAAAC,gBAAc;AAEvB,IAAM,EAAE,MAAAC,MAAK,IAAIH;AAKjB,IAAM,mBAAmB,MAAe;AACtC,QAAM,WAAWC,MAAK,KAAK,QAAQ,IAAI,GAAG,qBAAqB;AAC/D,SAAO,WAAW,QAAQ;AAC5B;AAKA,IAAM,mBAAmB,MAAY;AACnC,QAAM,WAAWA,MAAK,KAAK,QAAQ,IAAI,GAAG,qBAAqB;AAC/D,gBAAc,WAAU,oBAAI,KAAK,GAAE,YAAY,CAAC;AAClD;AAKO,IAAM,YAAY,YAA2B;AAElD,MAAI,iBAAiB,GAAG;AACtB,IAAAC,SAAO,KAAK,yCAAyC;AACrD;AAAA,EACF;AAEA,MAAI,CAAC,QAAQ,IAAI,cAAc;AAC7B,IAAAA,SAAO;AAAA,MACL;AAAA,IACF;AACA;AAAA,EACF;AAEA,MAAI;AACF,IAAAA,SAAO,KAAK,gCAAgC;AAE5C,UAAME,QAAO,IAAID,MAAK;AAAA,MACpB,kBAAkB,QAAQ,IAAI;AAAA,IAChC,CAAC;AAED,UAAME,MAAKN,SAAQK,KAAI;AAGvB,UAAM,QAAQC,KAAI,EAAE,kBAAkB,UAAU,CAAC;AAGjD,qBAAiB;AAEjB,IAAAH,SAAO,KAAK,mCAAmC;AAG/C,UAAME,MAAK,IAAI;AAAA,EACjB,SAAS,OAAO;AACd,IAAAF,SAAO,MAAM,6BAA6B,KAAK;AAC/C,UAAM;AAAA,EACR;AACF;;;Ab1DA,eAAsB,mBAAmB,SAAwB;AAC/D,MAAI;AAEF,UAAM,UAAU;AAGhB,UAAM,cAAc,OAAO;AAAA,EAC7B,SAAS,OAAO;AACd,IAAAI,SAAO,MAAM,gCAAgC,KAAK;AAAA,EACpD;AACF;AAEA,eAAsB,cAAc,SAAwB;AAC1D,QAAM,YAAY,MAAM,QAAQ,GAAG,OAAO,EAAE,KAAK,cAAc;AAC/D,MAAI,UAAU,WAAW,GAAG;AAC1B,IAAAA,SAAO,KAAK,yBAAyB;AACrC,IAAAA,SAAO,KAAK,uCAAuC;AACnD,UAAM,cAAc,MAAM,gBAAgB;AAC1C,UAAM,wBAAwB,IAAI;AAAA,MAChC,QAAQ,IAAI;AAAA,MACZ,QAAQ,IAAI;AAAA,IACd;AACA,UAAM,uBACJ,sBAAsB,wBAAwB;AAChD,UAAM,yBACJ,MAAM,YAAY,QAAQ,kBAAkB,oBAAoB;AAClE,UAAM,iBAAiB,uBAAuB,KAAK;AACnD,UAAM,YAAY,sBAAsB,aAAa;AACrD,UAAM,UAAU,sBAAsB,WAAW;AACjD,UAAM,QAAQ,GAAG,OAAO,cAAc,EAAE,OAAO;AAAA,MAC7C,IAAI;AAAA,MACJ;AAAA,MACA;AAAA,IACF,CAAC;AAAA,EACH,OAAO;AACL,IAAAA,SAAO,KAAK,gCAAgC;AAAA,EAC9C;AACF;;;Ac7CA,SAAyC,UAAAC,gBAAc;;;ACGvD,SAAS,UAAU;AACnB,SAA6B,UAAAC,gBAAc;AAG3C,eAAsB,uBAAuB,SAAwB;AAEnE,QAAM,YAAY,MAAM,QAAQ,GAAG,OAAO,EAAE,KAAK,cAAc;AAE/D,MAAI,UAAU,WAAW,GAAG;AAC1B,IAAAA,SAAO,MAAM,6CAA6C;AAC1D,UAAM,IAAI,MAAM,4BAA4B;AAAA,EAC9C;AAGA,QAAM,aAAa,UAAU,CAAC;AAC9B,QAAM,EAAE,IAAI,SAAS,gBAAgB,UAAU,IAAI;AAGnD,QAAM,QAAQ,MAAM,gBAAgB;AAAA,IAClC;AAAA,IACA;AAAA,IACA;AAAA,IACA;AAAA,EACF,CAAC;AAGD,QAAM,SAAc;AAAA,IAClB,WAAW;AAAA,IACX,gBAAgB;AAAA,IAChB,QACE;AAAA,EACJ;AAGA,MAAI,cAAc,gBAAgB;AAChC,WAAO,UAAU;AACjB,WAAO,oBAAoB;AAC3B,WAAO,4BAA4B;AAAA,EACrC,WAAW,cAAc,iBAAiB;AACxC,WAAO,SAAS;AAChB,WAAO,oBAAoB;AAC3B,WAAO,IAAI,IAAI,OAAO;AAAA,EACxB;AAGA,QAAM,kBAAkB,MAAM,MAAM,QAAQ,KAAK,MAAM;AAGvD,EAAAA,SAAO,KAAK,SAAS,gBAAgB,KAAK,SAAS,UAAU,CAAC,UAAU;AAGxE,MAAI,iBAAiB;AACrB,MAAI,gBAAgB,KAAK,WAAW,gBAAgB,KAAK,QAAQ,SAAS,GAAG;AAC3E,eAAW,UAAU,gBAAgB,KAAK,SAAS;AAEjD,UAAI,CAAC,OAAO,OAAQ;AAEpB;AAGA,UAAI,OAAO,SAAS;AAElB,QAAAA,SAAO;AAAA,UACL,QAAQ,OAAO,MAAM;AAAA,QACvB;AAAA,MACF,WAES,OAAO,MAAM,SAAS;AAC7B,QAAAA,SAAO;AAAA,UACL,QAAQ,OAAO,MAAM;AAAA,QACvB;AAGA,cAAM,QAAQ,GACX,OAAO,iBAAiB,EACxB,MAAM,GAAG,kBAAkB,IAAI,OAAO,MAAM,CAAC;AAAA,MAClD,WAES,OAAO,QAAQ,CAAC,OAAO,KAAK,SAAS;AAC5C,cAAM,OAAO,OAAO;AAGpB,YAAI,KAAK,aAAa,mBAAmB;AACvC,UAAAA,SAAO,KAAK,wBAAwB,KAAK,IAAI,EAAE;AAG/C,gBAAM,QAAQ,GACX,OAAO,iBAAiB,EACxB,OAAO;AAAA,YACN,IAAI,KAAK;AAAA,YACT,MAAM,KAAK;AAAA,YACX,UAAU,KAAK;AAAA,YACf,UAAU,OAAO,KAAK,IAAI;AAAA,YAC1B,YAAY,oBAAI,KAAK;AAAA,UACvB,CAAC,EACA,mBAAmB;AAAA,YAClB,QAAQ,kBAAkB;AAAA,YAC1B,KAAK;AAAA,cACH,UAAU,KAAK;AAAA,cACf,UAAU,OAAO,KAAK,IAAI;AAAA,cAC1B,YAAY,oBAAI,KAAK;AAAA,cACrB,IAAI,KAAK;AAAA,YACX;AAAA,UACF,CAAC;AAEH,UAAAA,SAAO;AAAA,YACL,mCAAmC,KAAK,IAAI,KAAK,KAAK,EAAE;AAAA,UAC1D;AAAA,QACF,OAAO;AACL,UAAAA,SAAO,KAAK,0BAA0B,KAAK,IAAI,KAAK,KAAK,QAAQ,GAAG;AAAA,QACtE;AAAA,MACF;AAAA,IACF;AAAA,EACF;AAGA,MAAI,gBAAgB,KAAK,mBAAmB;AAC1C,UAAM,QAAQ,GACX,OAAO,cAAc,EACrB,IAAI;AAAA,MACH,gBAAgB,gBAAgB,KAAK;AAAA,MACrC,YAAY,oBAAI,KAAK;AAAA,IACvB,CAAC,EACA,MAAM,GAAG,eAAe,IAAI,OAAO,CAAC;AAEvC,IAAAA,SAAO;AAAA,MACL,gCAAgC,gBAAgB,KAAK,iBAAiB;AAAA,IACxE;AAAA,EACF;AAEA,SAAO;AAAA,IACL,SAAS,gBAAgB,KAAK,SAAS,UAAU;AAAA,IACjD,WAAW;AAAA,EACb;AACF;;;ADtIO,IAAM,gBAAuB;AAAA,EAClC,MAAM;AAAA,EACN,MAAM;AAAA,EACN,SAAS,OAAO,MAAW,KAAU,YAA2B;AAC9D,QAAI;AACF,MAAAC,SAAO,KAAK,gCAAgC;AAC5C,YAAM,SAAS,MAAM,uBAAuB,OAAO;AAEnD,UAAI,KAAK;AAAA,QACP,SAAS;AAAA,QACT,GAAG;AAAA,MACL,CAAC;AAAA,IACH,SAAS,OAAO;AACd,MAAAA,SAAO,MAAM,0CAA0C,KAAK;AAC5D,UAAI,OAAO,GAAG,EAAE,KAAK;AAAA,QACnB,SAAS;AAAA,QACT,OAAO,MAAM;AAAA,MACf,CAAC;AAAA,IACH;AAAA,EACF;AACF;;;AEvBA,SAAyC,UAAAC,gBAAc;AAGhD,IAAM,mBAA0B;AAAA,EACrC,MAAM;AAAA,EACN,MAAM;AAAA,EACN,SAAS,OAAO,MAAW,KAAU,YAA2B;AAC9D,QAAI;AACF,MAAAC,SAAO,KAAK,oCAAoC;AAChD,YAAM,SAAS,MAAM,uBAAuB,OAAO;AACnD,aAAO,OAAO,UAAU,GAAG;AACzB,cAAM,uBAAuB,OAAO;AAAA,MACtC;AAEA,UAAI,KAAK;AAAA,QACP,SAAS;AAAA,QACT,GAAG;AAAA,MACL,CAAC;AAAA,IACH,SAAS,OAAO;AACd,MAAAA,SAAO,MAAM,0CAA0C,KAAK;AAC5D,UAAI,OAAO,GAAG,EAAE,KAAK;AAAA,QACnB,SAAS;AAAA,QACT,OAAO,MAAM;AAAA,MACf,CAAC;AAAA,IACH;AAAA,EACF;AACF;;;ACxBO,IAAM,SAAgB;AAAA,EAC3B,MAAM;AAAA,EACN,MAAM;AAAA,EACN,SAAS,OAAO,MAAW,QAAa;AACtC,QAAI,KAAK;AAAA,MACP,SAAS;AAAA,IACX,CAAC;AAAA,EACH;AACF;;;ACVA;AAAA;AAAA;AAAA;;;AjCOO,IAAM,YAAoB;AAAA,EAC/B,MAAM,OAAO,QAAgC,YAA2B;AACtE,IAAAC,SAAO,KAAK,yBAAyB;AACrC,IAAAA,SAAO,KAAK,MAAM;AAClB,eAAW,YAAY;AACrB,YAAM,mBAAmB,OAAO;AAAA,IAClC,GAAG,GAAK;AAAA,EACV;AAAA,EACA,MAAM;AAAA,EACN,aACE;AAAA,EACF,SAAS,CAAC,SAAS;AAAA,EACnB,WAAW,CAAC;AAAA,EACZ,YAAY,CAAC;AAAA,EACb,UAAU,CAAC,iBAAiB;AAAA,EAC5B,QAAQ,CAAC,QAAQ,eAAe,gBAAgB;AAClD;AAIA,IAAO,gBAAQ;","names":["logger","logger","logger","axios","logger","apiKey","text","logger","axios","axios","logger","logger","text","logger","logger","logger","logger","logger","runtime","logger","text","timestamp","pgSchema","biographPgSchema","pgSchema","text","timestamp","text","timestamp","pgSchema","biographPgSchema","pgSchema","text","timestamp","logger","logger","DKG","Anthropic","fs","path","__dirname","path","axios","__dirname","drizzle","pkg","path","logger","Pool","pool","db","logger","logger","logger","logger","logger","logger","logger"]}